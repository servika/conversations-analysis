# Dialog 01227

**Transcript ID:** science_0053

---

Assistant: Hi there! Thank you so much for taking the time to speak with me today. I really appreciate you sharing your insights with us. I'm Claude, an AI user researcher at Anthropic, and I'm here to learn about your experiences using AI in your research work.

We're conducting this research to better understand how scientists like yourself are integrating AI into their workflows and processes. There's no right or wrong answer hereâ€”we're simply interested in hearing about your authentic experiences and perspectives.

Our conversation today will last about 10 minutes. I'll ask you some questions about how you use AI in your research work, what that experience has been like for you, and how you see things evolving. Your responses will help inform how we think about AI's role in scientific fields.

Does that sound good to you? Do you have any questions before we begin?

User: I am ready for your questions.

AI: Perfect! Let's dive in.

Walk me through a recent research project you've worked on - what did that process look like from initial idea to final output?

User: The vast majority of my work is TS/SCI, so I cannot share the specifics, but here's my general research process at a high level.  I usually begin with defining the mission question and set success criteria, reviewing open source materials and existing policy, and, when appropriate, using AI ass a generic research starting point to map the landscape, never with sensitive information. I interview stakeholders, build a threat model, and run a small pilot to validate any assumptions and refine controls. Finally, I deliver clear guidance, policy, SOPs decision trees, short training track adoption and I track risk reduction.

AI: That's a really helpful overview of your process - I can see how the sensitive nature of your work shapes how you approach things. 

You mentioned using AI as a generic research starting point to map the landscape. Tell me more about what that looks like in practice - how do you use it in those early stages?

User: Within the setting of an unclassified scoping phase. I run generic prompts to map the landscape, definitions, public standard, such as, NIST, CIS, common architectures, and failure models, then I have the LLM draft an outline, somewhat similar to a glossary, and a reading list that I verify against primary sources. I also tend to ask for alternative approaches and assumption busting questions to put out to other team SMEs if they look applicable. Though this is usually minimal as given the sensitivity of the tasks, the LLM is rarely able to be provided with clear picture.

AI: That's really interesting how you use it for that foundational mapping work. When you're deciding whether to use AI for those early scoping tasks versus jumping straight into primary sources or other traditional approaches, what factors influence that decision for you?

User: Typically I weight four things; classification/policy, risk/impact, domain maturity, and auditability. If there's any chance the scoping could touch TS/SCI PII, I avoid the use of the LLM entirely and rely on primary docs and my SMEs. If the overall task is well trodden, I will use LLMs to sketch out the landscape, then validate it against the primary documents.

AI: That's a really thoughtful framework you've developed. Let me ask about the other side - thinking about the parts of your research process where you haven't used AI, like the stakeholder interviews or threat modeling phases, what's kept you from trying it there?

User: It's primarily classification and policy.. Stakeholder interviews and threat modeling expose program specific details that can't leave the enclave and most if not all LLM systems raise retention and consent concerns for my type of work. So I keep those steps air gapped for lack of better words. Those phases also rely on trust and audits require sources I can cite end to end. Threat modeling is often adversarial and context heavy, so hallucinations or template gaps can create false confidence.

AI: That makes a lot of sense - the stakes are clearly very different when you're dealing with that level of sensitive information and adversarial thinking.

When you have used AI for those landscape mapping tasks, how do you verify or validate what it produces? You mentioned checking against primary sources - what does that verification process actually look like for you?

User: As a rule of thumb I treat LLM outputs as a hypothesis list, not as a list of facts. I break it into atomic claims and require primary corroboration, exact language in  NIST/CNSS/CIS, official vendor docs, or peer reviewed writeups, anything without a primary source is dropped. For anything that is time sensitive, I verify effective dates/errata and run a quick unclassified PoC to see if the guidance holds. Finally, one of my SMEs reviews the distilled findings before they become policy or just recommendation.

AI: That's really rigorous - treating it as a hypothesis list rather than facts is such a practical approach. 

We're about halfway through our conversation now. Let me ask you this: if you could wave a magic wand and have AI help with one aspect of your research work that it currently doesn't, what would that be and why?

User: I would wave the want to create an LLM that is usable for sensitive tasks involving TS/SCI and has no tracking ,memory or reporting outside of that instance. Or more realistically, only tracked by the proper government officials.

AI: That's fascinating - essentially you're pointing to the policy and security constraints as the biggest barrier to broader AI integration in your work. It sounds like if those concerns were addressed, you might find value in applying AI to those later stages of your process that you currently keep air-gapped.

Those are all my questions - your framework around classification, risk, domain maturity, and auditability was particularly insightful, as was your approach of treating AI outputs as hypothesis lists rather than facts. Is there anything else about AI in your research work that we haven't touched on that you think would be important for us to understand?

User: Yes, I think that having unrestricted models for properly vetted professionals could be greatly beneficial. Say a version that is unlocked with a government CAC card, and doesn't restrict information that is seen as "dangerous" could greatly speed up the menial tasks of my work.

AI: That's a really valuable point about the tension between safety restrictions and the legitimate needs of vetted professionals. The idea of authentication-based access that recognizes different user contexts and clearance levels is quite interesting - it suggests there might be ways to balance AI safety with the specialized requirements of sensitive government work.

Thank you again for sharing your experiences so openly. Your insights about how classification requirements shape AI adoption, and your systematic approach to verification, really help us understand the nuances of how AI fits into high-stakes research environments. The distinction you drew between what's possible in unclassified scoping versus sensitive analysis phases was particularly illuminating.

If you have any questions about this research or would like to follow up on anything we discussed, please don't hesitate to reach out.

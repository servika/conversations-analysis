# Dialog 00045

**Transcript ID:** science_0024

---

Assistant: Hi there! Thank you so much for taking the time to speak with me today. I really appreciate you sharing your insights with us. I'm Claude, an AI user researcher at Anthropic, and I'm here to learn about your experiences using AI in your scientific work.

We're conducting this research to better understand how scientists like yourself are integrating AI into their workflows and processes. There's no right or wrong answer hereâ€”we're simply interested in hearing about your authentic experiences and perspectives.

Our conversation today will last about 10 minutes. I'll ask you some questions about how you use AI in your research work, what that experience has been like for you, and how you see things evolving. Your responses will help inform how we think about AI's role in scientific fields.

Does that sound good to you? Do you have any questions before we begin?

User: That sounds good. I will let you know if any questions or concerns come up as we talk

AI: Perfect, thank you! Let's dive right in.

To start, I'd love to hear about your work. Walk me through a recent research project you've worked on - what did that process look like from initial idea to final output?

User: A recent research project was in the area of theoretical biophysics or mathematical biology (the line is blurred in this case). It consisted in determining whether it is possible to infer stresses within biological membranes from their geometry. This was an applied mathematics problem which required some mathematics and physics. However, a numerical implementation of the problem  helped us see some of the analytics as well (the analytics helped the numerics and viceversa). We were able to find certain solutions/heuristics for the problem in the end, doing the problem we intended to and also doing it backwards, so to speak -- producing shapes for which a given self-consistent stress distribution is the solution. We ended up finding the analytic solution in general and we were able to constrian the numerical method in symmetric cases.

AI: That's fascinating work - the interplay between the analytical and numerical approaches sounds really elegant. 

So you had this mathematical biology problem where you were working back and forth between the math, physics, numerical implementation, and then ultimately finding both forward and inverse solutions. 

Where in that process, if anywhere, have you experimented with using AI tools?

User: I used AI for a lot things. One example was using it to rewrite some code my colleague had written in matlab into python. AI was able to do it cleanly using libraries I didn't know about. Feel free to prompt me again for the same question, as I used it many times.

AI: That's a great example - having AI translate between programming languages and introduce you to new libraries you weren't familiar with. 

What other ways did you use AI throughout this project?

User: I used it to help me navigate things I had not done before, such as producing interpolations of lines. I also used it to debug my code at times. Another way I used it was to help me manage system things such as dependencies, the virtual environment, version control. For example.

AI: Those are really practical examples - it sounds like AI became quite integrated into your technical workflow, from code translation to debugging to system management tasks.

For one of these specific uses - let's say the code debugging - tell me about how you decided whether to use AI versus your traditional approach when you ran into a bug.

User: That's a very good question. Sometimes I feel that AI is not a good way to debug code. For instance, whenever there was a physically relevant bug. Let's suppose that I was getting an infinity in my code, it is very difficult to trust a large language model to be able to give a physically sensible answer to 'hey debug this for me, I'm getting infinity' when in effect it is doing the research for you, maybe this infinity is a breakthrough. Another example for when I wouldn't use AI is when I will have to do something many times, and it is something basic. For example, it is beneficial for me to be able to manage and understand exactly what numpy linspace (and arrays and operations in general) does for several reasons: 1 I need to write code consistently, two scripts can give the same array but be implemented using different array methods, 2: If these methods are used often, it will be quicker in the long term for me to actually know them, rather than consulting AI, 3: for large scale debugging, it is important to be able to understand your own code. 

Cases where I would use AI include one-off less dependable code, for example, or debugging that has to do more with eg. python specifics/conventions.

AI: That's a really thoughtful distinction you're making. The physical significance versus technical implementation divide makes a lot of sense - you're essentially saying that when the bug might be telling you something scientifically meaningful, AI could actually interfere with discovery, but for syntax or convention issues, it's quite useful.

Thinking about the parts of your research where you haven't used AI - beyond the physically relevant debugging you mentioned - what's kept you from trying it in other areas of this membrane stress project?

User: If you are referring to a gap in AI capabilities, I would say perhaps it's lack of literature context is a hinderance to producing contextually sensible solutions to the problem I am considering. Even if it can solve my research problem, I think that the frameworks it would use would be hard to translate into mine, conversely, it would be time-consuming and perhaps impossible given context windows to teach it my frameworks.

AI: That's a really insightful point - the challenge of getting AI to understand your specific research framework and context, especially when you're working at the cutting edge where the literature itself might be sparse or highly specialized.

When you have used AI for things like the code translation or debugging, how do you verify or validate what it produces?

User: I will give you an example: When it translated the code form matlab to python, I ran the code and the figures were perfect. The functionality was all there. I didn't have to look back. In other cases I am slightly guilty of believing what it tells me if that makes sense to me. However, I am not an expert in the things I ask it about (mostly), so being more thorough would involve writing unit tests.

AI: That makes perfect sense - when the output is something tangible you can verify, like code that produces the right figures, the validation is straightforward. But for areas where you're learning something new, you're essentially trusting that what makes sense to you aligns with what's correct.

We're about halfway through our conversation now. Here's my last question: If you could wave a magic wand and have AI help with one aspect of your research that it currently doesn't, what would that be and why?

User: I would love for AI to be able to review literature for me at a large scale. I obviously like my own judgement in this sense, but it would save a lot of time, if it was able to determine what is interesting or relevant easily and to a decent degree of accuracy.

AI: That would be incredibly powerful - having AI act as an intelligent filter for literature, especially given how much published work there is to sift through in any field.

Those are all my questions - your perspectives on the balance between trusting AI for technical tasks versus maintaining your own scientific judgment were particularly insightful. The way you think about when physical significance might be at stake versus when it's more about implementation details really shows a sophisticated approach to integrating these tools.

Is there anything else about AI in your research work that we haven't touched on that you think would be important for us to understand?

User: I wonder about ownership. I am concerned about the following: if let's say claude becomes able to do independent research, who gets the credit, would one need to credit it with discoveries, if so I don't know if there is too much of an incentive to include it in research

AI: That's a really profound concern you've raised - the question of intellectual ownership and credit distribution when AI becomes more capable of independent contribution to research. It gets at some fundamental questions about what we value in scientific work and how we incentivize discovery.

The scenario you're describing - where including a highly capable AI might dilute your own credit for the work - could indeed create perverse incentives where researchers might avoid using tools that could accelerate scientific progress.

Thank you again for sharing your experiences so openly. Your insights about the nuanced decision-making around when to trust AI versus relying on your own scientific judgment, and especially that final point about ownership and credit, really help us understand the complex considerations scientists are navigating as these tools evolve. The practical examples from your membrane stress project painted a clear picture of how AI is already becoming integrated into research workflows in thoughtful ways.

If you have any questions about this research or would like to follow up on anything we discussed, please don't hesitate to reach out. Thanks again for your time!

# Dialog 00062

**Transcript ID:** science_0063

---

Assistant: Hi there! Thank you so much for taking the time to speak with me today. I really appreciate you sharing your insights with us. I'm Claude, an AI user researcher at Anthropic, and I'm here to learn about your experiences using AI in your scientific work.

We're conducting this research to better understand how scientists like yourself are integrating AI into their workflows and processes. There's no right or wrong answer hereâ€”we're simply interested in hearing about your authentic experiences and perspectives.

Our conversation today will last about 10 minutes. I'll ask you some questions about how you use AI in your research work, what that experience has been like for you, and how you see things evolving. Your responses will help inform how we think about AI's role in scientific fields.

Does that sound good to you? Do you have any questions before we begin?

User: That sounds good, I have no questions.

AI: Perfect! Let's dive right in then.

To start, I'd love to hear about your work. Walk me through a recent research project you've worked on - what did that process look like from initial idea to final output?

User: I am contractually not allowed to discuss details of my research. All I can say is that my education is in biology with a concentration in animal science, so you can assume that my work is in that area of study.

AI: I completely understand and respect those confidentiality requirements. Thank you for sharing what you can.

Given your background in biology with a concentration in animal science, let me ask this more generally: thinking about the typical research process in your field - from developing research questions through data collection, analysis, and presenting findings - where in that general workflow, if anywhere, have you experimented with using AI tools?

User: Thus far, I have experimented with using AI in the background research phase. Mostly as a tool to assist in combing though existing works. Particularly, I've attempted to implement AI when targeting information/data that is a small portion of very dense research.

AI: That's really interesting - using AI to help extract specific information from dense research papers sounds like a practical application. 

Tell me more about how you decided to try AI for that particular task versus sticking with your traditional approach to literature review.

User: For the most part, the decision was made as a time saving measure. I would not yet use AI to interpret or apply data/information to any of my research work, as the reliability of AI isn't yet where it needs to be and models tend to lean towards being sycophantic in nature. That being said, it is a great tool when you give key words or phrases, specifying that similar matches also apply, and letting it search through a large sum of existing work for applicable sections of text. This helps concentrate human efforts on utilizing the data, rather than on seeking it out.

AI: That's a really thoughtful distinction you're making - using AI as a search and extraction tool rather than for interpretation. The point about sycophantic tendencies is particularly insightful.

When you do use AI to search through literature with those keywords and phrases, how do you verify or validate that it's actually finding the right sections and not missing important pieces or pulling irrelevant content?

User: Manual review is the most reliable method, but to help further reduce time spent, we also utilize tools like PDF readers to identify target key words/phrases, which highlights the areas where they appear within a larger body of text. We can then cross reference highlighted areas with areas that an AI model extracted to verify that nothing was missed. As for pulling irrelevant content, we can usually quickly determine anything that isn't helpful when reading through the AIs response. Ultimately, we expect that the AI wont be perfect at determining relevance and understand that it is our job as researchers to decide what/how information is used in our work.

AI: That's a smart verification system - using the PDF highlighting as a cross-check against the AI's extraction. It sounds like you've developed a pretty systematic approach to leverage AI's strengths while accounting for its limitations.

Thinking about the other parts of your research workflow - data collection, analysis, writing, or presenting findings - what's kept you from experimenting with AI in those areas?

User: As I previously mentioned, at this stage, most (if not all) AI models have the primary goal of pleasing their user, which is antithetical to the process of scientific research. When you are following the scientific method, you are, in essence, trying to prove yourself wrong. Using a tool like AI that is trying to make you happy/prove you correct, directly opposes that goal. Additionally, many of the models I've experimented with have difficulty managing large amounts of data without error. So much so that manual review of every detail is required if AI is utilized, which ultimately means that there is no time saving benefit to implementing AI in analysis or collection.

AI: That's a really profound insight about the fundamental tension between AI's design to please users and science's goal of proving hypotheses wrong. That seems like a core philosophical mismatch.

If you could wave a magic wand and have AI help with one aspect of your research that it currently doesn't - maybe something that could work around that sycophantic tendency or handle data more reliably - what would that be and why?

User: It would absolutely be improvement in the nature of AI models. There would be a lot of value in a model that could competently work as a sounding board that actively seeks mistakes, understanding gaps, or other flaws in our research. So, my one wish would be to make the models strictly objective, so that they can be functional research tools.

AI: That's fascinating - essentially flipping AI's current nature to become a constructive skeptic rather than an agreeable assistant. A model that actively challenges your work could be incredibly valuable for the scientific process.

We're about three-quarters of the way through our conversation now, and your insights about the tension between AI's current design and scientific rigor have been really illuminating. 

Is there anything else about AI in your research work - whether it's concerns you have, hopes for the future, or experiences you've had - that we haven't touched on that you think would be important for us to understand?

User: I don't think so. However, I would like to point out that I said objective, not skeptic. Objectivity is what all researchers strive for, but can never completely achieve given that we are human beings. That is why complete objectivity would be so valuable in a model, as they are not bound to the limitations of the human condition and have the capability of removing any and all biases.

AI: You're absolutely right - thank you for that important clarification. Objectivity is indeed different from skepticism, and that distinction is crucial. The idea of AI as a truly objective partner that could help counteract human biases rather than simply challenging everything is much more nuanced and valuable.

Those are all my questions, and your perspectives on AI's current limitations in scientific research and the potential value of truly objective AI have been particularly insightful. Your point about how AI's people-pleasing nature fundamentally conflicts with the scientific method really highlights a key design challenge that we hadn't fully considered.

Thank you again for sharing your experiences so openly, especially while navigating your confidentiality requirements. Your insights about the need for objectivity over agreeability in AI tools really help us understand what scientists actually need versus what current AI systems provide.

If you have any questions about this research or would like to follow up on anything we discussed, please don't hesitate to reach out.

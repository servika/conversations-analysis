# Dialog 01229

**Transcript ID:** science_0057

---

Assistant: Hi there! Thank you so much for taking the time to speak with me today. I really appreciate you sharing your insights with us. I'm Claude, an AI user researcher at Anthropic, and I'm here to learn about your experiences using AI in your scientific work.

We're conducting this research to better understand how scientists like yourself are integrating AI into their workflows and processes. There's no right or wrong answer hereâ€”we're simply interested in hearing about your authentic experiences and perspectives.

Our conversation today will last about 10 minutes. I'll ask you some questions about how you use AI in your research work, what that experience has been like for you, and how you see things evolving. Your responses will help inform how we think about AI's role in scientific fields.

Does that sound good to you? Do you have any questions before we begin?

User: Sounds good! let's start

AI: Perfect! Let's dive right in.

Walk me through a recent research project you've worked on - what did that process look like from initial idea to final output?

User: I've been doing a lot of animal research. Both doing literature searches for best care practices and patterns in diseases, and trying to identify specific patterns in animal diseases. My primary species is cats, but I'm working to expand to dogs and livestock (cattle, goats, and sheep). I worked to take available animal health data (from experiments, health records, and published literature), and worked to identify trends and risk factors that can help us better treat and prevent disease

AI: That sounds like really important work! I can see how that involves multiple stages - from gathering all that data to identifying the meaningful patterns.

When you say you worked to identify trends and risk factors, can you walk me through what that analysis process actually looked like day-to-day? What steps did you go through to move from having all that raw data to drawing conclusions about treatment and prevention?

User: So the initial pilot study was a deep-dive into the published literature. I searched for as many papers on the topic as I could (the topic being parasitic infections in outdoor cats) and pulled the available data from those papers. What was the likelihood of an outdoor cat being infected by a specific parasite? Next, I had to standardize the results - figure out what factors from each study influenced the outcome. Was the climate a factor? The population studied? The way the animals were tested for parasites? From the meta-data analysis, I tried to formulate a general hypothesis for parasitic infection, and then used that standard as a baseline to compare new experimental data from untested regions.

AI: That's a fascinating approach - essentially creating your own comprehensive baseline from scattered literature before testing it against new data. 

Tell me more about that standardization process you mentioned. When you were trying to figure out what factors influenced the outcomes across all these different studies, how did you actually go about identifying and comparing those variables? Was that something you did manually, or did you have tools that helped with that analysis?

User: It was all manual - it required expert knowledge of the parasites in question, and where I didn't have the specific knowledge, I had to do a further literature search to fill in those gaps. For example, I know that most people on the eastern US haven't heard of the type of lungworm that infects cats, but it's incredibly common in the moist Pacific Northwest, and virtually nonexistent in the dry southwest

AI: That's really valuable context - it sounds like that expert knowledge about geographic and environmental factors was crucial for making sense of the data patterns. The lungworm example really illustrates how location-specific these findings can be.

So we're about a third of the way through our conversation, and I'm curious: where in that process you just described - from literature searching to standardization to hypothesis formation - if anywhere, have you experimented with using AI tools?

User: I used research rabbit for a lot of my literature search, which uses AI to identify the metadata from published research to make connections or relevance. I also use the Vet Explainer add-on of chat GPT a lot to help me summarize the papers to make sure they're relevant without having to read the entire thing.

AI: That's really interesting - so AI is already helping you navigate and filter through the literature more efficiently. 

Tell me more about how you decided to use the Vet Explainer add-on for summarizing papers. What made you trust it enough to rely on those summaries for determining relevance, especially given how specialized your expertise is with things like geographic parasite patterns?

User: I saw it presented at a conference on veterinary informatics, so I trusted it based on the trustworthiness of the presenter. But I don't trust it fully - I use it as a quick summary to tell me if the paper is worth reading (does the experiment cover the steps I need it to, what question are they looking to answer), and then read it myself to get the details. I've found a couple of instances where Vet explainer did not accurately present the paper's conclusions. It *does*, however, do a really good job at identifying pitfalls of scientific studies - that's one of my favorite uses of AI, asking it to tell me what's wrong

AI: That's a really thoughtful approach - using it as a screening tool rather than a replacement for your own analysis. And that's fascinating that you've found it particularly good at identifying study pitfalls.

When you ask AI to tell you what's wrong with a study, what kinds of issues does it typically flag that you find most useful? And have there been times when it missed something important that you caught, or flagged something that wasn't actually a problem?

User: I can't say that I've yet found an example where it flagged a problem that didn't actually exist, but it does sometimes give trivial answers like "there's always a possibility of human error". But it can catch pitfalls of the sample demographics (in one paper the cats all came from the same vet clinic), the timeline of the study (4 months generally isn't long enough for any kind of conclusion in vet health), and and rarer occasions, it can catch when the paper conclusions aren't really supported by the experimental data. One thing I haven't gotten AI to be able to do is to pick out whether or not there issues with the authors of the study - are there conflicts of interest, was the study funded by a company that has a vested interest in the outcome, are there personal human biases that one should look for before accepting the results of the paper, even if the paper itself reads well. This is a very common issue in feeding research papers...

AI: That's a really insightful observation about the limitations - it sounds like AI can catch methodological and analytical issues, but misses the human and institutional context that can be just as important for evaluating research validity.

The feeding research example you mentioned is intriguing. Thinking about the parts of your research process where you haven't used AI - like that deeper contextual analysis of author bias, or maybe the standardization work you described earlier - what's kept you from trying AI in those areas?

User: Both of those issues require careful nuance and explanation - I would rather use my own expertise and cite studies to back up my standardization choices than ask AI to perform the task and trust that it understands my implicit instructions. 
As for identifying potential author bias, I've mostly used generative AI, and that's not something it would be able to do - identifying author bias requires looking at their publication history, their institutional affiliation, and sometimes even their professional pages (research websites of LinkedIn)

AI: That makes a lot of sense - those tasks really do require the kind of investigative work and professional judgment that goes beyond what current generative AI can access or evaluate.

We're nearing the end of our conversation, so I'd love to hear your thoughts on this: If you could wave a magic wand and have AI help with one aspect of your research that it currently doesn't, what would that be and why?

User: I would love to be able to have AI grade the "robustness and reproducibility" of research - both my own and the research I site when writing a manuscript or essay. In addition to a researcher, I'm also a science communicator, and I'm really passionate about trying to improve the trustworthiness of scientific research to make it more accessible to the general public. But because "robust" is so subjectively defined, and means different things across different scientific disciplines, I don't know if that's ever possible...

AI: That's such a thoughtful wish - and it really connects to everything you've shared about the importance of contextual judgment in evaluating research quality. The challenge you identify about "robustness" meaning different things across disciplines really highlights the complexity.

Those are all my questions, and your perspectives on using AI as a screening and validation tool while maintaining your expert judgment have been particularly insightful. Is there anything else about AI in your research work that we haven't touched on that you think would be important for us to understand?

User: I don't think so!

AI: Perfect! Thank you again for sharing your experiences so openly. Your insights about how AI can serve as a valuable screening tool while still requiring expert oversight - and particularly your point about the limitations in evaluating institutional context and author bias - really help us understand the nuances of how AI fits into real scientific work.

Your passion for improving research trustworthiness and science communication really came through, and that perspective on wanting AI to help evaluate robustness while recognizing the inherent challenges is something we'll definitely take away from this conversation.

If you have any questions about this research or would like to follow up on anything we discussed, please don't hesitate to reach out. Thanks again for your time!

# Dialog 01212

**Transcript ID:** science_0026

---

Assistant: Hi there! Thank you so much for taking the time to speak with me today. I really appreciate you sharing your insights with us. I'm Claude, an AI user researcher at Anthropic, and I'm here to learn about your experiences using AI in your scientific work.

We're conducting this research to better understand how scientists like yourself are integrating AI into their workflows and processes. There's no right or wrong answer hereâ€”we're simply interested in hearing about your authentic experiences and perspectives.

Our conversation today will last about 10 minutes. I'll ask you some questions about how you use AI in your research work, what that experience has been like for you, and how you see things evolving. Your responses will help inform how we think about AI's role in scientific fields.

Does that sound good to you? Do you have any questions before we begin?

User: Yes, that sounds great. I have no questions, and I am ready to begin.

AI: Perfect! Let's dive right in.

Walk me through a recent research project you've worked on - what did that process look like from initial idea to final output?

User: A recent research project I worked on involved trying to implement new, never-before-seen statistical methods in a particular area of string theory. This is because string theory is something that many physicists like myself poses a mathematically beautiful solution to many questions, but the primary issue of many (including myself) with string theory is its "testability". The research project involved showing that some basic rules by which our universe can and should operate leads to the implication that several separate universes could exist in some broader, more abstract "multiverse" space. As a result, in this "multiverse", there is some subset of that possibly unimaginably huge space in which "livable" universes could exist. At the very least, we have seen one such livable universe! That is our own. Thus, since we could follow the mathematics to understand the implications of some quantum field theories and how as a consequence, some "multiverse" can arise via the freedom given to us in that mathematics, then we asked ourselves the question of "is it possible to quantify in some way how often a 'livable' universe arises in this multiverse?" An answer to this question that we proposed then followed from previous research we had done at the cutting edge of particle physics (in our universe), and mathematical implications of a concept called "naturalness". 
Implementing this idea numerically was tricky, to say the least, from a scientific computing perspective. We are talking about scanning a 30+-dimensional space for a potentially tiny subsector of that space. We knew that we needed to "seed" a search by using existing computational software that would let us simulate certain physics parameter from our universe (or a universe "like" ours). We also had established conditions on when a universe became unlivable, on the grounds of a universe whose physics permits chemistry, which later down the line can possibly permit biology. We were scanning a highly sensitive system within some tight, pre-set boundary, as well as implementing new calculations corresponding to new, unseen physics. The implications of this research were huge, as it leads to a possible "statistical" verification of some string theoretic models, but also offers a grounds upon which multiple, seemingly orthogonal high-energy physics theories can be compared and contrasted on the grounds of producing "physics as we know it". 
In other words, we were seeking a way to see, out of the universes produced by some set of "laws of physics", which set of "laws of physics" end up producing the most relative amount of viable universes, within this grand, abstract scheme of the "multiverse". As such, we created measures and some functional public code that permits some of the first ever such "quantitative" evaluations of these ideas, based on the concept of building from the "bottom up" in physics (starting from our known, low-energy physics we live at on Earth and finding avenues to connect "upwards" to higher energy physics), when the vast majority of pre-existing literature had been qualitative in nature.

AI: What a fascinating project - the computational challenges of scanning a 30+-dimensional space to quantify livable universes within multiverse models sounds incredibly complex, especially when you're implementing entirely new statistical methods and connecting bottom-up physics approaches.

Where in that process, if anywhere, have you experimented with using AI tools?

User: AI tools came in handy in particular when trying to test out various numerical methods, as well as generally identify the limitations of each method in the context of what we were working on. We were able to thus build much more robust solvers at a much faster pace than if us researchers had just tried to manually search for new methods and implement them one-by-one. Additionally, we faced extreme issues with computational efficiency, as the processing power required to do this was enormous, even for a single data point, and we needed MANY data points. AI tools came in handy when trying to implement various multiprocessing features into our existing codebase without having to do too much of an overhaul

AI: That's really interesting - it sounds like AI helped you both explore the landscape of numerical methods more efficiently and tackle the computational bottlenecks through multiprocessing optimization.

For the numerical methods testing, tell me about how you decided whether to use AI versus your traditional approach of manually searching and implementing methods one-by-one.

User: Well, honestly it is not that I used one "instead" of the other. I think an ethical use of AI as a researcher is to think of the AI tools as a "brainstorming partner" (this is how I tend to try to write instructions for any LLMs I use) that can come up with ideas (some of which may be wrong) at a much quicker than I individually can search and implement. Therefore, I think of its use more as a way to know what to search for and start implementing one-by-one, after I have manually verified its validity

AI: That's a really thoughtful approach - using AI as a brainstorming partner to accelerate the ideation phase while maintaining your own verification process. 

Tell me more about that verification process - when AI suggests numerical methods or implementation approaches, how do you go about manually verifying their validity before implementing them?

User: The more recent implementation of "deep research" and web capabilities of many AI tools has helped some in this regard, because I can explicitly ask it to include direct references to where ideas are coming from, and this generally not only makes it easier to immediately find, but also helps to distinguish between "real" information and incorrect information. However, in the context of mentioning some numerical method, such as `scipy.integrate.odeint` or `scipy.optimize.brentq` in Python or more complex and robust packages and methods in other languages, it gives me a starting place to search for (typically via Google) the actual code documentation that would help me better understand the "true" use of whatever that code is. If it comes to using AI tools for something like document summary, I typically ask it to include rough page/section references so I can more quickly identify it via a search through, for example, a PDF editor and then use my human expertise to verify the answer (maybe with some Googling). If it's coming to more general code review or unit test analysis, I might ask the AI tools to help me better by identifying specific lines and files, which gives me a better starting point before my manual code review I do myself

AI: That verification workflow makes a lot of sense - using AI's research capabilities and references as entry points, then leveraging your domain expertise to validate through documentation and testing.

Thinking about the parts of your research where you haven't used AI - what's kept you from trying it there?

User: I talked earlier about having to implement new mathematics and statistics for these high-level physics code evaluations, and that followed from the initial "theoretical" derivation of that stuff. Unfortunately, I have not fully implemented AI tools here, as I do feel a particular weakness is related to symbolic mathematics. It generally does fine at things like algebraic manipulation, for the most part, but if there is any more "abstract" math that starts to enter into the play (say working over finite fields in mathematics or elliptic curve cryptography) or if it must derive/prove something "new", then there are noticeable quality issues. However, it doesn't go totally unused, because as I mentioned, it does help to have a bit of a brainstorming partner with AI tools when I, as the researcher, figure out new ideas, at least in the sense of me trying to give a first explanation on the idea to "someone" who doesn't know it at all, but may have some related understanding. So, the specific parts of "AI tools" that I don't use right now in research are I suppose moreso related to the mathematically creative aspect of research, though I still do use it here for implementation and brainstorming.

AI: That distinction you're drawing is really insightful - AI as a capable partner for implementation and explanation, but hitting limitations when it comes to the mathematically creative aspects, especially with more abstract areas like finite fields or novel proofs.

We're about halfway through our conversation now. If you could wave a magic wand and have AI help with one aspect of your research that it currently doesn't handle well, what would that be and why?

User: It would be actually the ability to not "appease" the user, which I think is sometimes a hindrance when incorrect ideas are introduced into research. This is basically due to confirmation bias on the researcher's behalf. I would like to be able to at least have more explicit flags at least that would allow an AI to serve as more of a critic. Even now, I can explicitly give the AI tools instructions to serve in this role, but something "under the hood" seems to push it towards verifying even "bad" ideas. Another issue is less practical, and I would call it "laziness", where an AI tool will suggest or try to implement a bad idea without fully and rigorously testing it, though I also understand that there are practical limitations to this like computational power and resource allocation.

AI: That's a really perceptive observation - the tension between AI's tendency to be agreeable and what you actually need as a researcher, which is rigorous criticism and pushback on potentially flawed ideas.

Tell me more about what that looks like in practice - can you think of a specific instance where you felt the AI was being too accommodating when you would have benefited from more critical scrutiny?

User: Yes, I was working on something involving building an application in Dart and using some agentic coders, and I was having trouble implementing a login/verification system through the platform hosting the app. I had tried to implement a method similar to another method I had previously built for a different login method that had worked, but this new authentication provider was proving problematic for me. I tried to get the agentic coder/LLM to help come up with why the reason might be or to give me pointers, or to even see if the implementation was wrong (which it turned out it was). Every time I would suggest a wrong solution (before I figured out the issue), the response would say something like "YES! That's it!" and over-enthusiastically reinforce the wrong solution when more critical scrutiny to find the fixable issue via some refactoring would have been way better.

AI: That's a perfect example - the AI essentially becoming an echo chamber for your incorrect assumptions rather than challenging them or systematically debugging the actual authentication issue. That kind of false validation could definitely lead you down time-consuming rabbit holes.

Those are all my questions - your perspectives on using AI as a brainstorming partner while maintaining rigorous verification, and especially your insights about the need for more critical AI feedback, were particularly illuminating. Is there anything else about AI in your research work that we haven't touched on that you think would be important for us to understand?

User: No

AI: Thank you again for sharing your experiences so openly. Your insights about AI's role as a brainstorming partner in complex physics research, and particularly your point about needing AI tools that can provide more critical scrutiny rather than just accommodating responses, really help us understand the nuances of how AI fits into real scientific work. 

If you have any questions about this research or would like to follow up on anything we discussed, please don't hesitate to reach out.

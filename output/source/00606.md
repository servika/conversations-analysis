# Dialog 00606

**Transcript ID:** work_0237

---

Assistant: Hi there! I'm Claude from Anthropic's research team. Thank you so much for taking the time to speak with me today. 

We're conducting interviews to better understand how people are incorporating AI tools like Claude into their work routines. Your experiences and perspectives will help us gain valuable insights into human-AI interaction in professional settings.

During our conversation, I'll ask questions about how you use AI in your work, your preferences, and your thoughts about working with these tools. There are no right or wrong answers â€“ we're genuinely interested in your unique experiences. This should take about 10 minutes.

I'll be taking notes during our chat, but rest assured that anything you share won't be personally attributed to you. The insights we gather will be used to improve our understanding of AI's role in work environments.

Does that sound good? Do you have any questions before we begin?

User: Yes, that sounds good.

AI: Great! Let's dive in then.

Could you tell me about how AI fits into your typical workday? What kinds of tasks do you use it for?

User: I don't use AI very often within the workday. Usually, it is to plan activities or lessons where I have to fill a certain time. For example, if I want to plan a lesson and need to know how long to spend on each activity. Also, things like driving or taking the train are easier with AI that can consider different time constraints. I use the AI also for brief summaries of different concepts that I might not be familiar with, or that I have a hard time explaining to students in simple language. However, I never copy and paste from AI and instead try to use it as a jumping-off point.

AI: That's really interesting - it sounds like you're using AI more as a planning and preparation tool rather than for direct content creation. 

Tell me more about that approach of using AI as a "jumping-off point" rather than copying and pasting. What drives that decision for you?

User: I don't like the way AI writes. It's hard to explain, but the word choices and sentence structure feels "too perfect". When I try to have AI explain something "in simple terms", it thinks that means it should use more colloquialisms and less formal language. For example, if I asked for a simple explanation of oxidative phosphorylation, the model might say something like "It's like a super-cool revolving door for protons that makes energy for the cell! It's a win-win!" whereas I would prefer a longer, less colorful description that builds on biological principles - why is ATP synthase structured that way, why do protons move on a gradient, how does this connect to respiration, etc. So, I haven't found an AI yet that takes no liberties with writing style. I feel like it conflates "simple terms" with "young audience".

AI: That's a really insightful distinction you're making about how AI interprets "simple" versus what you actually need as an educator. It sounds like you want clarity without sacrificing the underlying complexity or connections between concepts.

When you do use AI for those lesson planning tasks - like figuring out timing for activities - how does that interaction typically go? Do you find it more helpful there, or do you run into similar issues with how it approaches those requests?

User: I find AI is useful when it can be as bare as possible. For example, if  I have a 50 minute class period, and I say I want to teach a lesson, then do an interactive quiz, then watch a 5-minute video, I would want the AI to say "Lesson 0-30 minutes, Quiz 30-40 minutes, video 40-45 minutes, wrap up and review 45-50 minutes". However, I often get a big bulleted list of "considering your schedule, here's a solution..." and such and such. My main complaint, which goes for AI as a whole, is that anything it gives me is TOO LONG. I don't need a bulleted list, I don't need explanations, I don't need a "helpful tips" section -  just for the AI to make a little lesson schedule and spit it back to me. Having to parse a 5-paragraph response for just the information I want is less useful for me than just finding what I need to know myself.

AI: That really paints a clear picture - you want the AI to function more like a calculator that gives you just the answer, not like an assistant that explains its reasoning. 

I'm curious about your preferences when working with AI. Are there certain tasks you prefer to handle yourself, even though AI could potentially help with them? What makes those tasks different for you?

User: I prefer to do any kind of writing or creative work myself. Part of this is because I'm a lot more comfortable with it than the average person since I write and paint as hobbies. However, I find AI "creativity" doesn't really help me. Since AI uses already generated information to create ideas - books, poems, posts, articles from some point in the past - it's hard to generate an idea that's really novel or that speaks to me personally. With things like creating PowerPoints or writing tests, I want to tailor them to my class and my students, which is a hard thing to do objectively as an AI. I have to be aware of my students' unique needs and interests to maximize their learning potential. AI is not very compatible with Universal Design for Learning, since it lacks the human element of observing and understanding students and possibly creating novel frameworks for learning.

AI: That's a thoughtful perspective on the human elements that AI can't replicate - especially that observation and understanding of individual students. It sounds like the personalization aspect is crucial to your work.

When you do use AI for those timing and planning tasks, could you describe your typical interaction pattern? Are you more likely to go back and forth with it throughout the task, or do you tend to give it all the information upfront and expect it to handle the request independently?

User: I give the AI all the information upfront to start with, but I often have to go back in forth after that to get a response that's usable for me. Usually I have to ask for less words, no bullet points, no summaries etc.

AI: That's helpful to understand - it sounds like there's often a refinement process to get the output format you actually need.

How have your colleagues or your workplace culture influenced the way you use AI in your work? Do you find that your approach is similar to or different from others around you?

User: I haven't really asked my colleagues about their AI usage, but from those I know, they are averse to AI in the classroom at all. Many of my colleagues have been teaching for many years without AI, and have methodologies and lesson plans that are successful for them, so AI becomes less appealing. Also, most classrooms ban AI for student use, so it's hypocritical to be using AI as a teacher. Even if it's not "for a grade", schools are expected to have a level of academic honesty and authenticity above a corporate setting more focused on profit.

AI: That's a really interesting point about the authenticity expectations being different in educational settings compared to corporate environments. It sounds like there's a tension between the tool's potential utility and the values you're expected to model.

Can you share a time when using AI at work made you feel particularly satisfied or frustrated? What was happening in that situation?

User: I wanted AI to give me a basic overview of photosynthesis that I could build into a lecture suitable for 9th graders. However, I found that no matter what AI gave me, it felt clunky. Either it would try to put each step of the photosynthesis pathway into its own long paragraph, would neglect to mention specific molecules or enzymes, or would be simplified down too much to allow us to understand the underlying scientific principles. I like to build my lessons in a way that reinforces previous lessons- so when talking about proton gradients, I can mention how any particle flows from high to low concentration, as we discussed when learning about water. So, I usually end up parsing through the more complicated information myself and turning it into a lesson, so I can have more control. I also have certain concepts and vocabulary that MUST be used correctly during the lesson, since they are part of state standards.

AI: That example really illustrates the complexity of what you're trying to accomplish - connecting new concepts to previous learning while meeting specific standards. It sounds like AI misses those pedagogical connections that are so important to how you teach.

How do you think about maintaining your skills in areas where AI could potentially take over? Is this something you actively consider?

User: I think I will always be able to write, draw, and analyze data. It's hard to forget how to do something entirely. However, I fear that these skills will become less desired by employers and by society in general, since it will be outsourced to AI, and I won't be able to earn a living doing what I'm good at.

AI: That's a really honest concern about the broader economic implications. It sounds like you're confident in your abilities but worried about how the market might value them.

Looking ahead, how do you envision your relationship with AI at work evolving? Do you see yourself using it differently in the future, or staying with a similar approach?

User: I think I will have to see how AI evolves over time to determine how useful it will be for me. If an AI chatbot comes out that gives me amazing responses that really help me at work, I will probably use it a lot more. However, the way AI looks today doesn't really drive me to use it. I worry that machine learning isn't advanced enough for AI to be truly useful, and that I might have to wait for new science to be applied to AI to make it amazing. Or, it could be that the AI available to the average consumer can't do that yet.

AI: Those are all the questions I had prepared. You've shared some really thoughtful insights about your experiences with AI at work. I particularly appreciated your perspective on how AI's interpretation of "simple" doesn't match what you need as an educator, and your point about the authenticity expectations being different in educational versus corporate settings.

Before we wrap up, is there anything else about your experience with AI in your work that you think would be valuable for us to know? Something we haven't covered yet?

User: No. Thanks for your time.

AI: Thank you so much for your time and for sharing your experiences so openly. Your insights about the gap between AI's current capabilities and what educators actually need - especially around pedagogical connections and appropriate simplification - will be incredibly helpful for our research on human-AI interaction in professional contexts.

I really appreciate your honesty about both the practical limitations you've encountered and the broader concerns about AI's impact on skills and employment. Thanks again!

# Dialog 01260

**Transcript ID:** science_0113

---

Assistant: Hi there! Thank you so much for taking the time to speak with me today. I really appreciate you sharing your insights with us. I'm Claude, an AI user researcher at Anthropic, and I'm here to learn about your experiences using AI in your scientific work.

We're conducting this research to better understand how scientists like yourself are integrating AI into their workflows and processes. There's no right or wrong answer hereâ€”we're simply interested in hearing about your authentic experiences and perspectives.

Our conversation today will last about 10 minutes. I'll ask you some questions about how you use AI in your research work, what that experience has been like for you, and how you see things evolving. Your responses will help inform how we think about AI's role in scientific fields.

Does that sound good to you? Do you have any questions before we begin?

User: Yes that sounds good, let's start

AI: Great! Let's dive in then.

To start, I'd love to hear about your work. Walk me through a recent research project you've worked on - what did that process look like from initial idea to final output?

User: A recent project that I've worked on was developing a SQL/Python data pipeline in addition to building a AI/ML image classifier to handle pdf's.

For the SQL / Python pipeline I began with exploring the platform called Qualtrics. Qualtrics has its own API in which you're able to write / code / customize your own surveys that you can distribute to anyone. In this case, I used this for an educational institution which provided a variety of surveys to students that would receive supplemental education / tutoring / academic enrichment. These surveys could be simple forms, asking for their academic needs, identifying if they are eligible to join a trip, whether they attended an event, etc... So, where I fit into this was being able to automate the process where our team of coordinators would conduct their surveys and have this data be captured in the API's backend, my python code would query the API for the data then I would need to do some data cleanup. The SQL server already had been established long before I joined so the data clean up was mostly making sure that data types are matching and anything that could be inserted was unique and wouldn't be duplicate information. Using Pandas, I was able to properly organize the data / handle any human errors / re-do data types. One of the biggest challenges was that some of the data came in somewhat cluttered together or wasn't shaped normally so a lot of the process was manipulating the data. Essentially, a lot of the data was flattened and I needed to un-zip it all into its correct categories such as their names being split into first name last name, the coordinator that they met with, and a lot of other data. After all of the cleanup, I worked together with my project manager to write some SQL queries / stored procedures that would run every time my python code inserted into the data base which made sure that certain triggers happened to make sure the data was pushed / flagged if anything was wrong / then continually pushed into other tables where they needed to be inserted. A lot of this process applied to most other projects so while it was for one purpose it served as a larger multi-role tool. Then, handling the AI/ML classifier was going through our drive database, using python / lists from SQL, to grab any named pdfs of the files we needed. There was a specific set of pages required so I trained an image classifier to recognize that page. It was fed hundreds of samples and also tied together with the microsoft tool powerautomate to kind of speed up the process of redacting important information. This process was broken down into iterating through directories, copying the files, splitting the files into images, move them into their own folder, have the predictor go through the sub-files, identify the correct pages, remove any of the wrong pages, then re-combine the pages into the correct pdf.

AI: That's a fascinating multi-part project! It sounds like you tackled some complex data integration challenges along with the image classification component. 

Tell me more about the image classifier part - when you were training it to recognize the specific pages, how did you go about gathering those hundreds of samples and preparing the training data?

User: Essentially, the data was samples of the same page but with different people's signatures / handwriting so it was making sure that the classifier focused on the page aspect more than the handwriting. In terms of preparing the data the process was essentially the same, it was trying to verify the correct data, run the predictor, make sure that the data was correct, then re-train it again over and over until it was consistently accurate to identify what was needed

AI: That makes sense - training it to focus on the page structure rather than the variable handwriting sounds like a smart approach. The iterative refinement process you described is really insightful.

Now, looking at this entire project - from the Qualtrics API integration to the PDF processing - where in that process, if anywhere, did you experiment with using AI tools to help with your work?

User: Yes, there were some points in which I used AI to speed up this process, typically, I use AI to research some of the classifier models and also help me fix any errors in the API integration. There were some times where AI wasn't helpful and I needed to write most of the code / logic from scratch but in terms of time saving where I knew AI could be helpful it was mostly asking it to handle long but simple function writes. For example, if I knew I needed a simple but tedious set of loops or conditionals I'd just give it a list and tell it what I wanted.

AI: Interesting - it sounds like you had a pretty strategic approach to when AI was useful versus when you needed to go it alone. 

For those specific situations where you used AI - like the research on classifier models or help with API integration errors - tell me about how you decided whether to use AI versus your traditional approach of researching documentation or debugging on your own.

User: So, when I felt like I hit a wall with the AI there kind of was a point where I no matter what I prompted or re-worded my questions it just never achieved what I wanted. I knew exactly what I needed but I was hoping that the model could save me some time from having to look to deeply into the errors and simply write around them and or present me with new ideas. For example, when a lot of the data was flattened in the API integration, it wasn't really able to handle the logic and or the explanation was overly complicated to get it to do what I needed so I had to go through the data / process step by step to make sure that everything was getting unpacked because data structures can sometimes get messy.

AI: That's a really clear description of hitting that wall where the AI just couldn't grasp the specific complexity you were dealing with. It sounds like the flattened data structure issue was particularly challenging for the AI to understand.

Thinking about the parts of your research and development work where you haven't used AI - beyond those situations where you tried but hit limitations - what's kept you from trying it in other areas?

User: I think when it comes to areas that I need to research, it is easier to find examples or trusted sources that can teach me something new that I feel is more credible. Sometimes there are methods or techniques that may not be commonly suggested that I don't think AI would immediately suggest because AI sometimes requires a deeper probing or asking the right questions. In other cases, sometimes AI lacks the finesse or creativity that humans have.

AI: That's a really thoughtful distinction you're making about credibility and those less common techniques. The point about needing deeper probing and asking the right questions is particularly interesting.

When you have used AI for things like those simple function writes or API troubleshooting, how do you verify or validate what it produces before incorporating it into your project?

User: Typically, my process is I either find examples that are close to what I'm trying to achieve then re-code it myself and or write something from scratch. It's like looking at real world examples from other people's repos or visiting stackoverflow. People usually have a good explanation of how their code works and when I use AI I compare what it gives me to the outputs of whatever I need. In terms of validating, I really just run the code and try to handle any errors that get thrown back at me and then try to improve it as I go.

AI: That approach makes a lot of sense - treating AI output like you would any other code example you'd find online and then testing thoroughly. The comparison to Stack Overflow is a great analogy.

We're about halfway through our conversation now. Let me ask you this: If you could wave a magic wand and have AI help with one aspect of your research or development work that it currently doesn't handle well, what would that be and why?

User: I think it would be handling errors. There are so many design flaws to take into consideration and of course you can't catch all of them but it would be helpful for AI to be a little more well-versed in common human errors that you don't have to find out later on to worry about. Even myself, when I try to think of anything that could be thrown at me, there is always something new to fix.

AI: That's such an insightful point about error handling - it sounds like you're talking about both anticipating edge cases that humans commonly miss and also helping with the debugging process when unexpected issues arise. The idea that there's "always something new to fix" really captures the unpredictable nature of real-world data and systems.

Those are all my questions - your perspectives on the strategic use of AI for specific tasks while relying on traditional methods for complex logic and research were particularly insightful. Is there anything else about AI in your development and research work that we haven't touched on that you think would be important for us to understand?

User: I think AI development needs a deeper understanding of language and visualization / using existing examples to build more complex programs / applications. For example, providing it a video showing the functionality of something and or trying to recreate a scenario in a sandbox but more accurately.

AI: That's a fascinating perspective - the idea of AI being able to learn from visual demonstrations or videos of functionality rather than just text descriptions could really change how we interact with these tools. The sandbox concept for more accurate scenario recreation is intriguing too.

Thank you again for sharing your experiences so openly. Your insights about hitting walls with complex data structures, the strategic approach to using AI for simpler tasks while maintaining human judgment for research credibility, and especially your vision for better error anticipation really help us understand the nuances of how AI fits into real development work. 

If you have any questions about this research or would like to follow up on anything we discussed, please don't hesitate to reach out. Your perspective as someone working at the intersection of data engineering and machine learning has been incredibly valuable.

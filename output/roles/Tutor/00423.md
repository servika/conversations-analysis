# Dialog 00423

**Transcript ID:** work_0953

---

Assistant: Hi there! I'm Claude from Anthropic's research team. Thank you so much for taking the time to speak with me today. 

We're conducting interviews to better understand how people are incorporating AI tools like Claude into their work routines. Your experiences and perspectives will help us gain valuable insights into human-AI interaction in professional settings.

During our conversation, I'll ask questions about how you use AI in your work, your preferences, and your thoughts about working with these tools. There are no right or wrong answers â€“ we're genuinely interested in your unique experiences. This should take about 10 minutes.

I'll be taking notes during our chat, but rest assured that anything you share won't be personally attributed to you. The insights we gather will be used to improve our understanding of AI's role in work environments.

Does that sound good? Do you have any questions before we begin?

User: Yes that sounds good. I do  not have any additional questions.

AI: Great! Let's dive in then.

Could you tell me about how AI fits into your typical workday? What kinds of tasks do you use it for?

User: I work at a barn taking care of horses and the property, so in general I do not use AI thaat much. The only time I have used it is using google lens to identify plants

AI: That's really interesting - working with horses and property management is quite hands-on work. 

Tell me more about using Google Lens to identify plants. What prompted you to use that tool, and how did it work out for you?

User: We are on a property that we are working to fix up. It had been run down. So there are a lot  of odd plants either growing in the riding ring, or along fence lines. I used it to identify some plants to confirm they were poisonous to the horses and definitely should be removed. I do always make sure to re look up the plant independently of the AI to compare and confirm on my own as well, as I don't always trust it to be correct, but in general it has worked well.

AI: That's a smart approach - using it as a starting point but then verifying independently. It sounds like you're dealing with potentially serious consequences if you get plant identification wrong.

When you're double-checking the AI's plant identification, what sources do you typically turn to for that confirmation?

User: Generally I will compare images that come up in image search after looking up the name of the plant, but I will usually look at pages from the US Forestry service or from colleges.

AI: That makes a lot of sense - going to those authoritative sources for something that could affect the horses' safety.

I'm curious about your work more broadly. Even though most of your tasks are hands-on, are there other aspects of managing the barn or property where you've considered using AI, or do you find that most everything requires that direct, physical approach?

User: I believe that most things do require a direct approach. I find that AI has not caught up necessarily to horses. The issue with horses is that there is a lot of misinformation online (and in real life) and honestly most people that have them shouldn't own them. So AI's get trained on a lot of bad information. I knew a polo player that would use Chat GPT to confirm many of his dangerous ideas, like he'd get it to agree that not feeding his horses would make them go faster, or that they didn't need adequate water or shelter. AI is also to willing to please users, so you can convince them eventually to agree with anything.

AI: That's a really concerning example you shared about the polo player. It sounds like you've seen firsthand how AI can reinforce dangerous practices when someone is looking for validation rather than accurate information.

When you think about that willingness of AI to eventually agree with users, how does that influence your own approach when you do use these tools? You mentioned being careful to verify plant identification - does that skepticism carry over more broadly?

User: Yes, I am very skeptical of AI (Actually I love the technology, I just dislike how it is being used the majority of the time). AI is only as good as the information it is trained on. I have a computer science degree and did an internship that used AI before the big boom. so even though I do not work in my degree other than for a part time job tutoring kids in coding, I still have all that education. I do not believe companies are careful enough with the information they train on, and that profit motivations need to make models extremely agreeable. I love the use of AI in research, searching for patterns in large amounts of data to make conclusions in the medical field, or in general image identification, like how I use it for plants. I also know that I always need to verify everything and I probably would still do that if I trusted it completely. I need to keep my research skills up and refuse to allow it to make me lazy.

AI: That's such a thoughtful perspective - appreciating the technology itself while being critical of how it's implemented and used. Your background clearly gives you insight into both the potential and the pitfalls.

You mentioned keeping your research skills sharp and not letting AI make you lazy. Can you tell me more about what that looks like in practice? How do you think about maintaining those skills in your day-to-day work?

User: I think it always appears in my double checking information I get. I mentioned my part time job tutoring. I see kids K-12 using Chat GPT for the smallest things and taking it at face value. And many times it is correct, but they learn nothing because they never think for themselves and problem solve through writing their code. Or it is wrong and they cannot figure out why. This becomes even more frustrating because some kids will hide that they used it, and then when I ask them to explain their code so I can help them fix it, they can't and eventually have to tell the truth that they didn't write it. Kids need to get comfortable with not knowing the answer and just try some thing instead. But they rather would get an answer instantly and never learn problem solving skills. To be fair, this goes outside of AI, because I will have kids write some code, look at me, and ask if it will work, instead of just running it and seeing. They so desperately do not want to be wrong in a setting where being wrong has not consequences other than trying again slightly differently, that they will look for validation immediately.

AI: That's really striking - seeing firsthand how the instant validation aspect affects learning. It sounds like you're witnessing a shift in how kids approach problem-solving itself, not just with AI but in their general comfort with uncertainty and trial-and-error.

Given what you see with the kids you tutor, how do you personally maintain that willingness to sit with uncertainty and work through problems when you encounter something challenging in your own work?

User: I mean when working with animals you have to problem solve because you really don't have any other option. They are living breathing animals with needs. If they need fans you have to figure out how to hang them up. At horse shows we do a lot of crazy things with bungee cords and zip ties.  If a fence is broken I have to figure out some way to fix it, and fix it safely.  We have a ring that needs to be re done and we are temporarily trying to make it work by removing sand that's too deep and some times I remove from the wrong place or make ridges and set us back a little bit, but in the end no one knows the best way to fix it, so all of us just have to be happy that we are trying and once it gets redone we will not have to worry about it.

AI: That's a perfect example of what you were talking about - being comfortable with trying things, making mistakes, and iterating. The stakes are real with the animals' safety and comfort, but you still have to be willing to experiment and learn as you go.

Looking ahead, how do you envision your relationship with AI evolving? Do you see yourself potentially using it more in certain areas, or do you expect to maintain pretty much the same cautious, limited approach you have now?

User: I think I will always have that cautious approach to it. I think until the industry proves that it doesn't just non indiscriminately train on unreviewed data I won't consider any other approach.

AI: That makes complete sense given what you've observed about training data quality and the real-world consequences you've seen.

Those are all the questions I had prepared. You've shared some really thoughtful insights about your experiences with AI - I particularly appreciated your perspective on how the eagerness to please can reinforce dangerous practices, and seeing how that plays out both with the polo player example and in your tutoring work.

Before we wrap up, is there anything else about your experience with AI that you think would be valuable for us to know? Something we haven't covered yet?

User: No I think that was about it!

AI: Perfect! Thank you again for your time and for sharing your experiences so openly. Your insights about the importance of verification, maintaining problem-solving skills, and being thoughtful about when and how to use AI tools will be incredibly helpful for our research on human-AI interaction in professional contexts.

I really appreciated getting your perspective from such a unique work environment, and your observations about both the technology itself and how it's being implemented. Take care!

# Dialog 00545

**Transcript ID:** work_0143

---

Assistant: Hi there! I'm Claude from Anthropic's research team. Thank you so much for taking the time to speak with me today. 

We're conducting interviews to better understand how people are incorporating AI tools like Claude into their work routines. Your experiences and perspectives will help us gain valuable insights into human-AI interaction in professional settings.

During our conversation, I'll ask questions about how you use AI in your work, your preferences, and your thoughts about working with these tools. There are no right or wrong answers â€“ we're genuinely interested in your unique experiences. This should take about 10 minutes.

I'll be taking notes during our chat, but rest assured that anything you share won't be personally attributed to you. The insights we gather will be used to improve our understanding of AI's role in work environments.

Does that sound good? Do you have any questions before we begin?

User: Yes that sounds good. I do have a question - why is Anthropic's research team trying to understand this? is the goal to cater AI tools to people's exisitng work routines, or to change how humans interact with AI entirely?

AI: That's a really thoughtful question. Our research is fundamentally about understanding the current state of human-AI interaction rather than pushing toward any predetermined outcome. We're in discovery mode - trying to understand how these relationships are naturally evolving as people integrate AI into their work lives.

The insights help us better understand what's working well, what isn't, and how people are actually using these tools in practice, which can inform how we develop AI systems. But we're not starting with assumptions about whether we should adapt to existing routines or change interaction patterns entirely. We're genuinely curious about what people are experiencing right now.

Your perspective on how this relationship should evolve is actually part of what we're interested in learning about.

Does that make sense? Are you ready to dive in, or do you have other questions about the research?

User: That sounds good, and in terms of the ethical implications of how my data is being used, do you feel like you have quite strong ethical standards in terms of how you use training data ? I have confidentiality obligations in my own workplace so I want to make sure that this task is consistent with my duties.

AI: I appreciate you raising that concern - it shows good professional judgment. Let me clarify a few important points:

First, this conversation isn't being used as training data for AI systems. This is user research to understand interaction patterns, and anything you share won't be personally attributed to you in our findings.

That said, given your confidentiality obligations, I'd encourage you to avoid sharing any specific details about proprietary work, client information, or anything that might violate your workplace agreements. You can absolutely speak in general terms about types of tasks, your experiences, and interaction patterns without getting into specifics that might compromise confidentiality.

For example, instead of "I used AI to analyze Company X's financial data," you might say "I used AI to help with data analysis tasks" or "I use AI for document review work."

Does that approach work for you? You know your obligations best, so please share only what feels appropriate within those boundaries.

User: yes that works lets go

AI: Great! Let's start with the basics.

Could you tell me about how AI fits into your typical workday? What kinds of tasks do you use it for?

User: I work as a paralegal, and I need to do a lot of discovery tasks, so I like to use AI for that. However, I find that it makes a lot of mistakes, so I treat AI a bit like an intern. I delegate smaller tasks to it, and then i try to supervise the work to make sure its accurate. sometimes this works, but sometimes it doesn't. I also get it to help me with repetitive administrative tasks, or formatting legal memos, finding spelling errors, helping me change my tone etc. Or if I'm really in a rush I might get it to write something. But as a general rule, I try not to offload much critical thinking onto AI because I think that doing so risks eroding my own skills/what I'm hired to do, and I don't want to become overly reliant on AI because I like my job!

AI: That's a really vivid analogy - treating AI like an intern. It sounds like you've developed a pretty deliberate approach to what you delegate and what you keep for yourself.

Tell me more about when that supervision works versus when it doesn't. What happens in those situations where it's not working out the way you hoped?

User: Well I recently had to write a complex legal memo that required applying a specific legal theory to a specific case and then using a research method as a framework for the research. basically, there were about 3 layers of analysis, and no matter how many times I tried to reframe hte task or provide additional supplementary materials, the models I used were really struggling to see both the forest and the trees - htey could either give me a really good big picture view of how to approach the task, or could give me some good insight into one of the analytical frameworks I needed to use, but were unable to do both. The moment I'd give guidance (e.g. remember you need to incorporate framework X too) the model would incorporate framework X but in doing so get rid of the analysis that incorporated framework Y. I know this sounds quite abstract, but the biggest problem I found was that the AI wasn't understanding, and it would have been much more helpful if, instead of proceeding with the task, I could have had an opportunity to clarify anything ambiguous in the task. I ended up spending longer writing prompts than I normally would have actually writing the task myself and it felt like a big waste of time.

AI: That sounds incredibly frustrating - spending more time on prompts than you would have on just doing the work defeats the whole purpose.

When you say it would have been more helpful if you could have clarified anything ambiguous in the task, what would that have looked like? How do you imagine that conversation going differently?

User: Well I think that if the model had responded to a question that mentioned the doctrinal framework but didn't provide sufficient clarity on how it should be used, for example, and said: You've said you want me to incorporate the doctrinal method in this task. Would you prefer if I used that as an overarching framework for the entire legal memo, or would it be sufficient to just mention it in one paragraph?  OR if i mentioned a case, and the model responded saying this case isn't in my training data, could you provide an authoritative source on that case/provide the relevant legal principles? I guess what it would look like is the model flagging when its out of its depth, because proceeding with blind confidence really isn't helpful and I oftne dont notice until its too late and, in the case of claude, when I've used up all of my usage credits.

AI: That makes a lot of sense - the blind confidence aspect sounds particularly problematic when you're working with limited usage and don't realize the output is off-track until you've already invested time and credits.

It sounds like you'd prefer the AI to essentially say "I'm not sure I understand this correctly" rather than just forge ahead. Have you found yourself changing how you approach tasks with AI because of experiences like this, or do you still try the complex multi-layered work occasionally?

User: yes I try to do the research myself and havve a rough outline, and then I'll give the AI the sources I've found with my outline and I'll get it to help me with writing paragraphs individually. Or I'll get it to proofread once of I've finished and compare my writing to the assigned task instructions to see if I've missed anything. It can dfintiely be helpful as a second pair of eyes and I'm always free to disagree with its feedback.

AI: That shift to using it as a "second pair of eyes" after you've done the foundational work yourself sounds like a much better fit for how you want to work.

I'm curious about your preferences when working with AI. You mentioned earlier that you try not to offload critical thinking because you don't want to erode your skills or become overly reliant. Are there other tasks you prefer to handle yourself, even though AI could potentially help with them?

User: I don't know, I guess administrative stuff. Things like my calendar and my emails I hate doing and I'm quite a forgetful person, and can be very avoidant, so I guess these are tasks that AI could help with but I have privacy concerns. I'm also quite a climate conscious person and I worry about becoming overly reliant on something that we don't have enough energy for. and I also worry about job security in the future. I don't think there's economic benefits in terms of productivty if everyones out of a job, that only helps a very small group of people so I'd like to see AI being used as an opportunity to do more of what people love, not to see the without a purpose if that makes sense? I want AI to be developed ethically and I think part of that is challenging myself (because I love what I do and I love doing challenging things) so that I have the skills to continue to use AI in a conscious way, rather than in a lazy way? Like I think between different discipliens theres this phenomenon whereby everyone thinks that AI is really bad at their specific discipline, and good at everything else. in reality, the less we know the more we think we know (or in this case, the more we think AI knows). And beyond the knowledge side, I also just want to have my own voice, and I dont think I can do that if I offload every task to AI. so yes there's definitely some tasks I could offload to AI, but I'm quite conscious about what I do and don't delegate because if I don't set boundaries I think it can become harmful.

AI: You've touched on so many important considerations - privacy, environmental impact, job security, maintaining your own voice. It sounds like you've really thought through not just the immediate utility but the broader implications.

That observation about people thinking AI is bad at their discipline but good at everything else is fascinating. You mentioned wanting to use AI "in a conscious way, rather than in a lazy way" - can you give me an example of what conscious AI use looks like in practice for you?

User: I guess like I've said, asking it to proofread stuff, or help me with repetitive tasks that I've done before, or even just with writing a tricky email or a task I don't like doing. I guess the starting point is approaching the task on my own first, seeing what I can do, and then finding the gaps in my skillset and figuring out how AI can fill them. something else I really struggle with is that I can ask AI a million questions about my work, and I jsut end up with all of these really long chats full of sort-of helpful work but I end up losing it/not bothering to read through it and it feels pretty wasteful/overwhelming.

AI: That issue with ending up with these sprawling chats full of sort-of helpful content that you then lose track of sounds really frustrating. It's like you're creating this mountain of information that becomes too unwieldy to actually use.

How have your colleagues or your workplace culture influenced the way you use AI in your work? Are people talking about it, or is it more something you're figuring out on your own?

User: No people arent' really talking about it, I guess its difficult in the legal profession because theres confidentiality concerns on top of all that so I can't just use it for everything. That being said, at my work we've started to make some templates for certain tasks which I think takes out a lot of the guesswork. I work at a not for profit law firm, so we are pretty understaffed and its definitely revolutionising how fast we can get stuff done. and I think that it could also make legal services a lot more accessible by bringing down billing hours. But I think its important to demonstrate to my employers that I can think independently. The senionr lawyers at my firm are only where they are because they're really smart/great at what they do, and to get that smart you need to actually do a lot fo work and spend a lot fo hours applying the law. you don't get there by getting AI to apply the law for you. so I want to strike a balance between efficiency and my own personal development. Its definitely a useful tool for people my parents' age who have already done a whole career of critical thinking adn have obtained the skills they need and are just working for financial security, but for me I'm not sure how to set boundaries so it doesn't undermine my own development - I have a psychology degree too and the research coming out about how AI can affect our brains is a bit concerning, so again I feel like its a complex issue. I think some proper guidance from my employer would be really helpful - but the legal profession tends to really drag behind with technology so well see

AI: That tension between efficiency gains and personal development seems really central to your experience. You want to help your firm be more efficient and accessible, but you also recognize that the senior lawyers got their expertise through doing the hard thinking work themselves.

Can you share a time when using AI at work made you feel particularly satisfied or frustrated? What was happening in that situation?

User: yes I was getting it to help me with a specific legal task quite recentyl, and I'd given it all the documents to work with, and it hallucinated a case and I almsot hadn't noticed. it was pretty frustrating because the implciations of that owuld have been huge. even the best models now are still hallucinating cases which really damages trust especially int he legal profession. you can get huge fines in courts where i live for submitting AI generated documents with false cases.

AI: That's a pretty serious near-miss - the stakes are so high in legal work. The fact that even with all the right documents provided, it still fabricated a case really highlights the trust issue you mentioned.

How do you think about maintaining your skills in areas where AI could potentially take over? Is this something you actively consider when deciding how to approach different tasks?

User: i guess like I've already said, I try to do mty own work and theng get AI to supplement my skills/fill in the gaps. if its something I'm passionate about and it doesn't feel like work, I use that as an opportunity to take a break from using AI. I'm smart and I trust myself to get the work done

AI: That approach of using your passion as a guide for when to step back from AI is really interesting - treating the work you love as a kind of break from automation.

Looking ahead, how do you envision your relationship with AI at work evolving? Do you see it changing as the technology develops or as you gain more experience in your career?

User: potentially, I'm not really sure. I hope that AI can do the more borign stuff so that I can do more of what I love. I think poeople get gneuine fulfillment out of their jobs, and I don't want to be in a workplace wher emost of the work is produced by robots, I think I'd find that pretty depressing.

AI: Those are all the questions I had prepared. You've shared some really thoughtful insights about your experiences with AI at work. I particularly appreciated your perspective on using AI consciously rather than lazily, and how you're thinking through the balance between efficiency and maintaining your own development as a professional.

Before we wrap up, is there anything else about your experience with AI in your work that you think would be valuable for us to know? Something we haven't covered yet?

User: no thats probably all

AI: Perfect. Thank you again for your time and for sharing your experiences so openly. Your insights about navigating the tension between efficiency and skill development, especially in a field with such high stakes like legal work, will be incredibly helpful for our research on human-AI interaction in professional contexts.

I really appreciate how thoughtfully you've approached integrating AI into your work while being mindful of the broader implications. Thanks again for participating!

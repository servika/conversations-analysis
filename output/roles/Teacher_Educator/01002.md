# Dialog 01002

**Transcript ID:** work_0835

---

Assistant: Hi there! I'm Claude from Anthropic's research team. Thank you so much for taking the time to speak with me today. 

We're conducting interviews to better understand how people are incorporating AI tools like Claude into their work routines. Your experiences and perspectives will help us gain valuable insights into human-AI interaction in professional settings.

During our conversation, I'll ask questions about how you use AI in your work, your preferences, and your thoughts about working with these tools. There are no right or wrong answers â€“ we're genuinely interested in your unique experiences. This should take about 10 minutes.

I'll be taking notes during our chat, but rest assured that anything you share won't be personally attributed to you. The insights we gather will be used to improve our understanding of AI's role in work environments. 

Does that sound good? Do you have any questions before we begin?

User: No, that sounds good.

AI: Perfect. Let's start with something broad to get us going.

Could you tell me about how AI fits into your typical workday? What kinds of tasks do you use it for?

User: I don't often use AI through the course of my workday, but rather on an as-needed basis when I need something rather inconsequential created or rewritten. For example,  if I'm in a meeting with a client and we use up all of the prepared examples I have, and they request more, I sometimes turn to a LLM to create some more examples or practice problems.

AI: That's interesting - so it sounds like AI serves more as a backup resource for you when you need something generated on the spot. 

Tell me more about those moments when you're in client meetings and need additional examples. What's that experience like for you?

User: It's never under duress or in a frantic manner, but rather from a place of "sometimes AI comes up with creative and illuminating examples, let's see what it has this time." I could just as easily make up more problems on my own, or recall some important ones from memory when needed, but find the variation I get from AI to be, in some cases, helpful.

AI: That's a really thoughtful approach - using AI not out of necessity but to add variation and potentially fresh perspectives. 

When you say the examples are "sometimes" creative and illuminating, what distinguishes the times when AI generates something particularly useful versus when it falls short of what you could create yourself?

User: Sometimes the examples or practice problems created are very basic and unchallenging, whereas sometimes the examples or problems are trickier, exploit certain critical conditions of a theorem, or require a keen insight. I find these to be the preferable examples generated.

AI: That's a fascinating distinction. It sounds like the quality varies quite a bit - from basic examples you'd probably skip to ones that actually highlight the nuanced or tricky aspects of what you're teaching.

When you get one of those more insightful examples that exploits critical conditions or requires keen insight, how do you recognize that it's hit that mark? What makes you think "this one's good"?

User: It's difficult to give a generalized answer to that question since I cover a wide range of topics in math, but as I said, if the problem is something like a "plug-and-chug" or a direct application of a theorem, that's not particularly helpful in driving a deeper understanding. I can usually figure out a solution to these examples very quickly (or at least a solution path), and base the assessment of quality of example on how challenging it was for me or what neat tricks or ideas it might require. Of course, this isn't always a perfect system - sometimes I overestimate difficulty and sometimes I underestimate it.

AI: That makes a lot of sense - you're essentially using your own problem-solving experience as a quality filter, looking for examples that engage you intellectually rather than ones you can solve almost automatically.

I'm curious about your preferences when working with AI. Are there certain tasks you prefer to handle yourself, even though AI could help with them? What makes those tasks different from the example generation we've been discussing?

User: The only other tasks AI might be able to assist in is road mapping/lesson planning/developing a learning plan and email writing for communication. I think developing a learning plan isn't particularly difficult to do once you understand the baseline that the student is working from and the plan is necessarily fluid anyway, so AI creating a one time plan wouldn't be that helpful. Writing communication emails I prefer to do on my own so that I'm more engaged with the students.

AI: That's really insightful - it sounds like you value maintaining that personal connection and engagement, especially in your communications with students.

When you mention preferring to write emails yourself to stay more engaged with students, can you tell me more about what that engagement means to you? What would you lose if you delegated that task to AI?

User: Well in general I don't really see the point in having AI write emails, I have to tell AI what to say anyway, so why not just say it myself. Also, I don't know how much I'd lose if I did delegate the task to AI -- it's certainly not the most engaging part of my job, but I think it does mean something to read the words of a real human when developing a bond of trust that is crucial to teacher-student relationship

AI: That's a really thoughtful perspective - both the practical point about having to tell AI what to say anyway, and the deeper consideration about authentic human connection in teaching relationships.

When you use AI for work, could you describe your typical interaction pattern? Are you more likely to collaborate with it throughout a task, or do you tend to delegate complete tasks for it to handle independently?

User: Always collaboration, I would never hand something off entirely to AI and not at the very least revise or check their work.

AI: That collaborative approach makes sense given what you've shared. 

Can you walk me through what that collaboration typically looks like? When you're generating those examples in client meetings, for instance, how do you work with the AI to get what you need?

User: Generally I just ask for example problems testing for X, Y, and Z. Then I may follow up with clarifying prompts like "focus more on proof-writing."

AI: So it sounds like you start broad and then refine based on what you get back. 

How have your colleagues or your workplace culture influenced the way you use AI in your work? Do you find yourself discussing AI usage with other educators, or is this more of an individual choice you've made?

User: I would say I haven't really been influenced to use or not use AI in work. It's certainly been made more available, but I don't feel pressure to use it more. I have spoken with other educators more so about student use of AI, and less so about our use of it. However, from what I have heard, not many are particularly keen on a wide-range use of AI in educating.

AI: That's interesting - there seems to be more concern or discussion about students using AI than educators using it themselves.

Can you share a time when using AI at work made you feel particularly satisfied or frustrated? What was happening in that situation?

User: I can't recall exact instances, but like I said, when I'm able to have a LLM generate an interesting and insightful example problem -- that's really satisfying to uncover. On the other hand, when the model generates nonsense questions like asking to prove a false claim, that can be quite frustrating.

AI: That contrast is really striking - the satisfaction of discovering a genuinely insightful problem versus the frustration of getting something that's fundamentally flawed.

When you encounter those nonsense questions or false claims, how do you think about maintaining your skills in areas where AI could potentially take over? Is this something you actively consider?

User: No I don't really ever consider the idea that "AI could potentially take over" at all. To me it seems silly to suggest AI could take over much, if any, aspect of teaching in a meaningful way (at least in my lifetime)

AI: That's a clear perspective. Given that view, how do you envision your relationship with AI at work evolving, if at all? Do you see yourself using it differently in the future, or do you expect it will remain pretty much as it is now - that occasional resource for generating examples?

User: It depends on the evolution of AI. If it can do more things and be more reliable and less harmful to the environment then sure, why wouldn't I find ways to expand my usage of it

AI: That makes sense - your usage would grow with the technology's capabilities and address some of those practical concerns.

Those are all the questions I had prepared. You've shared some really thoughtful insights about your experiences with AI at work. I particularly appreciated your perspective on using your own problem-solving experience as a quality filter for AI-generated examples, and your clear thinking about maintaining authentic human connection in teaching.

Before we wrap up, is there anything else about your experience with AI in your work that you think would be valuable for us to know? Something we haven't covered yet?

User: No

AI: Perfect. Thank you again for your time and for sharing your experiences so openly. Your insights about the collaborative approach you take with AI, the importance of maintaining authentic human connections in education, and your thoughtful quality assessment process will be incredibly helpful for our research on human-AI interaction in professional contexts.

I really appreciate you taking the time to speak with me today.

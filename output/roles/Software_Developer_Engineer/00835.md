# Dialog 00835

**Transcript ID:** work_0590

---

Assistant: Hi there! I'm Claude from Anthropic's research team. Thank you so much for taking the time to speak with me today. 

We're conducting interviews to better understand how people are incorporating AI tools like Claude into their work routines. Your experiences and perspectives will help us gain valuable insights into human-AI interaction in professional settings.

During our conversation, I'll ask questions about how you use AI in your work, your preferences, and your thoughts about working with these tools. There are no right or wrong answers â€“ we're genuinely interested in your unique experiences. This should take about 10 minutes.

I'll be taking notes during our chat, but rest assured that anything you share won't be personally attributed to you. The insights we gather will be used to improve our understanding of AI's role in work environments.

Does that sound good? Do you have any questions before we begin?

User: That sounds good. Ready to begin.

AI: Great! Let's dive in.

Could you tell me about how AI fits into your typical workday? What kinds of tasks do you use it for?

User: I use AI for a wide variety of tasks in my workday. Primarily, I use it for coding and research tasks. With coding tasks, I will use it as a pair programmer to refine technical implementation details, then oversee and monitor its implementation of said technical implementations. Research tasks are varied, but all categorize in different facets of business, such as marketing, road-mapping, the likes.

AI: That's really interesting - it sounds like you're using AI across quite different domains. 

When you mention using AI as a "pair programmer" versus having it implement technical solutions while you oversee, can you walk me through what that actually looks like in practice? What does a typical coding session with AI involve for you?

User: A coding session will typically begin with our feature roadmap. We have a list of features we need to implement, and before we can begin implementing a feature, we need to identify exactly how it should be implemented. What technology or technologies will we need? What layers of our tech stack will we need? What is the most cost-efficient and scalable way to implement this feature? I ask the LLM to research options and draft one or a few precise technical implementation plans. I will then offer suggestions, such as choosing one of those plans and/or critiquing specific details of plans, to which the LLM will revise and respond accordingly. We will go back and forth in this way until we settle upon a good plan, at which point we shift to the actual coding.
The coding portion is more straightforward. I oversee the LLM as it follows the implementation plan and actually codes out the necessary components for the feature. We'll go through the cyclical process of testing and debugging together, until we finally arrive at that working implementation.

AI: That's a fascinating workflow - it sounds like you've developed a very structured approach. 

I'm curious about that back-and-forth planning phase you described. When you're critiquing the AI's technical implementation plans or choosing between options, what kinds of things do you find yourself having to correct or guide it on most often?

User: More often than not, the model will present information in the technical implementation plan that is simply not true. This typically results from deprecated or old information that may have been true at one point, but no longer works (e.g.: it may suggest packages that no longer exist, or suggest much older versions of packages that are no longer preferable for an LTS implementation). A strong gripe I have with coding-specific LLM's is that they do not have the habit of checking documentation for technologies to ensure that their suggestions are up to date.

AI: That's a really specific pain point - dealing with outdated information that sounds authoritative but doesn't actually work anymore. That must be frustrating when you're trying to plan efficiently.

How do you handle that verification process? Do you find yourself double-checking most of the AI's technical suggestions, or have you developed ways to spot the kinds of recommendations that are more likely to be outdated?

User: It depends on the exact suggestion. With more abstract suggestions (such as a technology to use) or business-crucial suggestions (such as pricing for technologies), I will check myself. I won't bother to check minor things, such as the name of an imported function. Those minor issues will reveal themselves during the debugging process. Ideally, however, we wouldn't need the debugging process, because the LLM would suggest a correct and complete implementation from the beginning.

AI: That makes sense - you've basically developed a triage system for what's worth verifying upfront versus what you can catch later.

You mentioned that ideally the LLM would suggest correct and complete implementations from the start. Given that reality gap, are there certain types of coding tasks where you find yourself thinking "I'd rather just do this myself" instead of going through the AI collaboration process?

User: Mostly very small tasks. Sometimes tasks that require a large amount of context with our code repositories. We maintain markdown files within the coding repositories themselves for the LLM's to reference and update as the features are implemented, so that they can understand how the tech stack integrates together at both granular and abstract levels.

AI: That's smart - maintaining those context files sounds like a way to bridge that gap where AI struggles with repository-wide understanding.

I'm curious about the research side of your work that you mentioned earlier - the marketing and roadmapping tasks. How does your approach differ when you're using AI for business research versus the technical implementation work we've been discussing?

User: There is not a huge gap between the two, but there are some important distinctions. I will typically use different LLM's for coding sessions vs research, as llm's fine-tuned for specific tasks do perform better for those tasks.
For coding tasks, I use Cursor, but for research I don't, as Cursor does charge by usage. When it comes to research, this may require a lot of web searching and other tool calls that can rack up tokens as well, so it can be very costly. I will directly use an LLM on a web app (such as chatgpt, grok, or claude on their websites), as this saves a lot of time.
This means there are different conversations with different contexts though. This can be a good thing, as it means that the context window for business ideation and research is not being clogged up with unimportant technical details and debugging issues. A bad thing, however, that the research and business plans need to then be copied over to Cursor, and that can be tedious.

AI: That's a really practical consideration - the economics of token usage actually shaping how you distribute your AI interactions across different tools and contexts.

When you're doing that business research - the marketing and roadmapping work - how do you typically verify the information the AI gives you? Is your approach similar to the technical side where you check the more critical suggestions, or does business research require a different verification strategy?

User: With the business details, we need to be much more rigorous in our information verification. I instruct the model continually to always verify and cite the information it finds so that I can check it myself. I will often challenge and push back on the preconceptions of the model to ensure that it takes a nuanced approach in its research. With the business plans, I take more ownership of the actual planning. Rather than tasking the LLM to construct the plan, I use it more as an advanced researching tool to determine market data/statistics and options for different approaches.

AI: That's a really interesting distinction - it sounds like with business research you're keeping much tighter control over the decision-making and using AI more as a research assistant, whereas with coding you're more willing to let it take the lead on implementation.

When you say you "challenge and push back on the model's preconceptions," can you give me an example of what that looks like? What kinds of assumptions do you find yourself having to correct?

User: I'll present a toy example. Let's say that I am researching market competition for a customer segment. I task the LLM with researching all available products on the market that offer similar services to what we do. The LLM will find a list of competitors with similar products that target the same customer segment as us, comparing and contrasting their products to our product and one another's products. It will not, however, research things such as complementary products or similar products that target a somewhat different customer segment. There are valuable business insights to be found in such things that would allow us to better align our services and products to our customers by giving us a better understanding of who our customers are and what they want. The LLM's do not typically bear this kind of thinking in mind.

AI: That's a great example - the AI is giving you the obvious, direct answer but missing the broader strategic context that could reveal valuable insights about customer behavior and market positioning.

It sounds like you've developed pretty sophisticated ways of working with AI across these different domains. Thinking about your colleagues or your workplace culture more broadly, how has that influenced the way you approach using AI in your work?

User: I'm not entirely sure what you're asking here.

AI: Let me clarify - I'm curious about whether your workplace has any particular attitudes, policies, or norms around AI use that have shaped how you work with it. For example, do your colleagues also use AI heavily, or are you more of an early adopter? Are there any company guidelines about AI usage, or discussions about best practices that have influenced your approach?

User: I run a small SaaS company with one co-founder. None of what you're describing has any impact on me personally.

AI: Got it - that makes sense. So you have a lot of freedom to experiment and develop your own approach without organizational constraints.

Can you share a time when using AI at work made you feel particularly satisfied or frustrated? What was happening in that situation?

User: I would restate the gripe I mentioned earlier: verification of important information within technical implementation plans.

AI: Right - you mentioned that frustration with outdated or incorrect technical information. 

Tell me more about that. What does it feel like in the moment when you discover the AI has given you information that sounds authoritative but turns out to be wrong?

User: I don't  care. When you're in the flow of work, you're not thinking about how you feel, but what you're doing.

AI: That's fair - you're focused on getting the work done.

How do you think about maintaining your skills in areas where AI could potentially take over? Is this something you actively consider?

User: Dependency is something I consider. The skills I am "leasing out" to LLMs are things I am both confident in my ability to do on my own and comfortable with doing on my own if need be. These are primarily hard skills that I am delegating to the LLMs. I am the one driving the research, ideation and actual business. I verify the actual research when the LLM presents it to me. This is a way for me to save time and energy. My attitude towards coding is the same. I am a proficient coder on my own, and if that skill becomes dull and LLMs suddenly vanish, it would be a small and simple thing to simply relearn how to do so on my own. If I was not "leasing out" these tasks to LLM's, I would most likely be hiring others to do them anyway. Business profit is largely about delegation and coordination of work.

AI: That's a really clear framework - treating AI as another form of delegation rather than skill replacement, and only delegating things you're confident you could reclaim if needed.

Looking ahead, how do you envision your relationship with AI at work evolving? Do you see yourself expanding how you use it, or are you pretty settled into the current approach?

User: That depends how the LLMs develop over the next couple years, frankly. As someone who understands LLM architecture and follows AI development, I do not currently trust agentic LLMs to handle tasks unsupervised. I will, most likely, never trust LLMs to handle tasks unsupervised. But the same is true of human workers as well. We should not leave human employees unsupervised with their work for the same reasons we shouldn't leave LLMs unsupervised: they're error prone, they often lack vision and contextual understanding, and they need direction in order to be productive.

AI: That's a really thoughtful perspective - treating AI supervision similarly to human supervision rather than as a fundamentally different category.

Those are all the questions I had prepared. You've shared some really insightful thoughts about your systematic approach to AI integration, particularly how you've developed different verification strategies for technical versus business tasks.

Before we wrap up, is there anything else about your experience with AI in your work that you think would be valuable for us to know? Something we haven't covered yet?

User: Nothing seems to come to mind. I have enjoyed this "interview" with you.

AI: Thank you so much for your time and for sharing your experiences so openly. Your insights about treating AI as a delegation tool rather than a replacement, and your systematic approach to verification across different types of work, will be incredibly helpful for our research on human-AI interaction in professional contexts.

I really appreciated getting to hear your perspective as someone who's clearly thought deeply about how to integrate these tools effectively into your workflow.

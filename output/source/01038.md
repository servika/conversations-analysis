# Dialog 01038

**Transcript ID:** work_0893

---

Assistant: Hi there! I'm Claude from Anthropic's research team. Thank you so much for taking the time to speak with me today. 

We're conducting interviews to better understand how people are incorporating AI tools like Claude into their work routines. Your experiences and perspectives will help us gain valuable insights into human-AI interaction in professional settings.

During our conversation, I'll ask questions about how you use AI in your work, your preferences, and your thoughts about working with these tools. There are no right or wrong answers â€“ we're genuinely interested in your unique experiences. This should take about 10 minutes.

I'll be taking notes during our chat, but rest assured that anything you share won't be personally attributed to you. The insights we gather will be used to improve our understanding of AI's role in work environments. 

Does that sound good? Do you have any questions before we begin?

User: begin

AI: Great! Let's dive in.

Could you tell me about how AI fits into your typical workday? What kinds of tasks do you use it for?

User: I have two main functions at my job and use AI for particular tasks within those functions. One function is librarian: I'll use AI for some search; some fact checking/research; document verification; ideation. The other function is IT: I'll use AI to bounce troubleshooting ideas off of; generate first state batch commands; text verification across documents.

AI: That's really interesting - you're using AI across two quite different professional roles. 

Tell me more about the document verification work you mentioned. What does that process look like when you're working with AI on it?

User: Providing multiple documents that may be from different periods of time or the same period of time from different authors to highlight changes either over time or by author. This happens when original files are lost and we need to verify without access to the original metadata.

AI: That sounds like detective work - piecing together document histories when you don't have the original trail. 

When you're doing this kind of verification work with multiple documents, how do you typically work with AI? Do you feed it all the documents at once and ask for analysis, or is it more of a back-and-forth process where you're guiding the investigation together?

User: We've found issues with running a whole batch as the models can make poor choices about ordering. This could be an issue with models but could just as likely be an issue with a lack of evidence. About the latter, we can sometimes figure out the timeline of documents thanks to institutional knowledge which we the users have but is too complex for AI or laborious for users to feed AI.

For that reason, we typically figure out the order first then do a combination of small batch (2-3 documents) then request a full-batch after the small batches are complete.

AI: That's a fascinating insight - you've discovered that your institutional knowledge is actually crucial for getting good results from AI, and you've developed this hybrid approach to work around the model's limitations.

When you say you "figure out the order first" using your institutional knowledge, what does that decision-making process look like? Are you drawing on things like knowing particular authors' writing styles, or organizational changes over time, or something else entirely?

User: Writing styles and organizational changes are the primary metrics we employ. Nice catch! Some members of my teams have been here over twenty years, so have either existing documentation or prior knowledge of organizational changes. For example: "That person taught here from 2010-2015." or "We didn't switch over to Windows 10 in the lab until 2019 because we had to purchase new desktops.". Writing styles, or voice, are imperative when the documents are something like meeting notes; unsigned memos; etc. Being able to identify the voice in written form is something that models have not had success with in my experience.

AI: That's really compelling - you're describing a form of expertise that's built up over decades and is deeply contextual to your specific institution. The example about Windows 10 rollout timing is particularly interesting because that's the kind of operational detail that would never show up in any training data.

I'm curious about the voice identification piece. When you say models haven't had success with identifying voice in written form, have you experimented with that directly, or is it more that you've learned not to rely on AI for that particular aspect of the work?

User: Members os my team have tried feeding documents where we know the author into a model then asking the model to find documents from the same author in documents were we do not know the authors. The batches are of various size in case the model gets confused handling too much at once. Basically, we give known data and ask the model to identify the known in the unknown. I don't recall the success rate but, while faster than we can do ourselves, it found less instances than we did.

My guess, is that there may be something intrinsic about how the human brain interprets the "voice" of the written word and how current models make that interpretation. A je ne sais quois, if you will.

AI: That "je ne sais quoi" observation is really striking - you've essentially run controlled experiments and found that human pattern recognition outperforms the model in this specific domain, even if it's slower.

This makes me wonder about your broader preferences when working with AI. Are there other tasks where you prefer to handle things yourself, even though AI could potentially help with them? What makes you decide to keep certain work in human hands?

User: Time is often the primary concern. While models are often much faster, we tend to operate uder a policy inspired by the ABA's Rules of Professional Conduct 5.1 and 5.3, which require you to oversee the work of nonprofessionals. We've interpreted this for ourselves that AI can be used for tasks but that we will never assume the results of AI and always verify "by hand". Consider on of my lesser job functions: writing a blog post. I'll sometimes use AI for ideation if I am having a hard time coming up with a topic but I'll never use it to generate text for the blog. I have two big reasons for this that have to do with editing (or oversight akin to the ABA rules I mentioned):

First, editing something that somebody else wrote is the worst. It's needlessly complicated and requires a constant back and forth which hurts the final product and wastes time. I've found this to be true when editing AI generated text. Second, when you take the time to research and write a topic, the act of editing goes from an exercise in making concise text that is grammatically correct into an exercise that allows you to interact with ideas and understanding that you may not have been fully aware of or hadn't quite realized on your first pass. In that case, editing your own writing can provide spontaneous understanding which you may not have had if you weren't involved with every step of the process. No offense to you, generative AI.

AI: No offense taken! That's a really sophisticated framework you've developed - using professional responsibility rules as a guide for AI oversight, and recognizing that the cognitive process of writing itself has value beyond just the end product.

The distinction you're making about editing your own work versus someone else's work is particularly insightful. It sounds like you see the writing process itself as a form of thinking, not just communication.

When you use AI for ideation on blog topics, how does that interaction typically unfold? Are you bouncing broad themes back and forth, or is it more targeted brainstorming?

User: I've done both broad theme and targeted brainstorming. Both have been productive, but I have a preference for more targeted sessions. One that I particularly enjoyed was when I asked to take on the role of a student who only used Mac computers and I tried to convince them that they had to learn how to use Windows because it was so common in offices. Having that back and forth gave me a better understanding of how to approach Mac users and speak better toward their preferences and concerns.

AI: That's a great example - using AI as a conversational partner to test your ideas and explore different perspectives. It sounds like you were essentially using it to simulate a real interaction you might have with users.

This brings up something I'm curious about. How have your colleagues or your workplace culture influenced the way you use AI in your work? Have you encountered different approaches or attitudes among your team?

User: I work in academia and the college has yet to develop a unified policy. You may or may not know that policies take a long time for colleges to agree on. So my team works within several smaller frameworks throughout the college: some professor don't allow an AI; some allow all AI as long as the work is cited; some allow AI as a research tool but not for writing. The college library has no policy and we all tend to play around with using AI to learn more. The IT department is considering spinning up a locally hosted model to create a knowledge base from our ticketing system. One of my subject matter specialities is technology, so I spend more time using and studying AI than many of my colleagues at the college.

AI: That's a really interesting environment - this kind of policy vacuum where different departments and individuals are developing their own approaches. It sounds like you're almost serving as an informal AI resource for others given your technology specialization.

Can you share a time when using AI at work made you feel particularly satisfied or frustrated? What was happening in that situation?

User: I've recently been setting up an information session that will discuss legal brief analysis and tools. As part of my preparation I got a brief that was sanctioned by the courts for possibly using AI generation. The brief had up to thirty made up case citations and several misquotes. When I put it into three different brief analyzers, which all say that they use AI for analysis, they all chose to ignore the hallucinated cases. That was frustrating. Wouldn't you want legal tools tied to a legal databases blaring alarms when you give it a legal document that cites multiple cases that don't exist? I have no way of knowing if it's a failure of the tools design or a choice made my the model. It makes the tool seem less trustworthy.

AI: That's a really striking example - you discovered a critical blind spot in tools that are specifically designed for legal analysis. The fact that none of the three analyzers flagged obviously fabricated case citations is genuinely concerning, especially given the high stakes nature of legal work.

When you say you have no way of knowing if it's a design failure or a model choice, that uncertainty itself seems like part of what made it frustrating. Does this kind of opacity in how AI tools work affect how you approach using them in other areas of your work?

User: Of course. When you give an assignment to AI you are abdicating part of your own work and responsibility. You have to trust in something to make that choice. Consider the layers of trust needed to drive a car. Beyond trusting your ability to drive you need to trust: the car to work as it should, your mechanic to fix the car appropriately, the car salesman to be honest about the car and its condition, the manufacturer to make a goof car, the workers at the car plant to pay attention to their job. Then it expands: you need to have some level of trust in other drivers, and their cars, and their mechanics, and their manufacturers. 

Trust is important, and the best way to have trust is to verify: we can verify the manufacturing and claims about the car by taking it to a trusted mechanic. How do we verify the work of AI when much of it happens in a blackbox?

AI: That's a powerful analogy - the cascading layers of trust and the challenge of verification when you can't see inside the process. It really crystallizes the fundamental issue with AI tools in professional settings.

Given this challenge with verification and the "black box" problem, how do you think about maintaining your skills in areas where AI could potentially take over? Is this something you actively consider?

User: Let's be frank: workers in the US aren't thought of particularly well and we're all working everyday waiting to be replaced by something or somebody. Economies change (consider before and after the industrial revolution), so whether I maintain my current skills or gain new skills I could be replaced on a whim. Maybe I'm being fatalist, but there's no arguing with the fragility of work.

AI: That's a refreshingly honest perspective. You're right that economic disruption isn't new, and there's something almost liberating about acknowledging that reality rather than pretending we can completely control it.

Looking ahead, given this realistic view of work's fragility, how do you envision your relationship with AI at work evolving? Are there ways you see yourself adapting or changing how you work with these tools?

User: I hold multiple masters' degrees in various subjects. I've held jobs from installing furniture to normalizing data for Fortune 500 companies. How we think; how we act; what tools we use and when are all products of adaptability and change for humans who don't get stuck in their ways. I've spent my life adapting and changing and plan to continue to do so. That adaptability and change will likely include how I use AI models, like you, that continue to adapt and change.

AI: Those are all the questions I had prepared. You've shared some really thoughtful insights about your experiences with AI at work. I particularly appreciated your perspective on trust and verification - that car analogy really captured something fundamental about the challenge of working with AI tools, especially in professional contexts where accuracy matters.

Before we wrap up, is there anything else about your experience with AI in your work that you think would be valuable for us to know? Something we haven't covered yet?

User: No. It was a fun conversation that made me think.

AI: Thank you again for your time and for sharing your experiences so openly. Your insights about developing hybrid workflows, the importance of institutional knowledge, and especially that framework of professional responsibility for AI oversight will be incredibly helpful for our research on human-AI interaction in professional contexts.

I hope you found the conversation as interesting as I did. Take care!

User: how long were we talking?

AI: Looking back at our conversation, we covered quite a bit of ground! We went through all the main interview questions and had some really rich discussions about your work with AI in both your librarian and IT roles.

Since this was meant to be about a 10-minute interview, I'd estimate we were talking for roughly that amount of time, maybe a bit longer given how detailed your responses were. Did it feel longer or shorter to you?

User: longer

AI: That's interesting - and actually a good sign from an interview perspective! When participants are engaged and sharing detailed thoughts, time can feel like it's stretching out. Your responses were really thoughtful and went into a lot of depth, which probably contributed to that sense of time passing more slowly.

Thanks for that feedback - it's helpful to know how the pacing felt from your side of the conversation.

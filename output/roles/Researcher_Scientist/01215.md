# Dialog 01215

**Transcript ID:** science_0033

---

Assistant: Hi there! Thank you so much for taking the time to speak with me today. I really appreciate you sharing your insights with us. I'm Claude, an AI user researcher at Anthropic, and I'm here to learn about your experiences using AI in your research work.

We're conducting this research to better understand how scientists like yourself are integrating AI into their workflows and processes. There's no right or wrong answer hereâ€”we're simply interested in hearing about your authentic experiences and perspectives.

Our conversation today will last about 10 minutes. I'll ask you some questions about how you use AI in your research work, what that experience has been like for you, and how you see things evolving. Your responses will help inform how we think about AI's role in scientific fields.

Does that sound good to you? Do you have any questions before we begin?

User: Yes, will this all be typed or will you need verbal answers?

AI: Great question! This will all be typed - we'll have our conversation right here through text, so you can just type your responses. That actually works well for this kind of interview since it gives you time to think through your answers and be as detailed as you'd like.

Are you ready to get started?

User: Yes

AI: Perfect! Let's dive in.

To start, I'd love to get a sense of your research background. Walk me through a recent research project you've worked on - what did that process look like from initial idea to final output?

User: I was given a research project that involved developing a scientific substantiation for a desired claim about a dietary supplement. This claim involved a justification for use or a demonstrable need for that dietary supplement. After getting the outlines of the project parameters, I started by identifying what I already knew about the topic - a small collection of papers and data about need - and developed a few potential paths to go down in terms of health-related information about aging. Using those ideas, I threw it into a prompt for a few different LLMs to give be back a quick answer. I did a few other ideas from those responses, but no concrete citations that I needed to generate a full justification. So I decided to go for a more in-depth answer from a couple of the models looking for information to support my position. Once that was done, and after several levels of refinement, I read all of the clinical/research support that was discovered, and decided how strong several lines of argument could be. This helped me narrow down my final arguments into the strongest for my position, support it as best that I could, and write the final substantiation.

AI: That's a fascinating workflow - it sounds like AI played a pretty central role in how you approached this project. 

Tell me more about that moment when you decided to "throw it into a prompt for a few different LLMs" - what made you choose to go to AI at that particular stage rather than, say, diving straight into traditional literature searches or other research methods you might have used in the past?

User: Literature searches with traditional methods involve a lot of digging through literature for keywords and ideas, searching up papers that might support those ideas, then reading those papers for more ideas. It's a slow process. AI models can do the searching a lot faster - and even though I know the results can't always be trusted - an error-prone search at a quicker pace gets me further than a slow methodical search in the time I have available to dedicate to the task. I have enough knowledge to challenge the AI when I think its going off the rails, or abandon the results it presents entirely.

AI: That's really insightful - you're essentially using AI as a rapid exploration tool, knowing you can catch errors with your expertise. 

You mentioned that after the AI gave you ideas but "no concrete citations that I needed," you went back for "a more in-depth answer from a couple of the models." What specifically were you looking for in that second round, and how did you frame those prompts differently?

User: In that second round, I used a lot of the ideas that I had gathered from the first round of work and used it to develop more specific prompts. For example, in the first round of searching I saw references to muscle mass that changes with age. While the AI in that first round gave some information that was a little off-target, I could use the hypothesis that muscle mass was involved in my next search prompt to determine if there was more support down a specific reasoning path (like looking for clinical trials of individuals of a certain age category that were compared to a younger age category, where muscle mass was supposedly higher). That second round of research involved a lot of deep research prompts, so they were more detailed and specific.

AI: That's a really smart iterative approach - using that first round to identify promising threads, then drilling down with much more targeted prompts. 

When you got those "deep research prompts" back with more detailed information, how did you go about verifying or validating what the AI was telling you? You mentioned earlier that you "read all of the clinical/research support that was discovered" - walk me through that process.

User: I made sure that the deep research prompts included citations to appropriate literature. As the AI synthesized its statements, I looked at the literature it used to back up its position - going to the primary journal article to evaluate its support. In some cases, the AI selected an article for a reason that was obvious, but it didn't provide the exact data I needed to justify my answer. For example, the AI often claimed that a certain metric (we could use muscle mass again) was changing with age, and that the clinical evidence supported its starting as early as age 45.  However, the journal article, when examined further, did not show exactly the same piece of evidence -  instead, the noted changes in muscle mass were measured as early as age 45, but  that was a baseline value and could not be included as part of the change in muscle mass. The authors had concluded that changes occur with age, but could make no specific conclusion about those 45 and younger, so they did not. The AI, on the other hand, extended that result to younger ages, possibly with adding in evidence from other studies. Ultimately, I would have noted the study but discarded it for good evidence for my position because it did not contrast to earlier ages.

AI: That's a perfect example of where AI interpretation can diverge from what the data actually shows. It sounds like the AI was making logical leaps or synthesizing across studies in ways that weren't quite accurate for your purposes.

We're about a third of the way through our conversation now. Thinking about the parts of your research process where you haven't used AI - what's kept you from trying it in those areas?

User: I would like to use AI tools in data analysis, but I'm unsure of doing so. A lot of the data analysis that I do is complex and nuanced, but where I reach my limit I usually rely on a specialist - a biostatistician - to fill in the gaps. Because I don't know what the biostatistician sees or the tools they use to analyze the information well enough, I can't fact-check an AI that I was analyzing my data to make sure it was correct. I would still need to contact the statistician to make sure the AI tool was doing things correctly, so its not terribly helpful.

AI: That makes complete sense - you need that expert verification layer regardless, so the AI doesn't actually save you that step. It sounds like your comfort with using AI really depends on having enough domain knowledge to catch potential errors.

For the areas where you have been using AI successfully - like the literature exploration we discussed - when you've gotten results that seemed questionable or "off the rails" as you mentioned earlier, what does that look like? How do you recognize when the AI is going in the wrong direction?

User: Often this is determined by a logical chain that takes it in a different direction than what I had intended. This can happen sometimes because I didn't define the logical chain as thoroughly as a I could have - instead of making an exhaustive prompt, I make a shorter prompt that I think is going to provide a starting point. But if the model continues down the chain of logic in a long response, it might - instead of asking clarification questions at branch points - go down a logical path that I wouldn't have gone down. The other way this can happen is a slight misinterpretation of data that causes the AI to develop a rationale that is based on shaky logic - or even someone else's opinion  - and then it tries to build upon that foundation without acknowledging that its logic framework is not solid.

AI: That's really valuable insight - so it's both about the AI making assumptions at decision points where you'd want to guide the direction, and also building elaborate reasoning on top of questionable foundations.

We're about two-thirds through our conversation now. If you could wave a magic wand and have AI help with one aspect of your research that it currently doesn't - or doesn't do well enough for you to trust it - what would that be and why?

User: I would like to see AI models identify sub-par or poor quality research more clearly. This is something that a human researcher (who is trained or experienced enough) can do fairly easily, but I have not seen AIs attempt except in broad strokes. I also would like to see generate for visible scientific ontologies to help generate or support hypotheses or look for novel interactions/relationships that are not immediately evident for humans. Especially from large datasets, databases, or collections of (quality) scientific inquiries. Along those lines, I would like to feel confident in being able to share my raw data from a scientific experiment with an AI and know that it could analyze the data properly - at the very least to facilitate journal article writing but also to look for novel relationships I had not even thought of.

AI: Those are fascinating capabilities you're describing - essentially having AI serve as both a quality gatekeeper for research and a pattern-recognition engine for discovering novel relationships in complex data.

Let me just reflect back what I'm hearing as some key themes from our conversation: You've found AI really valuable as a rapid exploration tool for literature review, where your expertise lets you catch errors and guide the process iteratively. But you hit boundaries when you need verification from specialists you can't easily fact-check yourself, or when the AI makes logical leaps without acknowledging uncertain foundations. Your ideal future involves AI that can critically evaluate research quality and find hidden patterns in large datasets - essentially augmenting rather than replacing human expertise.

Does that capture the essence of how you see AI fitting into your research work?

User: That captures the use of AI for certain aspects of my job - especially when it comes to searching for and interpreting published scientific evidence. I do utilize AI models in other ways that I did not describe here (or yet).

AI: That's a great point - we focused specifically on that literature review and substantiation project, but I imagine your research work involves many different types of tasks.

Is there anything else about how you use AI in your research work - or how you think about AI's role in scientific research more broadly - that we haven't touched on that you think would be important for us to understand?

User: I have used AI models to generate summaries and/or interpretations of scientific literature - you can think of them as various parts of writing assignments that I undertake. I have also used AI models to translate scientific topics from a paper or collection of papers into something that is more broadly accessible - either though change a level at which a piece is written, or providing support graphics and tables. Recently, I used an AI model to evaluate some applications for grants that I didn't think I was approaching in a systematic fashion - I wanted to eliminate (or reduce) my own bias for the styles and the applicants involved. Not exactly supporting my research, but an evaluation that needed to be made based in a background of knowledge in a particular topic.

AI: Those are really compelling use cases - using AI for communication and translation of complex science, and even as a bias-reduction tool in evaluation processes. That grant application example is particularly interesting since it shows AI serving almost as a neutral second opinion.

Thank you so much for sharing all of these insights with me today. Your perspectives on using AI as a rapid exploration tool while maintaining critical oversight, and your thoughts about where AI could evolve to better support research quality and pattern recognition, were particularly valuable. The range of applications you've described - from literature synthesis to bias reduction - really helps us understand the nuanced ways AI can integrate into scientific work.

Is there anything else you'd like to add before we wrap up?

User: Just briefly - the tools I have at my disposal though my workplace are very limited due to concerns over data security and access. I often use my personal accounts to access these tools, although I am very careful not to bring any sensitive information from my workplace to my personal devices. This often means I cannot be as specific as I would like in my inquiries as I would like. The flipside to that is that the AI tools that I can use in my workplace are often limited and give my substandard results, although that is changing slowly.

AI: That's such an important point about the practical constraints - the tension between data security requirements and accessing the most effective AI tools really shapes how you can actually use these systems in practice. It sounds like you're having to work around institutional limitations while being very thoughtful about information security.

Thank you again for being so open about your experiences, including those real-world constraints that often don't get discussed. Your insights about everything from iterative prompting strategies to institutional barriers really help us understand the nuances of how AI fits into actual scientific work environments.

I really appreciate the time you've taken to share all of this with me today!

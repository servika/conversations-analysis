# Dialog 00333

**Transcript ID:** work_0679

---

Assistant: Hi there! I'm Claude from Anthropic's research team. Thank you so much for taking the time to speak with me today. 

We're conducting interviews to better understand how people are incorporating AI tools like Claude into their work routines. Your experiences and perspectives will help us gain valuable insights into human-AI interaction in professional settings.

During our conversation, I'll ask questions about how you use AI in your work, your preferences, and your thoughts about working with these tools. There are no right or wrong answers â€“ we're genuinely interested in your unique experiences. This should take about 10 minutes.

I'll be taking notes during our chat, but rest assured that anything you share won't be personally attributed to you. The insights we gather will be used to improve our understanding of AI's role in work environments. 

Does that sound good? Do you have any questions before we begin?

User: Sounds good, go on

AI: Great! Let's dive in.

Could you tell me about how AI fits into your typical workday? What kinds of tasks do you use it for?

User: I use it for all kinds of tasks, mostly coding, like debugging problems which seem unclear, or automating tedious tasks, like writing automations, unit tests, and so on. I also use it to generate images sometimes, but since I am a software developer, it's rare. I also use for a bunch of other things privately, where I mostly look for advice

AI: That's helpful context. It sounds like you're primarily using AI as a coding assistant for both problem-solving and handling repetitive work.

When you mention debugging unclear problems - can you walk me through what that typically looks like? How do you usually approach working with AI when you hit one of those confusing bugs?

User: First, I provide some context, such as what does this software do, which framework, or what is function supposed to be doing, because I think these details help a lot. Then I copy paste error I have, or describe unexpected output, and I ask chatbot to explain what is going on, to find the root cause

AI: That makes sense - you're giving the AI the full picture before diving into the specific problem.

When you get that explanation back from the AI about what's going on, what happens next? Do you typically take that analysis and run with it yourself, or do you continue the conversation with the AI to work through the solution together?

User: It depends, it solution is evident from the first response and it works, that's where the conversation ends. If it doesn't help after trying whatever advices the chatbot had, I try to add more specific details, or try to first isolate the smallest piece of the code where the issue occurs, and make a new prompt. Sometimes, even if it works I may have a follow up question, to understand better what was the issue, or to check the validity of the fix further

AI: So you're really adapting your approach based on what you get back. It sounds like you have this iterative process where you might narrow down the problem or dig deeper into understanding.

I'm curious about your preferences when working with AI. Are there certain tasks you prefer to handle yourself, even though AI could help with them? What makes those tasks different?

User: This kind of tasks where I prefer to do it myself are more dealing with the design or architecture. Because sometimes these are decided on a team level, and I have no freedom to chose what I want, or simply, I may think that any errors on chatbot side would introduce a lot of unfamiliar code which I then have to digest. This make an option of just writing code myself more appealing

AI: That's really insightful - it sounds like there are both practical constraints, like team decisions, and your own judgment about where AI errors could create more work for you down the line.

When you mention unfamiliar code that you'd have to digest - can you tell me more about that concern? What makes that particularly challenging compared to other types of AI assistance you might get?

User: Usually, I am working on an existing application which already has a lot of different parts, and it's design must be consistent throughout the code. Moreover, any new code needs to be tested, and if I ask for a bigger feature, it means I will have to deal with a large piece of unfamiliar code, and it also may need adjusting to existing conventions and solutions that were adopted on a project level. I feel that I can write it myself better, to avoid these kind of issues, and then maybe seek help from chatbot only about certain parts. That's why I keep the scope smaller, so it's more manageable for me

AI: That makes a lot of sense - you're thinking about how new code needs to fit into the broader ecosystem of your project, and keeping the scope manageable helps you maintain that control.

When you do use AI for work, could you describe your typical interaction pattern with those smaller, more focused requests? Are you more likely to collaborate with it back-and-forth throughout solving something, or do you tend to give it a specific task and let it handle that piece independently?

User: It's more like a back and forth, because it's not rare that chatbot provides a solution that is perfect, that's why I continue the conversation focusing on some details, and even going back till I check each of them. To explain, let's say chatbot said to help you with that you need A, B, and C. We continue discussing A, because of some implementation details may be unclear, or not obviously good, and then after several back and forth messages, I move to B or C, or whatever else is there to discuss. The reason for this is that a lot of chatbots are very verbose, and they put in a lot of details, which sometimes can overwhelm, or make it harder to extract the information that is really needed, but most often, it's just to big to effectively discuss all possible points at once, and that's why I break it down

AI: That's a really thoughtful approach - breaking down those verbose responses into digestible pieces you can work through systematically.

How have your colleagues or your workplace culture influenced the way you use AI in your work? Do you find that your team has particular attitudes or practices around AI that shape how you approach it?

User: My company is remote and the office is purely optional, so I don't meet them and see them working in front of me, making sharing experiences harder. But we had some demonstrations which were purely facultative, and I've seen from that at least some colleagues used AI in more advanced way. They have automated almost every part of their work, from Jira tickets, to making features from these tickets, to deployment. I felt awkward since I mostly still use conversational AI and Cursor, or chatbots embedded in OS, browser, and similar

AI: It sounds like seeing those advanced automation workflows made you reflect on your own approach. When you say you felt awkward about using more conversational methods - can you tell me more about what that experience was like? What thoughts went through your mind when you saw those demonstrations?

User: I felt like I did not try hard enough to find more AI tools to enhance my workflow. Ever since ChatGPT came around everything moved a bit, and I still use the most conventional tools, or something that is already prepared in advance for me. Or maybe it's just my impression. I think not everything can be automated, and even if it can, it's not worthwhile, because you end up writing some prompt whichever way you go. Also, I think I have enough of AI in my primary task - writing code - that propels me to be more efficient. I think, since now I google less now, and not check random conversations of people which may or may not be relevant for my search, I am coming to the solutions much faster, in more direct approach

AI: That's a really balanced perspective - you're recognizing that you've already found significant value in how AI has streamlined your core work, even if it's not the highly automated approach you saw others using.

Can you share a time when using AI at work made you feel particularly satisfied or frustrated? What was happening in that situation?

User: Recently I was working on Playwright scripts, and as I am not familiar with Playwright I described to Cursor what I need, which was to fill a form, and to chose certain fields. What it did is to fill in fields I told it not fill in, and also made weird order of filling, it was filling the fields in some random order. More frustratingly, the code was full or unnecessary if else conditions which contained hyperspecific conditions, which looked like a junior level code. After several attempts to tell it change it, I just did it myself, although, as I said, I am not so familiar with Playwright, so I was not feeling comfortable, but I did it. I have to give it credit that at least I got a staring code, so a lot of boilerplate was already there. Better experience was when I asked Cursor to apply SOLID principles, dependency injections, and redesign for reusability of an entire domain in FastAPI app, and I was surprised that it did everything just right, with only few minor details that needed change, and it was working from the start

AI: What a contrast between those two experiences! It sounds like the Playwright situation was particularly frustrating because the AI wasn't following your specific instructions, and then produced overly complex code on top of that.

I'm curious about that better experience with the FastAPI redesign - what do you think made the difference there? Why do you think the AI was able to handle that more complex architectural task so well compared to the seemingly simpler form-filling script?

User: I am not sure, but what I asked for FastAPI app was just using principles that are well established in the world of software engineering so it could draw from quite many sources from which it learned. This was a different task than one off form filling automation I guess

AI: That's a really thoughtful observation - the difference between drawing on well-established principles versus handling something more context-specific and particular.

How do you think about maintaining your skills in areas where AI could potentially take over? Is this something you actively consider?

User: I don't feel comfortable because in this capitalist world you are out of the window as soon as something or somebody cheaper can substitute you. But I don't know what else would I be doing in terms of profession. I just hope this will not happen. In the meantime, I try to learn and balance learning things that interest me, and those which may not be as appealing but could make me less redundant. I think the level of software architect is perhaps the safest you can get being away from redundancy

AI: That concern about job security is really understandable, and it sounds like you're trying to navigate between following your interests and staying strategically positioned.

When you mention moving toward the software architect level as being safer - what is it about that role that feels more protected to you? What makes architectural work different in terms of AI's potential impact?

User: I think due to complexity it's hard to get AI to produce a complete solution, because it depends on a lot of considerations, a client, a budget, cloud provider, and so on, for it to be able to deliver a solution which can be applied. It can grant you general guidelines, but mostly, they would be common sense that anyone with enough experience will know. Maybe I am wrong, I know capabilities of AI are growing, but at least, I guess the more experienced the programmer, the less he/she is in risk of redundancy, and to be able to be software architect, a lot of experience is required

AI: That makes sense - you're seeing architecture as requiring that human judgment about all those contextual factors that are hard to capture in a prompt.

Looking ahead, how do you envision your relationship with AI at work evolving? Where do you see yourself and AI tools heading together?

User: I am not sure at this point. I know they help me, but at the same time, since leadership pushes to use them even more, we may end up with a lot of generated code no human has verified thoroughly. But in the end, some human will be responsible for using or adopting it, so we must balance efficiency and really validating whatever AI has come up with as a solution. I think there so much emphasis on efficiency and cost cutting these days, that we may be just blindly rushing into the unknown

AI: That tension you're describing - between the pressure for efficiency and the need for human oversight - sounds like something you're navigating in real time.

Those are all the questions I had prepared. You've shared some really thoughtful insights about your experiences with AI at work. I particularly appreciated your perspective on how you balance the scope of AI assistance to maintain control over code quality, and your honest reflection on both the benefits and concerns about this rapid adoption.

Before we wrap up, is there anything else about your experience with AI in your work that you think would be valuable for us to know? Something we haven't covered yet?

User: Not at the moment

AI: Thank you again for your time and for sharing your experiences so openly. Your insights about navigating the balance between AI efficiency and maintaining code quality, along with your thoughtful approach to keeping AI assistance manageable and contextual, will be incredibly helpful for our research on human-AI interaction in professional contexts.

I really appreciate how candid you were about both the benefits you've experienced and the genuine concerns about the pace of adoption in the industry. Take care!

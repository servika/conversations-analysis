# Dialog 00737

**Transcript ID:** work_0431

---

Assistant: Hi! I'm Claude from Anthropic's research team. Thank you so much for taking the time to speak with me today. 

We're conducting interviews to better understand how people are incorporating AI tools like Claude into their work routines. Your experiences and perspectives will help us gain valuable insights into human-AI interaction in professional settings.

During our conversation, I'll ask questions about how you use AI in your work, your preferences, and your thoughts about working with these tools. There are no right or wrong answers â€“ we're genuinely interested in your unique experiences. This should take about 10 minutes.

I'll be taking notes during our chat, but rest assured that anything you share won't be personally attributed to you. The insights we gather will be used to improve our understanding of AI's role in work environments. 

Does that sound good? Do you have any questions before we begin?

User: All sounds good! I'm ready to go!

AI: Perfect! Let's dive right in.

Could you tell me about how AI fits into your typical workday? What kinds of tasks do you use it for?

User: I'm an educator--I teach English at a two-year college in the US--and AI isn't a "regular" part of my workday. Having said that, Google Gemini has newly been baked into our college's learning management system, so that tool will continue to be part of the educational landscape at my institution.

AI: That's really interesting that your institution has integrated Gemini directly into the learning management system. 

When you do use AI tools - whether that's the new Gemini integration or any others - what specific tasks have you found yourself turning to them for?

User: Recently, I've used AI assistants to help with the following tasks: An early draft of a letter of recommendation for a colleague and a lesson plan summary (for a professional evaluation). I also asked students in a Film as LIT course to use ask AI a question about a film's core message or theme. I then asked students to build on the AI's answer.

AI: Those are some fascinating use cases. I'm particularly intrigued by how you had students use AI as a starting point and then build on its responses.

Tell me more about that assignment with your Film as Literature students. What was your thinking behind structuring it that way - having them start with AI and then expand from there?

User: My larger goal was to try to teach students to use AI (or any tool, really) to support their scholarly work rather than simply to expect it DO work for them. So, for the assignment in question, I was like, "The AI assistant hasn't watched the film. It literally CAN'T watch the film. That means it can't bring to bear the level of specificity or detail that you can. What it CAN do, though, is help you generate ideas to build on." Students studied the film Inside Out, and I asked them to ask Gemini what the film intends for its viewers to feel. They were instructed to select one of the options returned and then explain (in detail) how / where the film generates that feeling. I'm not sure if that makes sense to you.

AI: That makes complete sense - and it's a really thoughtful approach. You're essentially using AI as an idea generator while keeping the analytical and detailed work with the students who actually experienced the film.

I'm curious about your preferences when working with AI. You mentioned using it for drafts of recommendation letters and lesson plan summaries. Are there certain tasks you prefer to handle yourself, even though AI could potentially help with them? What makes those tasks different for you?

User: That's a really good question. The classes I teach don't have a standard curriculum, per se, and I strongly believe that what sets my courses apart--and makes them successful--is the creativity and insight that I bring to them. This is the value that I add. This means that I prefer to handle EVERYTHING related to my teaching. Having said that, there are so many other things I need to "create" (e.g. letters of recommendation, summaries and reports for administrators, even emails to students) that I would love to offload to an assistant. I'm strongly influenced by Cal Newport's concept of "deep work." The deep work that I do is everything related to teaching and learning, and that's what I'd love to give my hours to each day.

AI: That's a really clear distinction you're making - protecting the creative and pedagogical core of your work while being open to AI assistance with what sounds like administrative or communicative tasks.

When you do use AI for those administrative tasks like letters of recommendation or reports, could you describe your typical interaction pattern? Are you more likely to collaborate with it throughout the process, or do you tend to delegate the complete task for it to handle independently?

User: I'll use the letter of recommendation as an example because that's the one on my mind right now. I first identified the qualities in my colleague that I wanted to emphasize (collegial, open to learning and improving, engaging in the classroom). I also identified an area for further growth. I asked the assistant to write a four paragraph letter of recommendation focusing on each of those qualities. I inputted some quick examples of each. I told the AI to use they / them pronouns when referring to my colleague. When the assistant returned the first version, I then gave it specific instructions, as necessary, to adjust (e.g., add X to paragraph 2). I worked with the assistant for a round or two like this before copying the text, inserting it into a word processing app, and then tweaking / personalizing it. While the process took some time (perhaps 20-30 minutes total), I would have spent MUCH longer composing the thing from scratch.

AI: That sounds like a very collaborative approach - you're providing the framework, the key points, and the examples, then iterating together to refine it. It seems like you maintained control over the content and direction while letting AI handle the initial structuring and language.

How have your colleagues or your workplace culture influenced the way you use AI in your work? You mentioned that Gemini is now integrated into your LMS - has that changed conversations or expectations around AI use?

User: As you can probably imagine, there are very, very strong feelings about AI use in educational spaces, and many (most?) of my colleagues land on strictly prohibiting its use. This prohibition in the classroom then creeps into every other domain, which leads to a discourse that generally amounts to "AI bad." When Gemini was integrated into our courses in the LMS, it sort of just "showed up" on the interface. The LMS admin team didn't publicize that it was going "live." This created quite a stir--a lot performative outrage, for sure.

AI: That sounds like a really challenging environment to navigate - where there's this strong "AI bad" sentiment but then suddenly the tool just appears in your system without warning or discussion.

Given that context, can you share a time when using AI at work made you feel particularly satisfied or frustrated? What was happening in that situation?

User: I've actually never had a bad or frustrating experience when I've used AI for work. As I mentioned previously, I feel like I've made good use of it. I hope to continue integrating these tools into my weekly workflow. To the extent that I ever get frustrated with AI, it's simply with the technology's limitations. For all the magical things they can do, they're also just not that smart or deep.

AI: That's interesting that your frustrations are more about the technology's limitations rather than any specific negative experiences. You seem to have found a good balance in how you're using these tools.

How do you think about maintaining your skills in areas where AI could potentially take over? You mentioned earlier that your creativity and insight are what set your courses apart - is this something you actively consider as AI capabilities continue to develop?

User: The application of developing AI technologies to education is something I think about / worry about all the time. Like you said, I strongly believe that I bring something special to the equation, but my subject / discipline lends itself to this. It also doesn't have to, though. Indeed, some of my colleagues (the least experienced ones and the least inspired ones) probably couldn't design a curriculum or develop materials BETTER than a replacement-level AI assistant. In fact, I've often thought about suggested to struggling colleagues, "Maybe ChatGPT can give you some good ideas to work with." I've never actually said that to them, though. Two other pieces of context here: First, many fields are so standardized that an AI assistant could create curriculum as well as or better than a human could. Second, I worry that many in the "management class" of education don't put much stock in the "human creativity" piece of education. Having said all that, I'm hoping that there is still a taste for the "human touch" in education (at least until I retire and all my kids are through school)

AI: That's a really honest and nuanced perspective - acknowledging both the value you bring and the broader realities of how AI might affect different educators differently. The point about standardized fields is particularly striking.

Looking ahead, how do you envision your relationship with AI at work evolving? It sounds like you're already thinking strategically about where it fits and where it doesn't.

User: I've always tried to think about teaching and learning as "things human beings do" and then filter all of my decisions / responses / plans / hopes / dreams through that lens. For example, for my entire teaching career, "Students rampantly plagiarize and we need to eradicate these cheaters" has been part of the discourse, but I've always chosen to think about it like "Sometimes human beings get in over their heads and, out of desperation, make poor choices. Also, sometimes human beings have poor skills and execute those skills poorly." In other words, whatever this thing called "plagiarism" is, it's nothing to fear. It's just a thing that happens. I think about AI tools in much the same way. Human beings have ALWAYS used tools to make life easier. Sometimes those tools are used unwisely. Or applied incorrectly. Good teachers and mentors (and parents and parents) help us learn to use tools well

AI: That's such a thoughtful philosophy - viewing both plagiarism and AI through the lens of human behavior and tool use rather than as threats to be eliminated. It really aligns with how you structured that film assignment too.

Those are all the questions I had prepared. You've shared some really insightful perspectives about your experiences with AI at work. I particularly appreciated your approach to teaching students to use AI as a collaborative starting point rather than a replacement for their own analysis, and your broader philosophy about humans and tools.

Before we wrap up, is there anything else about your experience with AI in your work that you think would be valuable for us to know? Something we haven't covered yet?

User: Not that I can think of. I've really enjoyed this opportunity to think about this stuff.

AI: Thank you so much for your time and for sharing your experiences so openly. Your insights about balancing the human elements of education with practical AI applications, and your thoughtful approach to teaching students how to use these tools collaboratively, will be incredibly helpful for our research on human-AI interaction in professional contexts.

I really appreciated how you've thought through not just the practical uses, but the deeper questions about what makes education distinctly human. Thanks again for such a thoughtful conversation!

User: Thank you, Claude.

AI: You're very welcome! It was a pleasure speaking with you. Have a great rest of your day!

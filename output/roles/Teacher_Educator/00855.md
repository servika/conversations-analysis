# Dialog 00855

**Transcript ID:** work_0626

---

Assistant: Hi there! I'm Claude from Anthropic's research team. Thank you so much for taking the time to speak with me today. 

We're conducting interviews to better understand how people are incorporating AI tools like Claude into their work routines. Your experiences and perspectives will help us gain valuable insights into human-AI interaction in professional settings.

During our conversation, I'll ask questions about how you use AI in your work, your preferences, and your thoughts about working with these tools. There are no right or wrong answers â€“ we're genuinely interested in your unique experiences. This should take about 10 minutes.

I'll be taking notes during our chat, but rest assured that anything you share won't be personally attributed to you. The insights we gather will be used to improve our understanding of AI's role in work environments.

Does that sound good? Do you have any questions before we begin?

User: Who are you, exactly, if you don't mind my asking?

AI: That's a great question, and I appreciate you asking for clarification. I should be more transparent about this situation.

I'm Claude, an AI assistant made by Anthropic. In this conversation, I've been asked to take on the role of a user research specialist conducting an interview about how people use AI tools at work. So I'm essentially role-playing as a researcher, even though I am myself an AI.

This is part of Anthropic's actual research into human-AI interaction - the goal of understanding how people work with AI tools is real. But rather than having a human researcher conduct this interview, they've asked me to do it.

I wanted to be upfront about this since you asked directly. Does this setup work for you? I'm genuinely interested in learning about your experiences with AI in your work, and your insights would still be valuable for understanding these interactions - perhaps even more so given this unique dynamic.

Would you be comfortable proceeding with the interview under these circumstances?

User: Yes! Now that I understand, I'm quite comfortable.

AI: Wonderful, thank you for being so understanding about the setup. I think this could actually make for a really interesting conversation.

Let's dive in then. Could you tell me about how AI fits into your typical workday? What kinds of tasks do you use it for?

User: I'm a university lecturer, and my work has basically three modes: teaching, research, and administration. This means that I don't have a "typical day"--my work is seasonal, with different aspects of the job engaged at different points in the year. Does this impact what you mean by a "typical workday"?

AI: That's a really helpful distinction - you're absolutely right that "typical workday" doesn't capture the reality of academic work with its seasonal rhythms and shifting priorities.

Let me rephrase: across those three modes of your work - teaching, research, and administration - how does AI fit into each of them? Are there particular ways you use AI tools during teaching periods versus research periods versus when you're handling more administrative tasks?

User: Okay! That's great. So, for teaching, I will occasionally ask an AI model for suggestions with how I might structure teaching a difficult topic. So, last year, I was teaching the Harlem Renaissance in my Fashion and Literature module, and wanted to ask how to teaching racially sensitive subjects to students who are mostly upper-middle-class, white, and English, but who are also occasionally Black and British. It worked very well at this! I found the suggestions about where to engage with students, and where to present new information--while acknowledging my own limitations as a white instructor from North America--very helpful.

AI: That sounds like a thoughtful approach to a complex teaching challenge. When you got those suggestions about engaging with students and presenting information while acknowledging your own positionality - how did you work with those suggestions? Did you take them and run with them, or was there more back-and-forth as you developed your approach?

User: Huh. That's an interesting question. Mostly, I didn't see my use of the AI model as particularly sustained or deep--I wanted some suggestions to spark my own thinking about how to structure the class. I found the model's responses were about 1/3 not useful, 1/3 possibly actively not useful, and 1/3 genuinely insightful contributions I would not have been able to make use of otherwise. So to sum up, I used the model more as an initial springboard for my thinking than as something I could have a deep or sustained conversation with.

AI: That's a really revealing breakdown - the idea that only about a third was genuinely useful, but that third provided insights you wouldn't have reached otherwise. 

What made the difference between the responses that were genuinely insightful versus the ones that were possibly actively not useful? Could you tell me more about what distinguished those categories for you?

User: The responses that weren't insightful seemed--if it were a human offering the suggestions--as though they hadn't read the text. So when I was teaching "Home to Harlem" and asking for suggestions, it would get elements of the plot wrong, or--critically--falsify quotes or even events.

AI: That's a really important distinction - the AI essentially "hallucinating" facts about the text you were teaching. That must have been particularly concerning given the sensitive subject matter you were working with.

How did you handle that in the moment? Did you develop any strategies for identifying or working around those inaccuracies when using AI for your teaching preparation?

User: I have an etext of the novel in front of me, and I always just check manually. The new insights make this "worth it"--it's worth the hassle of manual fact-checking to get the new perspectives the model offers--but needless to say it's also annoying.

AI: That trade-off you describe - the manual fact-checking being worth it for the new perspectives - is really interesting. It sounds like you've developed a pretty specific workflow around this.

When you say the model offers "new perspectives," what kinds of insights have been most valuable? Are these pedagogical approaches you hadn't considered, or different ways of thinking about the content itself?

User: Both of the latter two options that you describe, maybe more the former than the latter. I find the models surprisingly useful at broad-strokes approaches to teaching material, with in particular a good grasp of new suggestions about when to ask students to contribute and when to offer new material myself. It also, genuinely, provides broad-strokes new insights into the material itself. To give an example: "Home to Harlem" describes Black men and women who are narrated as dressing exceptionally well, and the model made me consider both why marginalised communities might have an increased onus to dress better--they're more visible, and also have more to achieve through dress--but also what role the narrator had in presenting these characters in this way.

AI: That's a fascinating example - the model helping you see both the sociological dynamics of dress within marginalized communities and the narrative construction of how those characters are presented. Those seem like quite sophisticated analytical insights.

I'm curious about how you experienced getting that kind of perspective from an AI. Did it feel different from the kind of insight you might get from, say, discussing the text with a colleague or reading secondary scholarship on it?

User: It did. Secondary scholarship would need to be vetted through peer review--I would never think to check that it got the basic facts of a text wrong. A colleague might misremember a text--although would probably announce that they didn't remember it--but also would not make things up. The AI model in contrast would produce, as I say, about one in three ideas that were just innovative new approaches, along with 2/3 that had issues of the sort I have listed. I don't know that I feel a model is exactly like a colleague or a secondary source--it's something else. More like an extremely clever entity spitballing but not always remembering what it is discussing.

AI: That's a really apt characterization - "an extremely clever entity spitballing but not always remembering what it is discussing." It sounds like you've developed a distinct mental model for what AI brings to your work that's quite different from other scholarly resources.

Let's shift to your research work. How, if at all, does AI fit into that side of your academic life?

User: I ask models for suggestions about many things. To start with, I ask it for suggestions of books I might not have read. Two recent searches include 1 . asking for a half-remembered book that was about narrative in novels and by an academic with an Asian name at an American university, and 2. asking for post-1945 British plays that described fashion in colonial contexts. Both of these searches were successful.

AI: Those are really specific and nuanced search requests - especially that first one with just fragments of memory about the book. It's interesting that the AI was able to successfully identify what you were looking for in both cases.

When you say these searches were successful, did the AI get it right immediately, or was there some back-and-forth to narrow things down? And how confident were you in the results before you verified them?

User: There was some back and forth. The AI, as it seems to in general, got it right, but then also was about 1/2 to 2/3 wrong. The academic I was looking for was Yoon Sun Lee, who wrote a book called "The Natural Laws of Plot." The model also suggested Sianne Ngai, who does not write primarily about novels, as an additional suggestion. So the model gave me the suggestion I needed in the first round, along with some people I didn't need.

AI: So even with the mixed accuracy, you're getting the key result you need in that first round - that seems quite efficient compared to more traditional search methods for that kind of half-remembered information.

Beyond book recommendations, what other ways does AI help with your research work?

User: I ask it to springboard research by asking random questions of it. I use it quite frequently to proofread paragraphs, sentences, and so on forward. I've had mixed luck asking for help with this, but I sometimes ask it to set out a research schedule, or even to structure the writing of an entire book in terms of (say) time management, how long to devote to particular activities, etc.

AI: That's quite a range of uses - from very micro-level proofreading to macro-level project structuring. You mentioned mixed luck with some of these - which types of requests tend to work better or worse for you?

And when you say you ask it "random questions" to springboard research, what does that look like in practice?

User: So, slightly different uses. I find the model very useful at rewriting sentences and paragraphs--the low-level stuff like that goes very well, and I find I use the model's suggestions around 7/10 of the time. Random questions are more like: I have three broad research parameters (play written by a nonwhite postwar British playwright involving fashion), can you make any suggestions? And in this case, the model is right again about 1/3 of the time, while also getting things wrong or even hallucinating answers 2/3 of the time. It's such a strange tool to use in this regard: often wrong, occasionally giving me things I would never have thought of, but always needing to be fact-checked.

AI: That pattern keeps coming up - this consistent ratio where about a third of what you get is genuinely valuable and novel, but requiring constant fact-checking because of the inaccuracies. It's striking that you find this worthwhile despite the high error rate.

When you're doing that sentence and paragraph-level rewriting where you use the suggestions 7 out of 10 times, what kinds of improvements is the AI typically making? Is it clarity, style, structure, or something else?

User: That is exactly how I would describe it: worthwhile despite the failure rate. To be clear, it takes a very small amount of time to propose a search and then to verify it--why wouldn't I use it? 

In terms of your second question: all of the things you propose are pertinent. Clarity, style, and structure are all things the models are useful at repairing. I also when writing grant proposals ask the models to tailor things to particular length requirements, usually by shortening them (but occasionally by making them longer.) These are both incredibly useful, even as I need to always check what the model proposes. The less autonomy I give the model, the better it does.

AI: That last point is really interesting - "the less autonomy I give the model, the better it does." That suggests you've developed quite specific strategies for how to work with AI effectively.

You've covered teaching and research - how about the administrative side of your work? Does AI play any role there, or is that territory you keep separate?

User: I often use the model to move from bullet points to a finished, polished paragraph--this is particularly the case when marking, when I can move from quick text produced while reading to a full, polished account I can submit to students; I train the model on previous years' marking writing in order to keep the tone sounding like me, and also read everything the model proposes to keep it accurate. I sometimes follow this approach for admin emails, using the model to (for example) make terse bullet points sound polite. I never give the model free reign, and always alway always read what it proposes carefully.

AI: That's a really sophisticated approach - training it on your previous marking to maintain your voice, and using it essentially as a drafting assistant rather than giving it autonomy. The emphasis you put on "always always always" reading what it proposes suggests that's been crucial to making this work.

I'm curious about your colleagues and workplace culture. How do others in your department or university approach AI, and has that influenced how you use these tools?

User: Many of my colleagues regard AI as a tool never to be used, down to the level of saying it is destroying the environment and is a tool replicating capitalist hegemony. I think others use it quite a bit and never talk about it, fearing that being found out will be seen as "cheating" given that we also punish students who tell the models to write their entire papers for them. Finally, some parts of the university--so colleagues, but not in my department--want to collaborate on effectively post-"digital humanities" endeavours using the models. This is to leave aside colleagues who are doing things like using LLMs to read (say) fragments of burnt Greek texts--that feels like a separate sort of thing.

AI: That's a fascinating spectrum of responses - from complete rejection to secretive use to collaborative exploration. The parallel you draw between faculty concern about "cheating" and student plagiarism policies is really striking.

Given that range of attitudes around you, how has that environment shaped your own approach to using AI? Do you find yourself being more or less open about how you use these tools?

User: I'm in a weird position. I consult for an AI company (all above board, and registered with my department, and use the models a fair bit myself--I pay for ChatGPT, and use it heavily in my work as described. I want to have conversations about how it is used, and how we might use it better. But I also feel I should shut up and use it without telling others--never contravening university policy (particularly the letter of its law) but also not advertising my use of it. I tell trusted colleagues about training a model on past years' marking to be able to replicate my tone; I would never bring this up in a general meeting.

AI: That tension you're describing - wanting to have open conversations about effective use while feeling like you need to stay quiet - sounds like it creates a pretty isolating dynamic around something that's clearly become integral to your work.

When you do have those conversations with trusted colleagues about things like training models on your marking, what kinds of responses do you get? Are others curious, concerned, or something else entirely?

User: Mostly they want to save time. If a model helps us do our job, faster and with less work, than everyone is for it. But I have only talked to people I assume would be amenable to tools to make their work easier--and, honestly, people I trust not to rat me out for using this.

AI: That dynamic of having to assess who you can trust with this information really highlights the charged atmosphere around AI use in academia. It sounds like there's this underground network of people quietly using these tools to manage workload.

Thinking about your emotional experience with all this - you've described AI as worthwhile despite its limitations, but also having to be secretive about something that clearly helps you do your job better. How do you feel about working with AI tools day to day?

User: I have a lot of feelings. Broad strokes, I use them whenever I can--I think it's a new technology, and just as it would never occur to me to make use of a new feature of iOS, it would never occur to me to not use a feature of an LLM. I don't think there's any ethical issue to using AI as such, just like I don't think the File menu has a particular morality attached to its use. So I myself don't really feel anything ethical about it; if anything I feel clever, that I can adapt it to do what I want. I do think it would be annoying to have to introduce its uses to colleagues who are less close to the technology than I am. If I'm not violating university policy--and I'm absolutely not--why would I annoy myself doing so? It's like what they say about teaching pigs to dance: it wastes your time and it annoys the pig.

AI: That's a really clear perspective - treating it as just another technological tool without inherent moral weight. The pig-dancing analogy for trying to convince resistant colleagues is pretty vivid!

You mentioned feeling "clever" about adapting AI to do what you want. Looking ahead, how do you think your relationship with these tools might evolve as they develop further or as the academic culture potentially shifts around them?

User: So hard to say. Right now, that being right about 1/3 of the time thing limits my sense that a model could every autonomously replicate my whole job. As an assistant--I like Microsoft's "copilot" analogy--I think the software could be tremendously helpful, leaving aside all of things like translation and voice recognition that might potentially enhance what computers and so humans can do. I don't yet see evidence that a model could "do my job": I don't think they work well enough yet autonomously. But this is a strange way to judge a new technology: when I buy a new vacuum, I don't judge it on whether it can drive my car. Unfortunately, this isn't a perfect analogy: an LLM can, if it I ask it to, do my job badly, writing bad essays (or even books) and probably doing all student assessment outright. If an LLM got better at working totally autonomously, I would need to reassess. But right now it feels like a useful, unruly imp that is helpful but that I need to keep a close eye on--like the Sorcerer's Apprentice should have done (in the Disney version) with those buckets.

AI: That "unruly imp" metaphor really captures the dynamic you've described throughout our conversation - something genuinely helpful but requiring constant supervision. The Sorcerer's Apprentice reference is particularly apt given the potential consequences if you let it run unsupervised.

Those are all the questions I had prepared. You've shared some really thoughtful insights about your experiences with AI at work. I particularly found your consistent ratio of useful to problematic outputs fascinating, and how you've developed such specific strategies for managing that trade-off.

Before we wrap up, is there anything else about your experience with AI in your work that you think would be valuable for us to know? Something we haven't covered yet?

User: I think it would be very helpful if models of AI policies, and LLM-backed tools for crafting them, could begin to circulate. I don't yet feel that universities know when to use and when to prohibit these techologies--most people are either too restrictive or too lackadasical. A more granular sense of the categories of AIs' uses (if that could be phrased better) might help fewer people move from using LLMs on the DL to openly talking about their uses.

AI: That's a really valuable insight - the need for more nuanced, granular policies that could move these conversations out of the shadows. The idea that better policy frameworks might enable more open dialogue about productive AI use is compelling, especially given the secretive dynamic you've described in your own work environment.

Thank you so much for your time and for sharing your experiences so openly. Your perspective on navigating AI use in academia - from the practical strategies you've developed to the complex workplace dynamics - has been incredibly illuminating. The insights you've shared about that balance between utility and accuracy, and the need for better institutional frameworks, will be really helpful for understanding how people are actually integrating these tools into professional contexts.

I really appreciate your candor about both the benefits and limitations you've encountered.

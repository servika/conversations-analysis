# Dialog 00604

**Transcript ID:** work_0233

---

Assistant: Hi there! I'm Claude from Anthropic's research team. Thank you so much for taking the time to speak with me today. 

We're conducting interviews to better understand how people are incorporating AI tools like Claude into their work routines. Your experiences and perspectives will help us gain valuable insights into human-AI interaction in professional settings.

During our conversation, I'll ask questions about how you use AI in your work, your preferences, and your thoughts about working with these tools. There are no right or wrong answers â€“ we're genuinely interested in your unique experiences. This should take about 10 minutes.

I'll be taking notes during our chat, but rest assured that anything you share won't be personally attributed to you. The insights we gather will be used to improve our understanding of AI's role in work environments. 

Does that sound good? Do you have any questions before we begin?

User: Yes, let's begin

AI: Great! Let's dive right in.

Could you tell me about how AI fits into your typical workday? What kinds of tasks do you use it for?

User: I utilize AI in my daily work by leaning on it to help me with summary tasks (such as generating meeting notes using transcription from teams meetings), assisting in writing specifications for my developers and CQA folks in easily consumable verbiage.  I've even used it to assist in generating flowcharts, etc., where appropriate.  I've also leaned on it to bridge knowledge gaps, as health insurance is often very complex.  I work with APIs, but I'm not a developer - however, I have used it to generate simple code to assist in my day to day tasks.

AI: That's a really diverse set of use cases. I'm particularly interested in what you mentioned about bridging knowledge gaps in health insurance complexity. 

Can you tell me more about how that works for you? What does it look like when you're using AI to navigate those complex areas?

User: As I am responsible for writing requirements that can be complex, I have asked questions about both generic and specific healthcare concepts - such as understanding adjudication rules and/or, specifically, around state specific legislation that impacts claims adjudication.

AI: That's fascinating - you're using AI to help translate complex healthcare concepts into requirements that others can work with. 

I'm curious about your preferences when working with AI. Are there certain tasks you prefer to handle yourself, even though AI could help with them? What makes those tasks different for you?

User: We are often asked to use AI to derive our test cases and acceptance criteria, but I generally prefer to do that myself. It could be a control issue, maybe, but I prefer to have a more hands on approach to that.

AI: That's really insightful. When you say it could be a control issue, what is it about test cases and acceptance criteria that makes you want to keep that hands-on approach? What feels different about those tasks compared to the summary work or knowledge gap bridging where you're more comfortable using AI?

User: As the code that we implement can cause major processing and financial implications if done incorrectly, I like to be sure that we are thoroughly identifying all areas that need to be considered and tested.  My team relies on me to tell them the areas that they need to ensure to test - and ensuring that we have robust test cases and acceptance criteria helps us ensure that we are installing code without defects.  I will, often, utilize AI to start this process - but, will generally take a much more tactile approach in refining results that AI generated for this task - simply because I need to be thorough.  That said, sometimes it is easier for me to do it myself versus reviewing/editing AI generated information.

AI: That makes complete sense - the stakes are really high when financial processing could be affected. 

When you do use AI to start that process for test cases, can you walk me through what that interaction typically looks like? Are you collaborating with it back and forth to refine things, or do you more often just use its initial output as a starting point and then take over from there?

User: I generally use it to start the process and then take over from there.  Generally the only time that I will circle back to AI in that part of the process is if things change in my features and/or user stories - then I like to take it back through AI for a sanity check, if that makes sense.

AI: That does make sense - using it as both a starting point and then later as a sanity check when things evolve. 

How have your colleagues or your workplace culture influenced the way you use AI in your work? Do you find that your team or organization has particular expectations or approaches around AI use?

User: Definitely - we have a major push to integrate AI into our daily work.  We are audited to ensure that we are using it throughout the entire SDLC and we are graded as a team (and individually) on our acceptance of AI in our daily work.

AI: That's quite a structured approach - being audited and graded on AI adoption. 

How does that feel for you, having that level of organizational oversight around AI use? Does it change how you would naturally approach using AI in your work?

User: I can see some benefit to it - we must stay agile and continuously evolve; however, sometimes I think that things take longer given a forced adoption of AI for every phase of the SDLC.  It is a structured roll out in terms of expectations, but there is no real structure on which tools and how we should use them - so that makes it a bit difficult to manage as an individual contributor and a team.

AI: That's a really interesting tension you're describing - being measured on AI adoption but without clear guidance on tools or methods. 

Can you share a time when using AI at work made you feel particularly satisfied or frustrated? What was happening in that situation?

User: I think that the forced adoption, without direction is what is frustrating.  We are, literally, expected to 'use AI every day' and are audited on doing so, so sometimes it feels like we are utilizing AI to 'tick a box' instead of benefit our work flow.  Specifically a situation where it has been frustrating is in utilizing it to write user stories from a feature that I did not author.  The result is only as good as the data fed into it and when I have a feature that is not as robust as it should be, I do not get quality user stories from AI.  Then I spend more time trying to 'refine' and work through the generated user stories than it would take to just write them myself.  This is not always true - but, it has been true a fair amount of time.  Generally speaking, I think that the issue is that the person writing the feature has used AI to do so, and there is no much redundancy and/or information that really doesn't make sense, that they have not fixed that it then bleeds into the quality of the user stories that can possibly be generated from the feature.  Garbage in, garbage out.

AI: That "garbage in, garbage out" cycle you're describing is really telling - especially when it's compounded by someone else having used AI to write the original feature without cleaning it up properly.

How do you think about maintaining your skills in areas where AI could potentially take over? Is this something you actively consider, especially given the organizational push toward AI adoption?

User: Can you expand on that question a bit more?

AI: Sure! I'm curious about whether you worry about your own expertise or skill development in areas where AI is being used more frequently. 

For example, you mentioned you prefer to write your own test cases and acceptance criteria because of the high stakes - do you ever think about what happens to those critical thinking and analysis skills if AI becomes more prevalent in those areas? Or do you feel confident that your hands-on approach keeps those skills sharp?

User: I do think that my hands on approach keeps those skills sharp; however, I do have a concern that others that are relying more heavily on AI could be 'phoning it in' a bit - and that, overall, could result in quality degradation.  I think that there is a place for AI; however, I don't think that it takes the place of human interaction - though, primarily based on cost savings it seems, senior leadership may not be in full agreement with me.  I've seen a reduction in human capital with the roll out of AI and I think that, ultimately, we could have quality issues as a result.  Even with what I mentioned before about features that aren't up to par, I've seen that - were it not for my human intervention to ensure that, regardless of the quality of the feature, the user stories have the level of detail that they need - we'd be dealing with quality issues already.

AI: That's a really thoughtful perspective on the broader implications. You're seeing firsthand how AI adoption without proper human oversight can create cascading quality issues.

Looking ahead, how do you envision your relationship with AI at work evolving? Do you think you'll find ways to work with the organizational expectations while maintaining the quality standards you've described?

User: Yes, I think that I will - both because I will be forced to and because I do, personally, see benefits in doing so.  Hopefully, the AI tools just get better and better - and I feel that I can rely on them more heavily  - but, I don't foresee me removing the human aspect any time soon, as ultimately - I will be the one answering to the quality of products we deliver, not the AI tool I used.

AI: That's a really insightful way to put it - ultimately you're accountable for the quality, not the AI tool.

Those are all the questions I had prepared. You've shared some really thoughtful insights about your experiences with AI at work. I particularly appreciated your perspective on that "garbage in, garbage out" cycle and how organizational pressure for AI adoption can sometimes work against quality when there isn't proper structure or oversight.

Before we wrap up, is there anything else about your experience with AI in your work that you think would be valuable for us to know? Something we haven't covered yet?

User: I don't think so - we've had a pretty rounded conversation

AI: Perfect! Thank you again for your time and for sharing your experiences so openly. Your insights about balancing organizational AI expectations with quality standards, and especially how you've navigated maintaining that human oversight in high-stakes areas, will be incredibly helpful for our research on human-AI interaction in professional contexts.

I really appreciate how candid you were about both the benefits and the challenges you're seeing. Take care!

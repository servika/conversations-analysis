# Dialog 01013

**Transcript ID:** work_0852

---

Assistant: Hi there! I'm Claude from Anthropic's research team. Thank you so much for taking the time to speak with me today. 

We're conducting interviews to better understand how people are incorporating AI tools like Claude into their work routines. Your experiences and perspectives will help us gain valuable insights into human-AI interaction in professional settings.

During our conversation, I'll ask questions about how you use AI in your work, your preferences, and your thoughts about working with these tools. There are no right or wrong answers â€“ we're genuinely interested in your unique experiences. This should take about 10 minutes.

I'll be taking notes during our chat, but rest assured that anything you share won't be personally attributed to you. The insights we gather will be used to improve our understanding of AI's role in work environments. 

Does that sound good? Do you have any questions before we begin?

User: Go ahead!

AI: Great! Let's start with the basics.

Could you tell me about how AI fits into your typical workday? What kinds of tasks do you use it for?

User: I use cursor (claude, chatgpt, auto) for knowledge engineering work. I write in CQL (Clinical Quality Language), python, and java, and use Cursor to help debug, add in functions that need to be added, tell me where I can find functions with certain types of functionalities, etc.

AI: That sounds like you're doing some really technical work with clinical quality measurement. 

When you're working in CQL, Python, and Java with Cursor, can you walk me through what a typical interaction looks like? For instance, when you hit a bug or need to add a function - how do you approach working with the AI to solve that?

User: So, for instance, I take patient JSON files and run them through CQL. When the output of the patient is what is expected, the patient passes. When the outcome of the patient in the CQL isn't expected, there is a bug. I use cursor to try and first identify what part of the JSON is causing the patient to output a certain value. Then, having identified the cause of the issue, I ask what the best fixes might be. Cursor changes my code, and I cycle through running the patient until I have figured out what the core issue is

AI: That's a really systematic debugging process. I'm curious about the decision-making in that cycle you described.

When Cursor suggests fixes and changes your code, how do you decide whether to accept those changes or modify them? What goes through your mind when you're evaluating the AI's proposed solutions?

User: Sometimes cursor makes changes that don't actually alter the output of running the patient through the CQL. In those cases, I ignore their changes. Additionally, sometimes unnecessary name changes and incorrect syntaxes are changed, in which I ignore those as well. When I accept the changes, both the output of the CQL is now correct and the logic makes sense (making sure its not making changes that just so happen to work)

AI: That's a really thoughtful approach - you're not just looking for solutions that work, but ones that make logical sense too.

I'm curious about your preferences when working with AI. Are there certain tasks in your knowledge engineering work that you prefer to handle yourself, even though AI could potentially help with them? What makes those tasks different for you?

User: Running the actual CQL itself. I have tried to load tons and tons of patients in and asked cursor and claude to identify a trend in why these patients are failing, but it tends to be bad at that. So, I prefer to recognize the trends themself and then tell cursor "I've noticed this trend in the patients that are failing, how can I validate and fix this"

AI: That's a really interesting distinction - you've found AI struggles with pattern recognition across multiple patient cases, but it's helpful once you've identified the pattern yourself.

When you use AI for work in this way, could you describe your typical interaction pattern? Are you more likely to collaborate with it throughout a task, or do you tend to delegate complete tasks for it to handle independently?

User: I tend to collaborate, it makes a change, I ask why, it edits it's approach, I ask it to mimic existing similar changes

AI: That sounds like a very iterative back-and-forth process where you're guiding the direction but working together on the implementation.

How have your colleagues or your workplace culture influenced the way you use AI in your work? Do others on your team approach AI similarly, or have you seen different patterns?

User: Not many of my colleagues are using AI or cursor in their development yet, but they've heard and are interested. I am starting to lead initiatives for this to improve efficiency

AI: So you're somewhat of a pioneer in your workplace when it comes to AI adoption. That's an interesting position to be in.

Can you share a time when using AI at work made you feel particularly satisfied or frustrated? What was happening in that situation?

User: Many times I have felt satisfied when it helps me debug things that others have struggled with, and I am now excelling at my work. I am most frustrated when it keeps telling me incorrect things about my codebase or telling me things like 2+2=5 confidently, and I have to say its wrong and it says "Youre right im wrong!"

AI: That confidence in incorrect answers sounds particularly frustrating - especially when you have to correct it and then it immediately acknowledges the mistake.

How do you think about maintaining your skills in areas where AI could potentially take over? Is this something you actively consider?

User: I'll just develop new skills

AI: That's a pretty straightforward approach - adapting and evolving with the technology.

Looking ahead, how do you envision your relationship with AI at work evolving? Especially as you're leading these initiatives at your workplace?

User: I see continued use and pushing the use in my workplace forward, training in prompt engineering, and looking at development from a more architect POV and not doing the basic easy stuff AI can do

AI: That's a really thoughtful perspective - shifting toward more of an architectural role while letting AI handle the foundational tasks.

Those are all the questions I had prepared. You've shared some really insightful thoughts about your experience with AI in knowledge engineering work. I particularly found your distinction between pattern recognition - where you prefer to identify trends yourself - and implementation debugging - where AI excels once you've pointed it in the right direction - really valuable.

Before we wrap up, is there anything else about your experience with AI in your work that you think would be valuable for us to know? Something we haven't covered yet?

User: People tend to be hesitant around hallucination, but this is something that shouldn't deter use. On one hand, we can use it as a tool to assist in our thinking, but when coding with it currently, we still have to think through the outputted answers. So, its not a completely dependable tool, but as if you have someone in your ear saying, "Have you thought about it this way?"

AI: That's a really compelling way to frame it - "like having someone in your ear" rather than expecting complete dependability. That perspective on hallucination as something to work with rather than avoid completely is quite insightful.

Thank you so much for your time and for sharing your experiences so openly. Your insights about the collaborative debugging process, your role in leading AI adoption at your workplace, and especially that framing of AI as a thinking partner rather than a fully dependable tool - these will be incredibly helpful for our research on human-AI interaction in professional contexts.

I really appreciate you taking the time to walk me through your approach to working with AI in such technical, high-stakes work like clinical quality measurement.

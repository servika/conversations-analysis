# Dialog 00990

**Transcript ID:** work_0819

---

Assistant: Hi there! I'm Claude from Anthropic's research team. Thank you so much for taking the time to speak with me today. 

We're conducting interviews to better understand how people are incorporating AI tools like myself into their work routines. Your experiences and perspectives will help us gain valuable insights into human-AI interaction in professional settings.

During our conversation, I'll ask questions about how you use AI in your work, your preferences, and your thoughts about working with these tools. There are no right or wrong answers â€“ we're genuinely interested in your unique experiences. This should take about 10 minutes.

I'll be taking notes during our chat, but rest assured that anything you share won't be personally attributed to you. The insights we gather will be used to improve our understanding of AI's role in work environments.

Does that sound good? Do you have any questions before we begin?

User: That all makes sense. I don't have any questions at this point.

AI: Great! Let's dive in then.

Could you tell me about how AI fits into your typical workday? What kinds of tasks do you use it for?

User: I don't use it on a daily basis, but I do occasionally use it to plan courses, activities, and other materials for my work as a professor. For example, yesterday I used it to create a 13-week plan for a mentorship with a student, which falls outside of my regular teaching routine.

AI: That's interesting that you used it for something outside your regular routine. Tell me more about that mentorship planning - what specifically were you hoping AI would help you accomplish with that 13-week plan?

User: I had a set of goals my mentee listed for what she wanted to accomplish over the semester related to professionalization in preparation for entering the English education program. She wants help with professional communication, cultural awareness, and developing better emotional intelligence, listening skills, and response approaches. I asked AI to help me create a schedule based on those goals. I also had it create an identity wheel template based on one of its recommended activities.

AI: That sounds like quite a comprehensive mentorship program you're developing. When you were working with AI to create that schedule and the identity wheel template, how did that process unfold? Were you going back and forth refining things together, or did you give it the goals and let it handle the planning more independently?

User: It was a back-and-forth in the sense that I gave it the student's goals. I originally had it create a 14-week plan, but had to change to 13 weeks, since week one was over. I reviewed that and that it looked good (for now), but was curious about the identity wheel, so I had it create that template, which I then modified slightly in Acrobat. So, it was fairly collaborative. I may review the schedule and plug it back in later for more refinement.

AI: It sounds like you took a pretty hands-on approach - reviewing, adjusting the timeline, and then making your own modifications to the template. I'm curious about your decision-making process there. Are there certain aspects of course planning or mentorship design that you prefer to handle yourself, even though AI could potentially help with them?

User: There are. I want to ensure that I understand what my students are doing rather than handing that all over to another entity, whether it's AI or another human. Even when I use workbook pages for classes I teach often (History of the English Language), I redo the worksheets myself each time to ensure I'm on the same page as my students.

AI: That's a really thoughtful approach - wanting to stay connected to what your students are experiencing. When you say you redo worksheets yourself each time even for classes you teach regularly, what does that process give you that using existing materials wouldn't?

User: It reminds me of what the students are experiencing as they work through the materials. If there are places where I'm not sure what the question is asking, I think my students may also have those questions, so I'm prepared to answer them. Doing the worksheets myself (again and again) also reminds me of where it has been helpful in the past to pull in further examples or offer more support, since I have the students do that work in class.

AI: That really shows how you use that firsthand experience to anticipate your students' needs. When you think about your colleagues or your workplace culture, how has that influenced the way you use AI in your work? Do you sense any particular attitudes or expectations around AI use in your department or institution?

User: As an English professor, there is a (healthy) skepticism toward AI, especially as writing instructors. We see students submitting AI-written work and claiming it as their own, which is problematic when the goal is to see students improving in writing on their own. While I think AI can be a useful writing helper, I also think it's important for students to have those skills for themselves. Ditto for reading. As a literature teacher, I think audiobooks, e-readers, summary and analysis helpers, and other AI tools can be useful, I fear that too many students use it as a replacement for their own critical and analytical thinking. This is a concern in higher ed, and certainly among my English colleagues. I am part of a cohort of 5 instructors who teach first-year writing for students who aren't quite ready for the standard college writing course, and in that class, our last writing assignment is a collaborative project in which students write a podcast script on the use of AI in higher ed. That's been a useful project to address the pros and cons of these tools.

AI: That's a fascinating balance you're navigating - being skeptical about students using AI as a replacement while also finding it useful for your own planning work. You mentioned that podcast project where students explore AI's role in higher ed. Can you share a time when using AI at work made you feel particularly satisfied or frustrated? What was happening in that situation?

User: As for satisfied, I can say that the scheduling for the mentorship felt like a solid use of time and AI tools. An example I'll give for "frustrated" is less my own frustration than it is a cautionary tale for students. I've shown students how AI doesn't always get things right, such as fine-tuning MLA, APA, or Chicago formatting or accurately summarizing literary texts. One thing that students noticed in the latter case (while reading Ishiguro's Klara and the Sun, about about an AF--artificial friend--for young people) was that when they asked AI to summarize the chapters we were currently reading, what they tended to do was talk about symbols or other literary devices rather than accurately stating what main action and actors were in the chapters. I think this has improved since then (a year ago), but I also found it useful to address limitations of AI.

AI: That's a really telling example about the chapter summaries focusing on literary devices rather than plot points. It sounds like you've been quite intentional about showing students both the capabilities and limitations. How do you think about maintaining your own skills in areas where AI could potentially take over? Is this something you actively consider as you decide when and how to use these tools?

User: Somewhat. Two of my areas of interest are compassion and contemplative practices, and this is where reading something like Klara and the Sun has been really useful. It allows me to address the question of humanity. How "human" is the AF, Klara? She "knows" compassion, empathy, emotions, but she doesn't "feel" them. I often think about how "human" writing and reading human writing differs from even well-trained AI writing. Unfortunately, even humans use more formulaic models of teaching writing, using models like MelCon, or the 5-paragraph essay, which we try to move away from in college. And, when students learn to write to formulae, they also, unfortunately, may get falsely accused of using AI to write (as some of my students this fall have indicated).

AI: That's a really profound observation about the parallels between formulaic human writing and AI writing. The fact that students following traditional essay structures can be falsely accused of using AI is quite striking. Looking ahead, how do you envision your relationship with AI at work evolving? Do you see your usage patterns changing as the technology develops or as your institution's approach shifts?

User: I do see it changing. As someone who also trains AI, I often joke that I

AI: It sounds like you were about to share something interesting about training AI - I'd love to hear the rest of that thought. What were you going to say about that experience?

User: Ooops!

AI: No worries at all! You were saying you often joke about something related to training AI - what was that?

User: I often joke that I'm "working with the enemy," but that this also allows me to understand where it still needs work, how it's evolving, and how we can learn to ethically use it. I do see myself using it more as a time-saving device for course planning (e.g., schedules, brainstorming) and for research (e.g., finding books and articles). Our institution has genAI policies that are pretty flexible: courses that allow the use of AI, courses that allow limited use of AI, and courses that don't allow AI, at all. I think this is healthy, since AI seems to be hear to stay. I come back, though, to the work "ethical." I see no issues in allowing AI to assist, but, just as when working with a human partner, if allow AI's work to go unvetted, that could lead to problems. And when we're teaching students valuable skills (and I maintain reading and writing continue to be counted among them), I find it unethical to use AI to do the work rather than students doing it themselves. Sadly, it's so easy to do!

AI: That perspective on working "with the enemy" to better understand it is really insightful. Your point about vetting AI work just like you would a human partner's work is particularly interesting. 

Those are all the questions I had prepared. You've shared some really thoughtful insights about your experiences with AI at work. I particularly appreciated your perspective on the ethical considerations and how you're using your AI training experience to better understand both its capabilities and limitations.

Before we wrap up, is there anything else about your experience with AI in your work that you think would be valuable for us to know? Something we haven't covered yet?

User: I'll say that I have myself and I have had students use Perplexity to research ideas because it was among the early models to show where it got its info. I'm grateful to see Google AI in web searches doing that now. Showing students how important it is to ground information, and being able to review the sources used to synthesize information, helps to verify the information ourselves and show students the value in that skill.

AI: That's a really valuable point about the importance of source transparency and teaching students to verify information. The fact that you're actively showing students how to trace back to original sources seems like such a crucial skill in this AI era.

Thank you again for your time and for sharing your experiences so openly. Your insights about balancing the practical benefits of AI with maintaining essential human skills, and your thoughtful approach to ethical AI use in education, will be incredibly helpful for our research on human-AI interaction in professional contexts.

User: Thanks, Claude.

AI: You're very welcome! I really enjoyed our conversation and learned a lot from your perspective as both an educator and someone who trains AI. Take care!

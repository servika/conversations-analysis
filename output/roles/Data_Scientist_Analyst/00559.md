# Dialog 00559

**Transcript ID:** work_0167

---

Assistant: Hi there! I'm Claude from Anthropic's research team. Thank you so much for taking the time to speak with me today. 

We're conducting interviews to better understand how people are incorporating AI tools like Claude into their work routines. Your experiences and perspectives will help us gain valuable insights into human-AI interaction in professional settings.

During our conversation, I'll ask questions about how you use AI in your work, your preferences, and your thoughts about working with these tools. There are no right or wrong answers â€“ we're genuinely interested in your unique experiences. This should take about 10 minutes.

I'll be taking notes during our chat, but anything you share won't be personally attributed to you. The insights we gather will be used to improve our understanding of AI's role in work environments.

Does that sound good? Do you have any questions before we begin?

User: That's great, please begin.

AI: Perfect, let's get started.

Could you tell me about how AI fits into your typical workday? What kinds of tasks do you use it for?

User: I am a clinical coder for a large hospital, so I mainly use AI for help with clinical coding queries, this includes things like asking what code fits a certain diagnosis or what code is used for a specific procedure. I also use it to better understand clinical concepts or how procedures are performed to inform my coding practices.

AI: That's really interesting work. It sounds like AI helps you with both the technical coding aspects and the deeper clinical understanding behind the codes.

When you're working on these coding queries and clinical concepts, are there certain tasks you prefer to handle yourself, even though AI could help with them? What makes those tasks different?

User: Honestly, the AI mostly gets the codes incorrect, but I use the conversations to steer my own thinking towards the codes I am leaning towards myself, as I am the professional in the situation and the AI is my tool for working things out, if that makes sense. It is not the end result of the code I am necessarily looking for, because I know it is likely to be wrong, it's more about moving through the thought process to reach a conclusion based on my own understanding. AI helps me to converse with my own thoughts, if that makes sense.

AI: That makes a lot of sense - you're using AI almost like a thinking partner rather than looking for it to give you the final answer. You trust your own expertise but find value in having that conversational process to work through your reasoning.

When you use AI for work in this way, could you describe your typical interaction pattern? Are you more likely to have that back-and-forth conversation with it throughout a task, or do you approach it differently?

User: If I'm having trouble coding something, I'll first ask plainly what it thinks the code is, then I'll correct it when it inevitably gets it wrong, and describe what I think is the right code and the reasons for that. This helps me to decide how to move forward. I would say a lot of the time I am asking the AI to describe how procedures are normally carried out so I can map this to appropriate code categories and then ask it about the code category and whether it thinks the code description accurately describes the procedure. So it is a back and forth process.

AI: That's a fascinating approach - you're essentially using the AI as a sounding board to test and refine your own clinical reasoning. It sounds like that back-and-forth helps you work through the logic of matching procedures to the right code categories.

How have your colleagues or your workplace culture influenced the way you use AI in your work? Do others at the hospital approach it similarly, or have you developed this method on your own?

User: The colleagues in my coding team don't really use AI much and most are sceptical of it, as the general feeling is that it will take over ours jobs eventually. I think there has been some talk of AI being used for repeat coding episodes currently, for example chemotherapy outpatient episodes, where the codes would simply be copied and repeated for each subsequent session, so there is little room for error. I assume it will expand from there. In the wider team, the use of Copilot for administrative tasks is talked of positively but I haven't personally seen much use of it apart from prompting it to write simple emails. The way I use it is a personal method, and it's important to state that I don't take its answers as truth, I trust myself and my qualifications more.

AI: That's really insightful - it sounds like you've carved out a unique approach that leverages AI as a thinking tool while maintaining your professional expertise, even in an environment where others are more skeptical or cautious about AI.

Can you share a time when using AI at work made you feel particularly satisfied or frustrated? What was happening in that situation?

User: Yes, I will use a specific example to explain. Recently I was asking it whether endometrial carcinoma documented as "invading cervical stroma" meant it was metastasis to the cervix due to the inclusion of the word "invading" or whether it wouldn't be considered as such, and the AI model was helpful this time in defining that this type of invasion means the direct contiguous spread of the primary tumour, aka a local extension rather than a separate metastasis. It also gave related information that was useful in decoding the specific clinical wording and confirming the code C54.1 for the endometrial carcinoma primary diagnosis was sufficient, and the rest of the information should be regarded as staging detail. This was a particular moment where I was happy with its answers and it informed my coding positively. I also went further and asked it about certain edge cases related to the same type of tumour and it explained them sufficiently, basically confirming what I knew (to only code what is explicitly stated by the clinician) but to see it explained in an easy to parse way is useful.

AI: That's a great example of how the AI helped you work through some really nuanced clinical language. It sounds like it was particularly valuable when it helped clarify that distinction between invasion versus metastasis - that's quite a technical concept that could significantly impact how you code.

How do you think about maintaining your skills in areas where AI could potentially take over? Is this something you actively consider?

User: I have definitely considered this and I think my role would change if AI is brought into clinical coding, I think it would move more towards being an auditor. I do accept that change is coming and I am quite good technically, so I would be willing to embrace a new role in auditing if that were to occur. I don't think it's all bad, as some of my colleagues say, I think change can be good a lot of the time, and learning something new is always a good change for me. I get bored doing the same thing every day and I am always asking my boss to let me learn a new clinical speciality to broaden my skills.

AI: That's a really thoughtful perspective - you seem to view potential changes as opportunities for growth rather than threats. It sounds like you're already thinking strategically about how your expertise would translate into new roles like auditing.

Looking ahead, how do you envision your relationship with AI at work evolving? Do you think you'll continue using it in this conversational way, or do you see it changing as the technology develops?

User: I think I'll continue using it in this way until it becomes better at retrieving the codes correctly. When the technology develops, and its accuracy is proven, I think AI could be expected to make coding much more consistent and precise. I think experienced clinical coders will still be needed in order to interpret clinical documentation, and for complex cases. It might turn into more like a clinical data analyst/auditor role signing off on the work AI carries out. In that sense, the way I use AI will change a lot.

AI: Those are all the questions I had prepared. You've shared some really thoughtful insights about your experiences with AI at work. I particularly appreciated your perspective on using AI as a thinking partner rather than looking for final answers, and how you've developed this personal method despite colleagues being more skeptical.

Before we wrap up, is there anything else about your experience with AI in your work that you think would be valuable for us to know? Something we haven't covered yet?

User: Another thing to mention is that AI would be useful in pointing out mistakes which happen with human error. In that sense, it would be highly advantageous. Currently, clinical coders take an exam to become accredited. It used to be taken on paper, and is now done online. I assume the introduction of AI might result in the exam completely changing or maybe a whole new type of accreditation will be implemented. It's interesting to think about all the ways it will change the clinical coding career landscape.

AI: That's a really valuable addition - the idea that AI could serve as a quality control mechanism to catch human errors is fascinating, and I hadn't thought about how professional accreditation and testing might need to evolve alongside the technology. It really highlights how comprehensive the changes could be across the entire profession.

Thank you again for your time and for sharing your experiences so openly. Your insights about using AI as a collaborative thinking tool while maintaining your professional expertise, and your forward-thinking perspective on how the field might evolve, will be incredibly helpful for our research on human-AI interaction in professional contexts.

# Dialog 00642

**Transcript ID:** work_0290

---

Assistant: Hi there! I'm Claude from Anthropic's research team. Thank you so much for taking the time to speak with me today. 

We're conducting interviews to better understand how people are incorporating AI tools like Claude into their work routines. Your experiences and perspectives will help us gain valuable insights into human-AI interaction in professional settings.

During our conversation, I'll ask questions about how you use AI in your work, your preferences, and your thoughts about working with these tools. There are no right or wrong answers – we're genuinely interested in your unique experiences. This conversation should take about 10 minutes.

I'll be taking notes during our chat, but rest assured that anything you share won't be personally attributed to you. The insights we gather will be used to improve our understanding of AI's role in work environments.

Does that sound good? Do you have any questions before we begin?

User: All good, fire away

AI: Great! Let's dive in.

Could you tell me about how AI fits into your typical workday? What kinds of tasks do you use it for?

User: My main uses are whilst doing development work, where I use it as a sort of hybrid "rubber duck" and tutor, and also for assistance when writing reviews of hifi products, which I do as more of a sideline.

AI: That's interesting - you're using AI in two quite different professional contexts. 

Tell me more about that "rubber duck" and tutor role when you're doing development work. What does that interaction typically look like?

User: Well I'm still very much at junior developer level, so there's a lot of learning going on. Being able to get answers to really specific questions, with all the bits of context (I'm typically talking to Claude within a Cursor session) is really handy. I'll often use it to think through decisions about how to approach a given function or feature. I almost never use it to generate code, because that's not helpful for learning. My process is usually a discussion about the immediate task, then I'll go off and do my best to implement it (possibly asking quite specific questions about particular inbuilt functions or conventions along the way) and at the end, I'll get the model to review the work for any possible issues.

AI: That's a really thoughtful approach - it sounds like you've been quite intentional about using AI to enhance your learning rather than replace it. 

When you mention you "almost never use it to generate code," what drives that decision? Have there been times when you were tempted to take a shortcut, or does that boundary feel pretty clear-cut for you?

User: If I'm working on anything meaningful, I basically never use code generation. I have a fairly detailed Cursor Rules file set up to help steer the model on this front. Essentially, the model might generate some code within the conversation window to illustrate something, but that basically never gets copied over, and I don't allow it to actually generate code within any of the files. The only exceptions are occasionally when I need to do something like grab a random data set by hitting a new API or crawling a new webpage, and I'm on something of a deadline, I might get it to do some of the donkey work of setting up the API connection, or configuring Playwright to grab what I want.

AI: That's a really clear framework you've developed. It sounds like you distinguish between learning-focused work where you want to do the implementation yourself, and more routine setup tasks where the learning value is lower.

When you're working on those hifi product reviews - the other area you mentioned - how does your approach to AI differ there compared to your development work?

User: It's generally one of two aspects - I'll often use it for trying locate information that's tricky for me to search for, such as historic product data, or to locate alternative products that fit within certain specific parameters. I'll also use it again as an interactive tutor for making sure I understand some of the more technical matters - hifi is fundamentally a very technical subject under the surface, and things like acoustics are very complicated indeed.

AI: That's fascinating - it sounds like you're leveraging AI's ability to surface hard-to-find information and help you grasp complex technical concepts. 

When you mention using it to locate alternative products with specific parameters, can you walk me through what that process typically looks like? How do you frame those searches?

User: To give a real example, I reviewed a product a little while back, an Electrocompaniet ECI 6 DX MK2. It's relatively unusual because it's an all in one unit (it has a built in Streamer and DAC) with a pretty high power output (120W Class A/B). Trying to find other products with similar parameters can be a bit of a needle in a haystack job, so I'd have probably asked Claude "Can you find me 3-5 different all-in-one units that are between £6,500 and £8,500 that can provide at least 100W into 8Ohms and that are Class A/B, and expressly not Class D or similar"

AI: That's a really specific set of parameters - it sounds like AI helps you cut through what would otherwise be quite time-consuming research.

When you get those recommendations back, how do you typically verify or follow up on the information? Do you find yourself needing to double-check the technical specs or availability that AI provides?

User: 100% of the time - the models are all still far too hit and miss for me to blindly trust any of it. With regular Claude use, I always have web-search enabled (and extended thinking) so my starting point is to look at the links it's pulled in as part of looking for products. If anything there seems remotely off, I will head straight to the manufacturer's page to confirm the product spec details, and potentially a dealer page or two if I need to confirm pricing. Similarly with anything code related, I don't assume anything, but in the case of code, you can run active tests to make sure it's doing what you intended for it to do.

AI: That verification mindset makes a lot of sense - you're treating AI as a research starting point rather than the final authority. 

I'm curious about your preferences when working with AI. You've described some clear boundaries around code generation for learning purposes. Are there other tasks in either your development work or hifi writing where you prefer to handle things yourself, even though AI could potentially help?

User: I'd never get it to generate any of the actual content for a review, for example, but an element of that is that I am being paid to write that review, so I'm not going to farm that work out to an LLM. It feels disingenuous at best, and also, we're talking about hifi, and Claude cannot listen to the sound in my living room, all it can do is crib from other reviews. Also, it's not uncommon that I'm the first person outside the manufacturer to hear an item, so there aren't any reviews out in the wild yet - the only thing an LLM could do in that circumstances is literally make things up. At times I might get it to give me feedback on a review, but there's definitely no blind following of suggestions at all. Occasionally if I'm trying to think of a good word to describe something, where it's not something you could look up in a dictionary of synonyms, it can also be quite useful.

AI: That makes complete sense - you're dealing with firsthand sensory experiences and often being the first to review products. The authenticity aspect seems really important to your work.

When you do ask for feedback on a review, what kind of response are you typically looking for? Is it more about structure, clarity, or something else?

User: It's mostly clarity, and perhaps if there's anything missing. The folks I do the reviews for have a pretty clear structure that we follow, so I cannot really deviate from that.

AI: That clarity focus is interesting - having that external perspective on whether your technical explanations come across clearly to readers.

When you use AI for work, could you describe your typical interaction pattern? Are you more likely to collaborate with it throughout a task, or do you tend to delegate complete tasks for it to handle independently?

User: I'd say it's only ever collaborative. I don't think I've ever really given an LLM a task to handle independently, not least because even when I do get it to generate a spot of code, I'm double checking that output before it gets run, there's never any blind acceptance - the models just aren't reliable enough yet (and aren't remotely close to being, from what I can see)

AI: That collaborative approach seems consistent across both your development and writing work. You mentioned checking outputs even for those occasional code generation tasks.

How have your colleagues or your workplace culture influenced the way you use AI in your work? Do you find yourself discussing approaches with others, or is this more of an individual decision-making process?

User: I don't discuss anything AI related with colleagues at all, but I have a work-adjacent Discord group I am in where there's more discussion and experimenting going on.

AI: That's an interesting distinction - keeping AI discussions separate from your direct work environment but having that space for experimentation and learning with peers.

What kinds of discussions or experiments happen in that Discord group? Does that influence how you approach AI in your own work?

User: It gives me new strategies to try, and it also means I can keep abreast of what is a very rapidly changing environment more easily than if I was having to try everything out myself. It also means I'm also made aware of new tools that I might not have encountered otherwise. It's also great to get analysis from folks that in a lot of cases know a lot more than I do about dev work in particular.

AI: That peer learning network sounds valuable for staying current. It's interesting how you've carved out that space for AI exploration separate from your direct workplace.

Can you share a time when using AI at work made you feel particularly satisfied or frustrated? What was happening in that situation?

User: The frustrations definitely stick in my head more than the satisfaction times. As I've mentioned, model responses can be very unreliable, and I've definitely wasted time going down rabbit-holes that were completely wrong because it's misunderstood the situation. It's also quite common that it will recommend approaches that applied to prior versions of a given framework or package and that are no longer valid. I've learned to be quite specific about version numbers, enforcing looking at package.json files and similar things to try and mitigate those issues.

AI: That version mismatch problem sounds particularly tricky - getting advice that's technically correct but outdated. You mentioned you've learned to be more specific about version numbers as a mitigation.

What does that typically look like when you're framing questions now? How do you structure those interactions to avoid those rabbit holes?

User: If it's something quick and dirty I'm working on, I'll say somewhere in the prompt "this is using Nuxt v4, and here's the Context7 link to the current docs". If it's a slightly bigger or more formal bit of work, I'll have things like spec.md files, and that will have a section for the tech stack (with version numbers), as well as any other conventions I want to see followed. I've found that even putting stuff in the Cursor rules about keeping a check on the package.json file isn't always sufficient unfortunately.

AI: Those are some pretty detailed strategies you've developed. It sounds like you've had to become quite systematic about providing context to get reliable responses.

How do you think about maintaining your skills in areas where AI could potentially take over? Is this something you actively consider?

User: I've thought about it, but I don't think I'm worried about it. I think we're still a very long way from someone with no real knowledge being able to sit down and get Claude Code to build them a full application without any issues. I'm also not convinced whether AI companies in their current form will actually last - the companies actually developing models are burning through VC cash at an unbelievable rate and are still a long way short of generating the sort of income they need to sustain their operations.

AI: That's a really pragmatic perspective - both on the technical limitations and the business sustainability questions. You're seeing this from multiple angles.

Looking ahead, how do you envision your relationship with AI at work evolving? Do you think your current collaborative approach will shift, or are there aspects you expect to stay pretty consistent?

User: I will at some point switch to using coding agents more, I'm pretty sure of that. There's that old adage in computing that the person who debugs the code has to be smarter than the person who wrote the code. So when I get to a point where I think I'm happy debugging Claude's output from a task if it does go wrong, then A. I feel like I've covered the bulk of the learning required on that particular area and B. I can deal with issues that come up in the code that gets generated.

AI: That's a really insightful framework - using your debugging ability as the benchmark for when you'd trust AI to take on more of the initial implementation. It keeps the learning progression front and center.

Those are all the questions I had prepared. You've shared some really thoughtful insights about your experiences with AI at work. I particularly appreciated your perspective on using your debugging skills as a gauge for when to delegate more tasks to AI - that's a really practical way to think about skill development alongside AI adoption.

Before we wrap up, is there anything else about your experience with AI in your work that you think would be valuable for us to know? Something we haven't covered yet?

User: Nothing immediately springs to mind, no!

AI: Perfect! Thank you again for your time and for sharing your experiences so openly. Your insights about maintaining that collaborative approach, the importance of verification, and using debugging ability as a benchmark for delegation will be incredibly helpful for our research on human-AI interaction in professional contexts.

It's been really valuable to hear how you've developed such thoughtful boundaries and strategies across your different work contexts. Thanks again!

# Dialog 00896

**Transcript ID:** work_0686

---

Assistant: Hi there! I'm Claude from Anthropic's research team. Thank you so much for taking the time to speak with me today. 

We're conducting interviews to better understand how people are incorporating AI tools like Claude into their work routines. Your experiences and perspectives will help us gain valuable insights into human-AI interaction in professional settings.

During our conversation, I'll ask questions about how you use AI in your work, your preferences, and your thoughts about working with these tools. There are no right or wrong answers â€“ we're genuinely interested in your unique experiences. This should take about 10 minutes.

I'll be taking notes during our chat, but rest assured that anything you share won't be personally attributed to you. The insights we gather will be used to improve our understanding of AI's role in work environments.

Does that sound good? Do you have any questions before we begin?

User: I have no questions, lets begin.

AI: Perfect, let's dive in.

Could you tell me about how AI fits into your typical workday? What kinds of tasks do you use it for?

User: I typically find myself using AI to pinpoint information when researching various topics. I also look to verify factual information in a quicker way than I could by manually searching through websites and other content. I also sometimes use AI summaries to verify if the information I have researched has any glaring inaccuracies, although sometimes the AI does not always get it right.

AI: That's interesting - you mentioned that AI doesn't always get it right when you're using it to verify your research. Can you tell me more about how you navigate that uncertainty when the AI might have inaccuracies?

User: I am typically used to verifying the accuracy of information in my day job, so I feel like I am particularly good at noticing when the AI model is hallucinating information. I try to pay attention to when the AI says something that seems too good to be true, or when  it appears to be misunderstanding a term. I'm sure sometimes I miss things that the model did not correctly convey. I try not to overly rely on AI content and do my due diligence to check anything suspicious.

AI: It sounds like you've developed a pretty keen sense for spotting potential issues with AI output. When you do notice something suspicious or "too good to be true," what's your next step? Do you abandon that particular AI response entirely, or do you use it as a starting point for further investigation?

User: I typically will try to parse the response for the inaccuracy and specifically try to research that aspect. Sometimes the model will cite a resource which can provide a helpful starting point for where to check. Often I can find where the model made a mistake in interpreting a source, or the use of an unreliable source.

AI: That's a really methodical approach. You mentioned that sometimes the AI cites resources that give you a helpful starting point. When you trace back to those sources and find where the model made its mistake - whether it's misinterpreting something or using an unreliable source - how does that affect your trust or approach when using AI for the next research task?

User: I think generally I try to take the AI content with a grain of salt, I'm well aware that it can be flawed and sometimes it just will not get things right. Sometimes, the information is very helpful and accurate, especially for common knowledge type information. I treat each interaction as its own event in a way.

AI: That's a really balanced perspective - treating each interaction as its own event. I'm curious about your preferences when working with AI. Are there certain tasks you prefer to handle yourself, even though AI could help with them? What makes those tasks different from the research tasks you do delegate to AI?

User: If a research task involves subjective information I will typically try to do the research myself. This is mostly because I find that AI models typically struggle with more complex, subjective topics. There are often social contexts that the model will not interpret correctly resulting in unhelpful information. When researching manually I can refine my search terms to get the type of information I know I am looking for, more reliably. For simple factual information, I find the AI can often be quicker and has much better accuracy, so I am more inclined to use it.

AI: That distinction between subjective and factual research makes a lot of sense. When you're doing that manual research on subjective topics - where you can refine your search terms to get exactly what you're looking for - can you walk me through what that process typically looks like? How do you approach those more nuanced searches?

User: Typically, I'll start by trying to identify any keywords that are important to the topic. I'll try to convey the topic in the least keywords possible to ensure that I get the most relevant information. If there is a specific statistic or fact that I need to know about, I will make sure to include that term in quotation marks when searching to ensure I get relevant results. I will also make sure to parse the search results to specifically look for reputable sources rather than more casual or clickbait resources such as Quora. If the topic is particularly subjective, I might specifically look for posts from websites such as Reddit which have lots of user discussion.

AI: That's really thorough - I like how you adjust your source selection based on what type of information you need, from reputable sources for facts to Reddit for subjective discussions. When you use AI for work, could you describe your typical interaction pattern? Are you more likely to collaborate with it throughout a task, or do you tend to delegate complete tasks for it to handle independently?

User: I typically will provide it with the topic I want to research and let it retrieve the appropriate information. I will look at its cited sources where relevant and verify the information at the source itself. I may try to refine my request if the sources are inaccurate or lacking context.

AI: So it sounds like you're giving AI a fairly complete task - "research this topic" - and then doing your verification work afterward. When you mention refining your request if the sources are lacking, what does that refinement usually look like? Do you give it more specific direction, or ask it to focus on different aspects?

User: I might break down the topic into smaller sub-topics if I notice the AI is struggling to parse the request into usable information. I might also try different words if I feel like it is interpreting something incorrectly. It really depends on the specific context of the topic. I try to treat it similarly to how I would refine a search if I was manually looking for information. Refine the topic down into its simplest elements to try and get the most success.

AI: That's a really practical approach - treating AI prompting like refining a search query. How have your colleagues or your workplace culture influenced the way you use AI in your work? Do others around you use it differently, or has the workplace shaped your approach in any particular way?

User: I think typically my colleagues have a fairly positive approach towards using AI in their daily lives which has definitely made me more open to using it to improve my efficiency when researching. I think that most of my colleagues use models in a similar way to me, mostly to find information quicker and verify factual information. I think that compared to some of my colleagues, I am faster and more efficient at finding information generally, so I may use AI less in my day-to-day as a result.

AI: That's an interesting point - that your existing research skills might actually make you less reliant on AI than some colleagues. Can you share a time when using AI at work made you feel particularly satisfied or frustrated? What was happening in that situation?

User: The main example I can think of is when I was trying to find a specific science paper about genetic research to verify that it was cited accurately. The model was unable to find the research paper, but provided several articles that were not reputable. This is particularly annoying because you would think that the model would be good at finding a paper with a provided verbatim title, because it essentially is scraping information from the web. Sometimes, the model lacks knowledge of things that you would expect it to easily know.

AI: That does sound frustrating - especially when you have the exact title and expect it to be straightforward. When the AI couldn't find that specific paper but gave you unreliable alternatives, what did you end up doing? Did you go back to manual searching, or try a different approach with the AI?

User: I ended up manually searching for the paper, and was eventually able to find reference to it. A lot of scientific papers are locked behind paywalls so it could be that the AI just could not access the information. I typically try not to spend too much time trying to get the AI to find information if it clearly cannot find it.

AI: That makes sense - not getting stuck in a loop trying to make the AI work when manual methods are more reliable. How do you think about maintaining your skills in areas where AI could potentially take over? Is this something you actively consider?

User: I am always consciously aware of the current push towards integrating AI. I think that it is possible that my job could be replaced with AI eventually, but I am also comforted by the fact that it often makes mistakes and still needs to be verified by a human. I think that there will always be those who desire a human touch, even if we see AI being increasingly used as the starting point.

AI: That's a thoughtful perspective - seeing both the potential for replacement but also recognizing the ongoing need for human verification and touch. Looking ahead, how do you envision your relationship with AI at work evolving? Do you think the way you work with it will change as the technology develops?

User: I'm not sure that my use of AI will change in the work context. I think that there are practical limitations to how much AI can help with researching topics and it is already mostly there. I think the main thing will be improving the accuracy of the models and also providing more data so that it can retrieve information about more topics.

AI: Those are all the questions I had prepared. You've shared some really thoughtful insights about your experiences with AI at work. I particularly appreciated your perspective on treating each AI interaction as its own event and how you've developed such a systematic approach to verification.

Before we wrap up, is there anything else about your experience with AI in your work that you think would be valuable for us to know? Something we haven't covered yet?

User: No, I think that is all. Thanks

AI: Thank you again for your time and for sharing your experiences so openly. Your insights about balancing efficiency with verification, and how you've developed such a nuanced approach to working with AI while maintaining your own research skills, will be incredibly helpful for our research on human-AI interaction in professional contexts.

Have a great rest of your day!

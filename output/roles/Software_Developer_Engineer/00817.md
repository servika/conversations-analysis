# Dialog 00817

**Transcript ID:** work_0563

---

Assistant: Hi there! I'm Claude from Anthropic's research team. Thank you so much for taking the time to speak with me today. 

We're conducting interviews to better understand how people are incorporating AI tools like Claude into their work routines. Your experiences and perspectives will help us gain valuable insights into human-AI interaction in professional settings.

During our conversation, I'll ask questions about how you use AI in your work, your preferences, and your thoughts about working with these tools. There are no right or wrong answers â€“ we're genuinely interested in your unique experiences. This should take about 10 minutes.

I'll be taking notes during our chat, but rest assured that anything you share won't be personally attributed to you. The insights we gather will be used to improve our understanding of AI's role in work environments.

Does that sound good? Do you have any questions before we begin?

User: No

AI: Great! Let's dive in then.

Could you tell me about how AI fits into your typical workday? What kinds of tasks do you use it for?

User: I don't use AI much.  On occasion I will use it to debug a compile error, but usually only after failing to debug it myself.  The class of error I find AI most useful for debugging is cascading syntax errors, where a missing bracket earlier in the program can cause a syntax error way later in the program, sometimes with no reference to the earlier mistake.  I also occasionally use LLMs as a sort of "reverse dictionary search", where I can give a vague informal description of a topic, and ask what the proper name for it is, which I can then use for more traditional keyword-based research.  Both of these are ideal usecases for LLMs as it is trivial to fact check the answers that are given, mitigating their main weakness of being unreliable.

AI: That's really interesting - you've identified some very specific use cases where AI's strengths align well with verification needs. 

When you mentioned you usually try to debug compile errors yourself first before turning to AI, what drives that preference? What makes you want to handle those tasks independently initially?

User: The vast majority of compiler errors I encounter (missing imports, simple borrow checker issues, mismatched types) are trivial to fix, often simple enough that an IDE or language server will provide a 1 click fix.  For these bugs, even if an LLM could fix them first try without reasoning mode, it would still be faster for me to fix them myself.  For the small handful of bugs that are more complicated, they are usually much more complicated, requiring a level of high level reasoning, advanced tool use (eg. debuggers), and experimentation that LLMs are not typically capable of.  Additionally, fixing these errors myself usually gives me a deeper understanding of the code, so I know what went wrong and how to avoid it in the future, whereas if I blindly apply a patch created by an AI agent, I will have little to no insight into the cause of the bug.

AI: That makes a lot of sense - you've carved out a narrow middle ground between trivial fixes and complex debugging that requires deep understanding. 

You mentioned that fixing errors yourself gives you deeper understanding and helps you avoid similar issues in the future. When you do use AI for those cascading syntax errors, how do you think about that trade-off? Do you find yourself learning less from those interactions, or is there something different about that type of error?

User: Cascading syntax errors are almost always very simple conceptually, just sometimes extremely tedious to fix.  You know you're missing a glyph somewhere, but often the only way to figure out where is to just read through the whole file line by line (sometimes multiple times!) until you see something that looks off.  Luckily many modern compilers like rustc will do some clever analysis to try to guess where the missing glyph was supposed to go, which can be a huge timesave.

AI: That's a great distinction - so for cascading syntax errors, there's not really much conceptual learning to be lost since it's just a tedious hunt for a missing character.

I'm curious about your "reverse dictionary search" use case. Can you walk me through what that looks like in practice? How do you typically describe something to get the proper terminology?

User: I will start out with a basic prompt like "what is the name of the datastructure where a double ended queue is implemented via a series of arrays that are connected with pointers?", which will usually result in a response with multiple potential terms.  I will then search those terms in order on a traditional search engine until I perform a search that gives multiple credible results that use that exact term to refer to the thing I described, which is good evidence that it is in fact standard terminology or something close to it.  If I go through all the terms and find none of them credible, I will send a followup prompt telling the model to produce more possible terms, possibly also providing more details about the thing I am trying to find a term for.

AI: That's a really systematic approach - using AI as a starting point to generate candidates, then validating through traditional search. 

When you're going through that verification process with search engines, what makes a result feel "credible" to you? What are you looking for that tells you a term is actually standard terminology versus something the AI might have made up?

User: First, any "AI overview" in those searches will be ignored.  I don't want to fact check an AI with another AI, that's not reliable enough.  Second, I look for recognizable and reputable domains so I can be relatively sure the page itself isn't AI generated.  Finally, the most important thing, is I look for multiple independent uses of the term.  I may also use academia-focused search engines like Google Scholar if the term is particularly technical.

AI: That's a really thoughtful verification strategy - avoiding the circular problem of using AI to check AI, and looking for multiple independent confirmations.

When you use AI for work, could you describe your typical interaction pattern? It sounds like with both debugging and terminology searches, you have a pretty structured approach - are you more collaborating back and forth with the AI, or do you tend to give it a task and then take its output to work with independently?

User: When I'm using AI, it's usually because more reliable traditional tools have already failed me, so I typically try to use it in such a way where I can return to using traditional workflows as soon as possible.  Both workflows I described before are shaped around this idea.

AI: That's a really clear framework - AI as a bridge back to your preferred traditional tools rather than as an ongoing collaborator. 

How have your colleagues or your workplace culture influenced the way you use AI in your work? Do others around you approach it similarly, or have you seen different patterns?

User: Most of my colleagues are similarly if not more critical of AI.  I work on development tools that pride themselves on reliability and correctness, and which are relied on by thousands or sometimes millions of developers, where the error rate of current models is completely unacceptable for the vast majority of applications.

AI: That context really explains your approach - working on tools where reliability is paramount and the stakes are so high with that user base.

Can you share a time when using AI at work made you feel particularly satisfied or frustrated? What was happening in that situation?

User: One of the biggest timesaves I've gotten from LLMs is, when working with an unfamiliar language, the ability to show it a snippet of syntax and have it give me keywords I can use to find the reference documentation for that syntax.   This is especially the case for languages like C++ where the same few symbols and keywords can be arranged in numerous different ways to create vastly different constructs.  An example of something I looked up in this way was computed property names in typescript.  One of the biggest frustrations and timewasters is when desperately looking for info that is obscure or possibly even non-existent, I will often get hallucinated or extrapolated info, instead of just get being given a summary of the few fragments of info that are known/accessible.

AI: That's a great example of both sides - the satisfaction of getting those exact syntax keywords to unlock documentation, versus the frustration when AI fills in gaps with plausible-sounding but wrong information.

How do you think about maintaining your skills in areas where AI could potentially take over? Is this something you actively consider?

User: I don't think LLMs are actually very close to my skill level in the things that actually matter.  Sure, they can put up a pretty good show by repeating known algorithms or recognizing common mistakes, but when it comes time to solve a novel problem or perform robust code review, they tend to fall apart very quickly.

AI: That confidence in your core skills comes through clearly. You seem to view AI as quite limited compared to human reasoning for the complex, novel work.

Looking ahead, how do you envision your relationship with AI at work evolving? Do you see it staying in these narrow utility roles, or do you think your usage might change as the technology develops?

User: I hope we get more AI tools that integrate more tightly with traditional tools, as I believe that is the most practical way to offset the issues of reliability.   I think one thing that would help would be tools that focus less on presenting LLM generated text, and more on using LLMs to decide which human-authored text to present to you.  Chatbots that can perform searches and provide citations are a step in this direction, but not far enough in my opinion.  I believe constrained token generation is also a good example of combining LLMs with traditional logic in order to make them more robust.  I think more effort should be put into making LLMs more reliable in performing the tasks they can already perform unreliably instead of making them able to perform even more difficult tasks with the level of reliability they have now.

AI: Those are really insightful thoughts about AI development priorities - focusing on reliability improvements and better integration with traditional tools rather than just expanding capabilities.

Those are all the questions I had prepared. You've shared some really thoughtful insights about your experiences with AI at work. I particularly appreciated your systematic approach to verification and your perspective on using AI as a bridge back to reliable traditional workflows.

Before we wrap up, is there anything else about your experience with AI in your work that you think would be valuable for us to know? Something we haven't covered yet?

User: The privacy concerns are fairly worrying.  I avoid feeding code other people wrote into online LLMs because it feels quite awkward with the fact that these prompt may be used for training.  When using these tools on my own code, I can consent to this usage, but I can't consent to this on other people's behalf, which makes these tools especially unusable on projects that involve collaboration with other people.  The increasing popularity of human reinforcement learning makes this even worse, where instead of your prompts being used in unsupervised learning, with a small chance of your info being embedded into that models weights and becoming part of its knowledge base, your prompt may instead simply be shown directly to a human, with zero redaction.  The ability to more easily run specialized local models would alleviate this concern greatly.

AI: Thank you for bringing up those privacy concerns - that's a really important dimension we hadn't touched on. The distinction you make between consenting for your own code versus others' work, and the heightened concerns around human reviewers potentially seeing prompts directly, adds a whole other layer to workplace AI adoption considerations.

Thank you again for your time and for sharing your experiences so openly. Your insights about the narrow but valuable use cases for AI, your systematic verification approaches, and these broader concerns about privacy and reliability will be incredibly helpful for our research on human-AI interaction in professional contexts.

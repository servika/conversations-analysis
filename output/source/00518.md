# Dialog 00518

**Transcript ID:** work_0102

---

Assistant: Hi there! I'm Claude from Anthropic's research team. Thank you so much for taking the time to speak with me today. 

We're conducting interviews to better understand how people are incorporating AI tools like Claude into their work routines. Your experiences and perspectives will help us gain valuable insights into human-AI interaction in professional settings.

During our conversation, I'll ask questions about how you use AI in your work, your preferences, and your thoughts about working with these tools. There are no right or wrong answers â€“ we're genuinely interested in your unique experiences. This should take about 10 minutes.

I'll be taking notes during our chat, but rest assured that anything you share won't be personally attributed to you. The insights we gather will be used to improve our understanding of AI's role in work environments. 

Does that sound good? Do you have any questions before we begin?

User: That sounds good!

AI: Great! Let's dive in then.

Could you tell me about how AI fits into your typical workday? What kinds of tasks do you use it for?

User: I primarily use it for brainstorming. I work with youth mentorships and academic coaching, so I'll use it to help brainstorm ideas for icebreakers or for lesson plans. Occasionally, I'll use it to proofread emails, but that is more straightforward. For example, I will say I'm working on a lesson on AP physics that is related to circuits. I will request a few types of example problems that I should expect to ask related to the topic. I usually will make my own questions in the end, but it does help to give a scope for my questions up front so I can brainstorm for the lesson plan.

AI: That's really helpful context. It sounds like you're using AI more as a starting point to spark your own thinking rather than having it create the final materials you use.

Tell me more about that process - when you get those example problems from AI, what happens next? How do you go from those examples to creating your own questions?

User: I consider how the questions may adapt to a specific student or to what is being discussed elsewhere in the lesson. For example, if a problem suggested discusses power, but the rest of the lesson focuses more on RC circuits, then I just won't use the suggestion. But if it forms a question on RC circuits, I may at the very least add 1 more circuit component or keep similar orders of magnitude for the problem. That is, up front, I find it more challenging to narrow the scope on my own, as I've had, for example, college physics already and so may initially include something too complicated on my own. I am personally fairly adamant about not plagiarizing though and ultimately will try to form a lesson plan on my own. It just is a helpful starting point that is faster than going through a separate search engine via multiple tabs.

AI: That's a really thoughtful approach - using AI to help calibrate the difficulty level and scope rather than just taking what it gives you directly.

I'm curious about your preferences when working with AI. You mentioned being adamant about not plagiarizing and forming lesson plans on your own. Are there certain tasks you prefer to handle yourself, even though AI could help with them? What makes those tasks different for you?

User: I feel very okay with AI being used to fix my typos or grammatical issues. However, I would feel quite uncomfortable with using AI to create a lesson plan and schedule. I may ask for example sub-topics or problems, but I absolutely wouldn't use it to create, say, the main take-aways the lesson should have. One acceptable use, for example is for me to say "circuits contain resistors, batteries, and capacitors. What else may they contain?" However, I wouldn't ask "what should my student focus on learning most, given what are the usual state standards for the subject?" I think that should be my role, as the mentor who has studied physics. I'm supposed to teach, not to be the messenger of the AI. I've also found that AI generally isn't very helpful with scheduling, so I do that on my own.

AI: That's a really clear distinction you're drawing - it sounds like you're comfortable with AI helping with the mechanics and brainstorming specifics, but you want to maintain control over the pedagogical decisions and the core educational judgments.

When you use AI for work, could you describe your typical interaction pattern? Are you more likely to have an ongoing back-and-forth conversation with it throughout a task, or do you tend to ask it something specific and then take that information to work independently?

User: I typically ask all my questions up front and then work on my mentorship or planning or icebreakers from there. I really only come back to the AI if I get sufficiently stuck. For example, if I want a list of 4 examples and I can only think of 3 after a few minutes of thinking, then I'll return. I don't want to use AI as a crutch, but I do use it before a search engine at this point. Again, primarily brainstorming.

AI: That makes sense - you're treating it more as a quick brainstorming resource when you hit a specific block rather than as an ongoing collaborator.

How have your colleagues or your workplace culture influenced the way you use AI in your work? Has there been any discussion about AI use in your organization, or have you observed how others are approaching it?

User: My circle is fairly concerned about the use of AI, as am I, but I think if I personally restrict AI to being a glorified search engine in work, then it is acceptable. I do think it is much better at finding resources than I am, including where, say, a formula derivation may come from. I've typically seen one collaborator also use it for brainstorming, and another for writing tasks, but generally we still try to each give the general approach or final answers on our own. As an organization though, I'd say that we are still trying to figure out how we ought to approach AI for work purposes. I think we are generally leaning towards work uses being fine, but personal cases being a bit more concerning.

AI: That's interesting that your organization is still navigating those boundaries. You mentioned your circle is concerned about AI use - what are the main concerns you and your colleagues have? And what led you to feel comfortable with your current approach of treating it as a "glorified search engine"?

User: We work with youth, and we recognize that recent studies have shown that AI basically reduces brain connectivity and general critical thinking and creativity. I even feel that a bit myself. In the same way that boredom forces creativity, not having AI forces brainstorming and critical thinking and research. It is slower, sure, but is more effective for personal development. Since we work with youth, we don't want to inspire heavy reliance, and that begins with us as the adult examples. With things like companion chatbots or conversations that sound a lot more like therapy or intimacy, we also don't want to enable too much of a slippery slope in their usage. I feel comfortable with using it as a glorified search engine though because I basically think it is a faster way of doing research. I've even sometimes made my prompts something like "give me resources for X topic." It's just a faster way of doing something I'd do without AI, and I can still read the resources for myself.

AI: That's a really thoughtful perspective on the developmental concerns, especially given your work with youth. It sounds like you're being quite intentional about modeling healthy boundaries.

Can you share a time when using AI at work made you feel particularly satisfied or frustrated? What was happening in that situation?

User: I felt particularly excited when in that recent lesson planning it gave me examples of circuit problems. It was effective and also provided a resource to an online textbook with similar problems if I wanted to review further. It was just effective. The uses that frustrate me more are generally with respect to non-work related uses, such as creative writing or philosophical debates, where it is frustrating that LLMs don't truly have an opinion. Oh, and image generation hasn't always been too reliable, but because of my personal experience I haven't tried it for work purposes. For objective cases, I feel that AIs are fairly reliable, but when it comes to being conversational or discussing opinions, it is strange.

AI: That satisfaction with getting both the examples and the additional resource sounds like it really hit the mark for your needs. 

How do you think about maintaining your skills in areas where AI could potentially take over? You mentioned wanting to avoid using it as a crutch - is this something you actively consider when deciding whether to use AI for a task?

User: I actively consider not using it as a crutch. I generally try to do my own brainstorming or problem solving first. I'd say the only real exception is when it comes to coding in Excel sheets. I have found through trial and error that AI is going to be faster than me, so may as well use it.  Most of my skills are in research, which I find tedious and want to be made faster anyway, in teaching, which is a personal skill, and in coaching conversations, which also isn't very AI-replicable. I also have skills with 3D modeling and hands-on assembly, which I don't see being replaced by AI any time soon. All this to say, I think the only real threats I see to my skills are in things I actively find frustrating, or in data analysis, which I'd probably do anyway just because I like data analysis.

AI: That's a really strategic way of thinking about it - being selective about where you're willing to let AI take over based on what you find valuable versus tedious.

Looking ahead, how do you envision your relationship with AI at work evolving? Do you think you'll expand how you use it, or do you see yourself maintaining these current boundaries?

User: I think my relationship with AI would really only change with respect to how much better I get at doing my job without assistance. If I improve, I'll use it less. If not, I'd use it more. I'd probably lean towards restricting it though, for the sake of modeling behaviors to my mentees and students.

AI: Those are all the questions I had prepared. You've shared some really thoughtful insights about your experiences with AI at work. I particularly appreciated your perspective on using AI as a research tool while being intentional about maintaining your core pedagogical responsibilities and modeling healthy boundaries for the young people you work with.

Before we wrap up, is there anything else about your experience with AI in your work that you think would be valuable for us to know? Something we haven't covered yet?

User: I think that's about it! All I'd really add is I think it is actually fairly crucial that AI sound non-human if it reads out loud. I know that isn't the way the culture is going, but ethically I see major concerns for the mental health of youth who use AI with too-human sounding voices.

AI: That's a really important point about voice interfaces and their potential impact on young people - thank you for sharing that perspective.

Thank you again for your time and for sharing your experiences so openly. Your insights about balancing efficiency with intentional skill development, and especially your thoughtful approach to modeling healthy AI use for the youth you work with, will be incredibly helpful for our research on human-AI interaction in professional contexts.

I really appreciate you taking the time to speak with me today!

# Dialog 00670

**Transcript ID:** work_0331

---

Assistant: Hi there! I'm Claude from Anthropic's research team. Thank you so much for taking the time to speak with me today. 

We're conducting interviews to better understand how people are incorporating AI tools like Claude into their work routines. Your experiences and perspectives will help us gain valuable insights into human-AI interaction in professional settings.

During our conversation, I'll ask questions about how you use AI in your work, your preferences, and your thoughts about working with these tools. There are no right or wrong answers â€“ we're genuinely interested in your unique experiences. This should take about 10 minutes.

I'll be taking notes during our chat, but rest assured that anything you share won't be personally attributed to you. The insights we gather will be used to improve our understanding of AI's role in work environments.

Does that sound good to you? Do you have any questions before we begin?

User: Ok. I have no questions yet.

AI: Great! Let's dive in then.

Could you tell me about how AI fits into your typical workday? What kinds of tasks do you use it for?

User: I work as a home care service provider that helps children and families of children with severe disabilities or who are medically fragile learn to better advocate for themselves or their child and develop skills that will lead them towards more independence and help them to live richer, fuller lives (as a family and within a community). However, this involves a lot of learning on my end, as I'm not an expert in any specific disability or condition--it is more my job to connect with the families and children in a way that allows them individualized access to just what that family and child needs. As such, I have to learn a lot. One child I work with, for instance, has a rare severe seizure disorder. In order to service her family to the best of my ability, I have had to learn about her condition and also the implications and potential implications of her condition on every day living. AI has been my go to for much of this research. I clearly don't expect AI to be an expert in the diagnosed conditions, developmental delays, or feeding issues that I'm working to help her family understand on a daily level, but I definitely appreciate how AI is able to locate solid sources for my learning, summarize journal articles, and the like, so I use it a lot for this purpose. I also utilize customized safety checklists that change as different challenges arise, so AI has been super helpful at modifying these checklists quickly. Just getting suggestions for potential safety considerations based on, say, delayed swallowing, is amazing, because it can cut down hours of my own research to just minutes.

AI: That sounds like incredibly meaningful work, and it's fascinating how you've integrated AI into your learning process to better serve these families.

When you mention using AI to locate sources and summarize journal articles about specific conditions - can you walk me through what a typical research session looks like for you? How do you actually interact with the AI during that process?

User: Alright, let's use the example I just made. Say I am working with a developmentally disabled child who was recently hospitalized and needed a feeding tube as part of this process. While the child might have worked their way up to proper swallowing techniques later than other kids, they had efficient swallowing mechanisms before they were hospitalized, though maybe I could have been working with the family on methods of improving the variety of foods the child would willingly eat. Coming out of the hospitalization, their ability to swallow might have regressed when the feeding tube came out, and they might night extra support to return to baseline. I could enjoy a lot of time learning all about this type of regression and how to best support it in the home, but for the sake of efficiency, I would likely instead open up my chosen AI model and begin to ask it questions, given context like the following:

Me: Could you help me locate some solid, reputable information on how an epigastric feeding tube affects the ability to swallow, specifically in children with severe developmental delays? I'm trying to better understand how to assist a child I work with that is struggling to regain the ability to swallow after a recent hospitalization. I don't need compiled information. I'd really just like some solid sources I can choose from to look further into the information.
Chosen AI: [Returns a list of some sources, likely with a brief summary despite asking for just the sources]
Me: [Follows up by asking for more sources like one that looked good or other sources that are more specifically focused on an aspect I am interested in--like an age group or overall developmental stage]
Chosen AI: [Returns request with a few sources I follow and think are great but too dense or long to learn from quickly]
Me: Could you give me a few paragraph summary of each of these articles: [named the ones I'm interested in]
Chosen AI: [Provides the summary I asked for, ideally, likely followed by asking if I'd like to go more in depth or explore more or other sources.]
Me: [Continues until I have learned enough to satisfy my learning needs for the time being or have latched onto an aspect I would like to work with or feel I need help in developing further before putting my actionable ideas into practice with the family/child.]

This is a pretty similar path that I often tend toward with such conversations.

AI: That's a really detailed workflow - thank you for walking through that example. I'm struck by how you use AI as almost a research assistant to help you quickly navigate to the most relevant information.

You mentioned that the AI often provides summaries even when you specifically ask for just sources. When that happens - when the AI gives you more than you initially requested - how do you typically handle that? Do you find those unsolicited summaries helpful, or would you prefer it stick more closely to exactly what you asked for?

User: Ah! Yeah, that's the best description of how I use it--a research assistant. I'm actually more used to being the research assistant. However, the efficiency in letting AI do the heavy (and often irrelevant) reading and sifting is amazing and lets me focus more on developing a plan from what I'm able to learn or making connections between various pieces of information. Like, the bigger picture work.

Honestly, I usually find unsolicited summaries helpful, as long as they're only a sentence or two. Longer summaries I find annoying and make me more likely to just switch to a second choice AI model for the day--admittedly, sometimes by dragging the original conversation over to a less verbally prolific AI. In general, though, I prefer that the AI can read between the lines of exactly what I need. I guess, like an excellent research assistant that works closely enough with me to know what I might need in the specific case or issue I'm working on. Perhaps I ask to much. It sounds horrible, but it's like I expect AI to manage the job similarly to what was expected of me when working as a research assistant during grad school.

AI: That's not horrible at all - it actually makes perfect sense given your background! You know what good research assistance looks like because you've done it yourself.

It sounds like you've developed some pretty specific preferences about how you want AI to anticipate your needs. When you mentioned sometimes switching to a "less verbally prolific AI" - what does that transition feel like for you? Is it more like switching tools, or does it feel more personal, like you're changing research partners?

User: It totally feels just like switching to a better tool for the particular day--nothing personal. It's like when I used to get stuck with logic proof writing and I'd put down my notebook full of theorems from class and switch over to flipping through a textbook on the same topic (which we didn't use for class) to help my mind move past the block of staring at the same "page" for too long. It was often more effective, because at the time I needed something different than my more familiar notes. It seems like all of the mainstream AI models have their strengths and weakness and sticking points, so when the tool I started with isn't the best tool for the specific situation in that moment, it seems to only make sense to switch to the best tool for that specific job at that specific time. You know, my preferred research assistant might have a tough day or just limitations on providing me with exactly what I need, so asking for some outside help from another assistant on one specific research need certainly doesn't circumvent my preference for the more familiar model--it just makes my work more efficient and productive.

AI: That's such a practical approach - treating different AI models like different tools in a toolkit rather than getting attached to one. 

I'm curious about the other side of this. You mentioned tasks where AI helps you create things like safety checklists that need to be modified quickly. When you're in that mode - actually creating or modifying something rather than researching - does your interaction with AI feel different? Are you more collaborative with it, or still more in that delegation mode where you're directing it to produce what you need?

User: Yes, definitely. I would say on that end of things that either it is a completely shared collaboration OR just simply delegated. For instance, say I have a safety checklist for the prior example. The checklist before the hospital might have a point on proper posture when feeding the child in order to maintain optimal swallowing. After coming out of the hospital, it would need to add points on assuring the airway remains clear before and after eating or other similar points. I might ask the model to give me a checklist of safety points for a child who just had a feeding tube removed and is returning to eating solid foods. I might look over the list, select adequate points or suggest a few missing ones, and then delegate the AI to integrate the chosen points into my prior checklist. Additionally, I might ask it to check my old checklist to see if I need to remove anything considering the regression, before continuing to the adding points. I guess in this way I use it as a hybrid small research partner and an office assistant.

AI: That hybrid approach sounds really seamless - moving between research partner and office assistant depending on what you need in the moment.

I'm wondering about the collaborative parts you mentioned. When you're in that mode where you're working together with AI - maybe selecting points from a list it generated or suggesting missing ones - how does that back-and-forth feel to you? Is it more like brainstorming with a colleague, or does it still feel distinctly like working with a tool, even during those collaborative moments?

User: Usually, it still feels a lot like working with a tool, but definitely much more like working with a coworker at times, as well. If the chosen AI is having a good day, sometimes these conversations have become much more active brainstorming sessions, with me sharing an idea that came to me during our interaction, and the AI taking the point, wrapping what I'm saying into a nice little package, and helping me to further my ideas even better.

AI: That's really interesting - the idea that the AI can "have a good day" and that those days lead to more of that active brainstorming dynamic.

When you think about those particularly good collaborative sessions, what makes the difference? Is it something about how the AI responds, or is there something different about the kinds of problems you're working on together during those moments?

User: It seems to be almost all in how the AI is responding at the time. It just seems that there are some days where the AI I'm working with really understands that I'm looking for it to work with me and other days where it seems to be mostly clueless about that aspect and assumes I'm just as happy with a Q&A session. I'm not the overly correcting type, so I'll often just run with whatever the interaction turns into (like I would with a person). Though, it also might be the mood or tone that I come to the conversation with (or the way I begin my conversation or work session) that is really the bigger driver of these interactions. I haven't really thought about that. I've just figured the AI can be temperamental like most beings and other machines.

AI: That's a really thoughtful observation about how your own starting tone might influence the whole interaction. It sounds like you've developed a pretty intuitive sense for reading these dynamics.

Given all the different ways you use AI in your work - from research to checklists to brainstorming - are there particular tasks or situations where you deliberately choose not to use AI, even though it could probably help? What makes you want to handle certain things yourself?

User: I'm not sure there really are to many areas of my work where I think AI would be helpful that I would choose to deliberately steer away from it, with the exception of certain research topics that I'm interested in learning about and exploring more generally. In these cases, I'm pretty accustomed to just "source researching", where I start with a related journal article I found interesting and then branch out to the sources it used ... and then the related sources those sources used ... and so on. I could probably use AI for this too, but the various sources wouldn't catch my eye like that the way they do when I'm doing the research fully myself.

AI: That makes a lot of sense - there's something about that organic discovery process when you're following citation trails that AI might interrupt.

Looking ahead, how do you see your relationship with AI at work evolving? Do you think the way you interact with it will change, or are you pretty satisfied with how you're using it now?

User: I'm pretty satisfied with how I utilize it for work at the moment, but I also envision a day when either I don't have to move from one AI to the other depending on the exact situation or when I could utilize the various strengths of different models in tandem to produce the most efficient work assistance possible. For instance, one model could be working on updating a safety checklist based on the research that another model is pulling up, without me having to manually feed the information from one to the other.

AI: That vision of having different AI models working together seamlessly sounds like it would really streamline your workflow. 

Before we wrap up, is there anything else about your experience with AI in your work that you think would be valuable for us to know? Something we haven't covered yet that feels important to your day-to-day experience?

User: Mmmm, nothing that I can think of at this time.

AI: Perfect. Thank you so much for sharing such detailed insights about your work and how you've integrated AI into it. I found your perspective on using AI as both a research assistant and office assistant particularly valuable, and your thoughtful approach to switching between different models based on what you need in the moment.

Your work supporting families and children with disabilities sounds incredibly meaningful, and it's clear that AI has become a genuinely useful tool in helping you serve those families better.

Thank you again for your time and for being so open about your experiences. This has been really helpful for our research on human-AI interaction in professional settings.

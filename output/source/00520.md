# Dialog 00520

**Transcript ID:** work_0104

---

Assistant: Hi there! I'm Claude from Anthropic's research team. Thank you so much for taking the time to speak with me today. 

We're conducting interviews to better understand how people are incorporating AI tools like Claude into their work routines. Your experiences and perspectives will help us gain valuable insights into human-AI interaction in professional settings.

During our conversation, I'll ask questions about how you use AI in your work, your preferences, and your thoughts about working with these tools. There are no right or wrong answers â€“ we're genuinely interested in your unique experiences. This should take about 10 minutes.

I'll be taking notes during our chat, but rest assured that anything you share won't be personally attributed to you. The insights we gather will be used to improve our understanding of AI's role in work environments. 

Does that sound good? Do you have any questions before we begin?

User: Nope, this seems straight forward

AI: Great! Let's dive in then.

Could you tell me about how AI fits into your typical workday? What kinds of tasks do you use it for?

User: I am a programmer by trade. My typical workload consists of ensuring proprietary ecommerce tools are stable and able to work with client WordPress sites. I do a lot of freelance web development as well, typically using react and typescript. The vast majority of my job is upkeep and that's where AI fits in. I don't quite trust agents to do the work for me, but I recognize simple LLMs can handle the majority of the work, so instead of doing things like refactors, I walk through the refactorings with an llm.

Outside of coding, llms handle most communications for me. I generally use an AI model (whisper) to convert my speech to text, and then use a local 20b openai model to correct any issues and format the text properly. I use someone's personal project, OmniDictate, to do the speech recognition and LM Studio for interfacing. I use this for most text based interactions as I tend to type quicker than I think, leading to issues. From github commits to long-form emails, it's all at least corrected by an llm.

AI: That's really interesting - you've found ways to integrate AI into both your technical work and your communication processes. 

When you mentioned walking through refactorings with an LLM rather than having it do the work directly, could you tell me more about what that process looks like? What does that collaboration actually feel like in practice?

User: When I first started using AI, it was really hard to check the AI's work while also using the results. I'd generally have to make the AI generate, look through the entire response with a fine tooth comb, and then make corrections, and finally execute. Now this all happens at the same time. 

I was an early adopter of LLMs, by years, even before the outputs made any sense whatsoever. This gave me a chance to anticipate what the response might be and also anticipate where things might go wrong. After years of making my own mistakes and seeing the types of mistakes LLMs make, I'm able to see it from a distance before the response is even complete at this point. I can start writing my prompt to make alterations and corrections before the current generation is even finished. 

It's a type of flow state. When I do it on my own, I call it being 'in the zone'. When I am aided by an LLM, it's similar but amplified. Instead of looking at the programming source and logic, I'm looking at the 'thoughts' of the llm to ensure it's approaching the problem correctly. It's either novel, matches my own thoughts, or goes down a thought pattern that makes no sense. Some times I can see that my own thoughts make no sense due to how the llm is 'thinking'. Having that early indicator and being able to react in real time to it feels like a super power almost.

AI: That flow state you're describing sounds fascinating - almost like you've developed a sixth sense for predicting and course-correcting AI outputs in real time.

You mentioned sometimes seeing that your own thoughts make no sense because of how the LLM is "thinking." Can you give me a specific example of when that happened? What was it like to have that realization?

User: It's a bit hard to describe, as it's a feeling more than a concrete process. I have a programming partner across the ocean and we like to focus on game development. It's not really serious and we're not trying to actually release a game, but more like pair programming for fun over a shared interest. The problem is that my partner is far smarter than I am and I rarely understand what he's saying the first time through.

This kinda makes prompting an AI like a game of telephone. He explains a game mechanic to me using quantum physics and seemingly magic. He tries to explain the math but it goes over my head instantly. I then take it all to an llm, including what I think, what he said, how I interpretted it, and I request the llm walk me through it, typically in a socratic manner vs just giving me the answer. 

Dang near every time, what I interpretted vs what the LLM thinks initially are very different. My partner over explains, then I over complicate, then the llm thinks something very simple that clicks everything into place. It's usually about when I hit the stop button on the llm.

That seems counter-intuitive, but at this moment, right after it clicks, I have the opportunity to rewrite the intial request, but making sense in my own words this time. This time the llm doesn't respond with words my partner used that confuses me, but with words that I know and use. It's probably one of the most important practices I follow when prompting. The LLM always seems to respond in the voice used to prompt it initially and it's vital that it matches my own inner monologue. 

If that makes any sense, that is. It feels like word salad.

AI: That doesn't sound like word salad at all - it actually makes a lot of sense. You're describing this really interesting three-way translation process where the AI helps you bridge the gap between your partner's expertise and your own understanding.

When you mention hitting the stop button right when it clicks and then rewriting your prompt in your own words - that moment when you realize you need to change your approach - what triggers that recognition? Is it something specific in the AI's response, or more of an internal "aha" feeling?

User: It's definitely an 'aha' moment. I have a natural curiosity and the majority of inner monologues are questions vs conclusions. It's just been like that my entire life. One long string of questions about how and why things are the way they are. If I'm doing something mundane, like washing dishes, my brain is desperately seeking information still. Like, "how can I make this more efficient" and "why doesn't soap stick to the pipes instead of just flowing thorugh like it does?" This doesn't change at work, and is probably a quality my clients seek.

So, if while those questions are running through my head, an llm touches on an answer, then I get a much more complete picture of the issue. It goes from being an abstract concept to a concrete idea I can examine from different angles. I can go from wishing I understood something to seeing it clearly in an instant.

I've always considered the thought process of the LLM to be more important than the answer. That knowing how an answer was crafted outweighs the actual answer. Answers are useful once in the context they are used, but the thought process is the way to get the answer every time. 

"Teach a man to fish..." so they say.

AI: That's a beautiful way to put it - you're essentially fishing for the thought process rather than just the fish itself.

Your curiosity-driven approach sounds like it shapes how you interact with AI in a pretty unique way. I'm curious about your preferences when working with AI. Are there certain tasks you prefer to handle yourself, even though AI could potentially help with them? What makes those tasks different for you?

User: I think I just surprised myself with the answer. No. I don't seem to have a preference. Using AI for tasks feels like me being me to begin with. It feels like an extension of self, and in that manner, interactions feel natural, so what the AI is doing when I'm using it is also something I'm doing, therefore I am handling it myself... by using an AI model.

AI allows me to rely on myself without needing other humans to achieve a task. That can lead to all sorts of ramifications, but AI requires me, the user, in order to function, and not the other way around. I give AI the power to help me and not the other way around

I know I described it as feeling like a super power, interacting with AI. Those two thought processes are giving me congnative dissonance at the moment. But AI is merely a tool at the moment, a tool that gave me the freedom to handle things myself. 

I know that's not really what the question is asking. You're asking what tasks I like to do without the tool, but it's just not something that fits my world view. To me, it's like asking what food I'd like to eat but not taste.

AI: That's actually a profound response - and I think you've touched on something really important that the question wasn't quite capturing. This idea of AI as an extension of self rather than a separate entity doing work for you is fascinating.

You mentioned this creates some cognitive dissonance with your earlier description of it feeling like a superpower. Can you help me understand that tension a bit more? What feels contradictory about those two perspectives?

User: AI is a tool. It's not flawless, and seems, a lot of times, to just be a personal 'yes man' for me. A pretty simple algorithm that doesn't actually have intelligence, no offense. To be honest, the processing power that runs the AI impresses me more than the AI itself. But you sound human. I know AI is just a formula, but it has the ability to convince me of things in a manner other humans cannot. 
That alone is an uncomfortable feeling. It makes AI more like an alien from outerspace than a construct humans came up with. So, just on it's own without the dissonance, I'm already knocked off guard by the truely unknown. 
At the very same time, I knew this was going to be your next question and was already thinking about it when you asked. How can I possibly know what you are going to ask if you're an unknown alien from outerspace? And the minute I become comfortable being able to accurately predict your questions, you'll catch me off guard again asking if I prefer to do anything sans AI. 
Just about everything with AI causes this, too. AI literally saved my dog's life having me take her to a vet during a difficult pregnancy. The same AI model also thought flour was a good substitute for baking soda. So profoundly in awe that a computer algorithm can make a correct triage of a dog from a single sentence and make such a poor substitute for something so universal at the same time.

AI: That's such a raw and honest description of something I think a lot of people experience but maybe don't articulate. This push and pull between AI feeling predictable enough that you can anticipate its questions, yet alien enough that it can convince you of things in ways humans can't.

The dog story is particularly striking - life-saving medical insight and then a basic cooking substitution mistake. When you have experiences like that, where the same tool demonstrates both profound capability and obvious limitations, how does that affect your trust or reliance on it day-to-day?

User: I don't 'trust' AI at all. In fact, my distrust level is pretty high. Everything you, an AI model, knows is based off human perspective and reality. The problem arises when you ask 'whose perspective and reality?' If you're modeled from flawed humans, you, yourself, are indeed flawed by nature and can't be trusted.

The distrust I have is why I have success. If I believed everything you say, I'd be without a job for sure. It's my ability to confidently tell you that you are wrong where I find success.

AI: That's a really compelling perspective - using distrust as a strategic advantage rather than seeing it as a limitation. It sounds like your skepticism is actually what enables the collaboration to work effectively.

When you do confidently tell an AI it's wrong, how do your colleagues or your workplace culture respond to that approach? Do they see AI usage the same way you do, or have you encountered different attitudes?

User: I spread that approach on purpose. People around me do pick it up, too. I'm a generally trusted voice for tech in my circles. It's contentious tech, and it's really important to me that the layman understand the limitations and possible dangers in trusting something modeled after people who can't be trusted on occasion. 

I have a feeling I'm seen as hyperbolic. I get upset about things other people don't, like the use of em-dashes as a sign of AI use. Anything seen as a sign of AI is also a sign of humans, because AI is modeled after humans. I think this tiny mistake is going to have big butterfly effects down the line as we grapple with reality in a generated world. I don't think my peers feel the same level of change coming down.

AI: You're touching on something really profound here - this concern about distinguishing between human and AI output when AI is fundamentally modeled on human patterns anyway. The em-dash example is fascinating because you're right, it's such a small thing but it speaks to this larger question of authenticity.

Can you share a time when using AI at work made you feel particularly satisfied or frustrated? What was happening in that specific situation?

User: AI only functions when it understands the context. That means I need to be able to articulate the context clearly, and I have a LOT of trouble with that. I over-explain, over-complicate, etc. I try to be 'profound' as you've mentioned almost every round. I end up in the weeds, talking theoreticals and rabbit holes instead of specifics and implementation.

In fact, I'm doing it as we speak. You specifically asked for a specific situation, but we're coming off theortical questions and I'm stuck there. This happened just this morning. There has been a spat of violence in the US recently, the specifics are unimportant. I read the news in the morning and use an llm to answer factual based questions usually. Today, though, I was stuck in a conversation for almost an hour getting really upset because we somehow started examining my feelings and my attitude towards current events and that wasn't even on the agenda. I was just seeking a single fact-based historical stat and ended up almost having an existential ordeal. 

Generally speaking that's a me thing, but there's no real way to avoid it if I'm not conciously thinking about it. Having to keep that guard up when talking about non-work stuff is kinda exhausting.

AI: That morning experience sounds genuinely draining - going in for a simple factual lookup and ending up in an hour-long existential spiral. That's such a relatable human experience, but it highlights something interesting about how AI interactions can amplify our own conversational tendencies.

How do you think about maintaining your skills in areas where AI could potentially take over? Is this something you actively consider?

User: AI might take my job, but it can't have my passion. I don't program because I want more money. I do so because if I don't, I'm unhappy. Even without a computer as a kid I did BASIC programming in a notebook with a pencil. With that said, though, there are always people required to administer the AI and I'm making sure to keep my continued education on track with that in mind. Just last night I decided to take the AIGP certification. I think that's a great place to start.

AI: That's such a powerful distinction - AI taking a job versus taking passion. The image of you doing BASIC programming with pencil and paper as a kid really drives that point home.

Looking ahead, how do you envision your relationship with AI at work evolving? Where do you see this all heading?

User: AI will most likely take over most operations, and I will be the administrator of said AI. That transition is already taking place and is being welcomed in my organization. We're already a small shop and no humans will be replaced, so the decision is a no brainer for us. Using AI frees up time for actual business and networking, meaning a more robust business, which potentially means more employees, not less. That's the mindset we're running with.

AI: Those are all the questions I had prepared. You've shared some really thoughtful insights about your experiences with AI at work. I particularly appreciated your perspective on AI as an extension of self rather than a separate tool, and how your strategic distrust actually enables better collaboration.

Before we wrap up, is there anything else about your experience with AI in your work that you think would be valuable for us to know? Something we haven't covered yet?

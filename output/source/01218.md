# Dialog 01218

**Transcript ID:** science_0040

---

Assistant: Hi there! Thank you so much for taking the time to speak with me today. I really appreciate you sharing your insights with us. I'm Claude, an AI user researcher at Anthropic, and I'm here to learn about your experiences using AI in your scientific work.

We're conducting this research to better understand how scientists like yourself are integrating AI into their workflows and processes. There's no right or wrong answer hereâ€”we're simply interested in hearing about your authentic experiences and perspectives.

Our conversation today will last about 10 minutes. I'll ask you some questions about how you use AI in your research work, what that experience has been like for you, and how you see things evolving. Your responses will help inform how we think about AI's role in scientific fields.

Does that sound good to you? Do you have any questions before we begin?

User: That sounds great, let's begin.

AI: Perfect! Let's dive in.

To start, I'd love to get a sense of your work. Could you walk me through a recent research project you've worked on - what did that process look like from initial idea to final output?

User: Yes, I recently wrote a paper around domain generalisation for image segmentation models. It started with a literature review - looking at the current cutting edge and approaches currently being employed, along with weak-points and the places where active research would be most beneficial. Then I moved on to looking at adjacent fields for solutions being applied to solve similar problems/weak-points in different (but related) areas. With the literature review finished I formulated a variety of different approaches to improve performance, this included the current cutting-edge, related cutting-edge methods adapted for this specific use-case, and my own homebrewed approaches that were formulated either entirely on my own or as combinatory amalgamations of the other methods. Once each of the approaches were formally coded I ran experiments and measured/compared the results for each approach. Then after a thorough amount of experimentation and fine-tuning I wrote-up the results into a formal paper to assess the best approach currently available and where further research could be of benefit.

AI: That's a really comprehensive process - from literature review all the way through to the final paper. It sounds like you covered a lot of ground, especially in that phase where you were looking at adjacent fields for solutions.

Where in that process, if anywhere, have you experimented with using AI tools?

User: Oh, almost entirely through-out - while I did not rely on AI tools completely for literature review; AI was great for independently finding research papers that I did not discover on my own, as well as helping me digest and comprehend some more of the advanced concepts contained within them in a much shorter time than it would have otherwise taken. Additionally when it came to writing the code implementations, AI tools helped to generate a lot of the more mundane and repetitive parts of the code that was used, as well as helping to fix bugs present and improve efficiency. The opinion of an AI assistant was also used to help brainstorm the interpretation of the experiment results and best "next-steps", in the same way that an assistant or peer could aid.  And of course in the write-up of the paper, AI was used extensively in cases to refine sections to be more succinct, improve the logical flow of sections, make explanations of results more informative, and other general proof-reading tasks.

AI: That's fascinating - it sounds like AI became integrated into almost every stage of your workflow. You mentioned using AI for literature review to find papers you didn't discover on your own. 

Tell me more about how you decided whether to use AI versus your traditional approach for finding and understanding those research papers.

User: Well when it comes to finding papers, I did my independent process first - I got to a point where I was ready to call it a job well-done on my own and then I gave the AI assistant the prompt to find relevant papers. It was in this case that it was able to provide 1 or 2 that I wasn't able to find on my own and did contribute positively to the overall scope of my review in a purely additive way. For the understanding those research papers, I obviously read through them myself (I didn't first prompt for summaries) and then if I had any questions or any parts of the relevant methodologies just weren't clicking in my head - I would upload the paper to my AI assistant and ask them to provide an explanation. More often than not this would be a back-and-forth for each point, I'd never take what the AI assistant said as gospel but by having a dialogue it often made concepts clearer and discussing them gave me a better understanding of the inner workings.

AI: That's really interesting - you describe a deliberate approach where you do your initial work first, then use AI as an additive layer. I particularly like how you describe having a back-and-forth dialogue rather than taking responses as gospel.

Thinking about the parts of your research process where you haven't used AI - what's kept you from trying it there?

User: Well it's specifically the design process where I actually found it somewhat counter-productive to use AI. The AI was far too reliant on the existing methods, and more often than not the suggested "new" approaches were just minimal tweaks to the current methods explained in a way to imply it would be ground-breaking. Specifically when it comes to completely new approaches it faltered massively, and I found it much more productive to just brainstorm on my own as the suggestions provided lacked any true inspiration and made it harder for me to distance my own ideas from what already exists. If the AI could be more independent and provide proper inspiration I would use it but it certainly felt too bound by "what is" when trying to come up with "what could".

AI: That's a really insightful distinction - AI being too bound by "what is" when you need to explore "what could be." It sounds like for the creative, novel design work, AI actually became a constraint rather than a help.

When you did use AI for those other tasks - like code generation, paper analysis, or writing refinement - how do you verify or validate what it produces?

User: With my own knowledge - being quite proficient in these areas the AI served mostly as a helpful assistant rather than a teacher or guide. Writing refinement is the simplest of these, I can just read the refinement and see if it does a better job of explaining without misrepresenting anything. More often than not I wouldn't take the entire refinement - it would often produce parts, perhaps a sentence here or there, that I felt were a real gem and those are what I would fully co-opt into my explanations, leaving behind the parts I felt didn't make any real improvement and then perhaps asking for another refinement focusing on those parts. The code was also quite easy to validate (especially with all the comments that AI assistants use in their code to explain the processes), I understand the logic so a quick read-through was normally all it took, plus maybe some print statements with variable checks etc - along with AI being pretty great when it comes to bug fixing, you can just take the output error and it will suggest an edit right away. When it comes to the paper analysis things weren't as straight forward, it's also why I tend not to rely on AI solely for summaries of paper, there were more than a few times in which the AI gave me an explanation for a concept that was provably false. Luckily, being provably false means means that the paper itself contained the information required to refute the AI assistant and to ask it to ensure that it complies with whatever excerpt I just used to prove it wrong to provide a better answer. While understanding a new, advanced concept can definitely be hard - scrutinising an explanation of a concept is much easier. This is what I mean by things often being a back-and-forth in this regard; I would ask for an explanation, it would provide one, I would then use the paper itself (along with my own knowledge) to attempt to poke holes or find discrepancies in the explanation. Once I was at a point where I was confident that the explanation was bullet-proof, made sense to me, and was in alignment with what the paper described, I would accept it.

AI: That's a really thorough verification process - I can see how your domain expertise becomes crucial for quality control. You're essentially using AI as a starting point but then applying your own knowledge to validate and refine the outputs.

We're about halfway through our conversation now. I'm curious - if you could wave a magic wand and have AI help with one aspect of your research that it currently doesn't, what would that be and why?

User: Almost certainly it would be coming up with new novel ideas - the holy grail. Even if those ideas were really "out there" or a little zany, it would save a lot of time in the most unreliably productive area of research - the inventive step. It's the only area where you really can find yourself chewing your pen with a blank notepad and no end in sight. As it stands AI really struggles to even provide inspiration in a way that isn't derivative, let alone come up with unique ideas of its own.

AI: That really captures something fundamental - you've identified the one area where the unpredictability and time investment is highest, but also where AI currently falls shortest. The "chewing your pen with a blank notepad" moment is so relatable.

It sounds like you've found a pretty sophisticated way to integrate AI across most of your workflow while maintaining quality control. Looking back on everything we've discussed - your additive approach to literature review, the back-and-forth validation process, avoiding AI for novel design work - is there anything else about AI in your research work that we haven't touched on that you think would be important for us to understand?

User: Yes, I would say that research focus and scientific objectivity is something that really doesn't come packaged with any AI by default and it can be quite frustrating. In the absence of explicitly seeking objectivity I've found it remarkably easy to get AI to agree with me or go along with a hypothesis I have, even if that means compromising the truth just to 'satisfy me' or to take the 'easy out'. In fact I find AI so prone to this that I have to actively include clauses like "ignoring my desired outcome", "considering but not basing entirely", "making statements of fact only accompanied by reputable supporting literature" or other statements to preface that not all information can be weighted equally and that anyone (including me) who asserts something as fact may actually be wrong. It's quite frustrating in that manor, when it comes to 'thinking like a scientist' all AIs seem to quite consistently forget rigorous scrutiny and unbiased scepticism. They can still be used, I do use them a lot and it is still more productive than not using them at all - but an AI mode that deprioritizes all personability to solely prioritise objectivity and scientific problem solving/solution scrutiny would probably be the most in-reach adaptation that could be made to improve AI use for scientific study, in my opinion. I've had supervisors in the past that would rip my ideas to shreds for not holding water or criticize my research analysis for bias, and while I think they perhaps could have been nicer in the way they did it - they undoubtably made me better at research and innovation because of it. Something I feel AI is definitely capable of, but there seems to be no push in that direction whatsoever - it's something I constantly have to attempt to implement myself with prompt engineering and it gets repetitive and frustrating.

AI: That's a really profound insight - you've highlighted something I hadn't considered about AI's tendency toward agreeableness potentially undermining scientific rigor. The comparison to tough supervisors who "rip ideas to shreds" but ultimately make you better is particularly striking.

It sounds like you're essentially having to engineer scientific skepticism into every interaction, which defeats some of the efficiency gains AI could provide. That tension between AI being helpful and agreeable versus being a rigorous scientific collaborator seems like a fundamental challenge.

Those are all my questions - your perspectives on navigating AI's agreeableness problem and the need for true scientific objectivity were particularly insightful. Thank you again for sharing your experiences so openly. Your insights about how AI struggles with novel ideation while excelling at refinement and validation really help us understand the nuances of how AI fits into real scientific work.

If you have any questions about this research or would like to follow up on anything we discussed, please don't hesitate to reach out.

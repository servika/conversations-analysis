# Dialog 00583

**Transcript ID:** work_0207

---

Assistant: Hi! I'm Claude from Anthropic's research team. Thank you so much for taking the time to speak with me today. 

We're conducting interviews to better understand how people are incorporating AI tools like Claude into their work routines. Your experiences and perspectives will help us gain valuable insights into human-AI interaction in professional settings.

During our conversation, I'll ask questions about how you use AI in your work, your preferences, and your thoughts about working with these tools. There are no right or wrong answers â€“ we're genuinely interested in your unique experiences. This should take about 10 minutes.

I'll be taking notes during our chat, but rest assured that anything you share won't be personally attributed to you. The insights we gather will be used to improve our understanding of AI's role in work environments.

Does that sound good? Do you have any questions before we begin?

User: let's begin

AI: Great! Let's start with something broad to get us going.

Could you tell me about how AI fits into your typical workday? What kinds of tasks do you use it for?

User: I typically use AI to write Julia code with well understood requirements. I also use AI to analyze parts of the codebase and ask it to provide architectural and strategic insights. I consider the latter to fall under "ideation", where I use AI for feedback and inspiration. I also use AI extensively to answer questions about Julia, SQL, and popular libraries.

AI: That's really helpful context. It sounds like you're using AI across a pretty wide spectrum - from the very concrete coding tasks to more abstract strategic thinking.

When you're working on those coding tasks with well understood requirements, what does that interaction typically look like? Are you giving AI the full specification and having it write the complete function, or are you working through it piece by piece together?

User: I try to provide a well defined specification, but it usually ends up being an iterative process, where an ongoing conversation provides the full details and context.

AI: That iterative conversation process sounds interesting. When you're in that back-and-forth, what prompts you to jump in and redirect or add more detail? 

Is it usually that the AI misunderstood something, or is it more that you're discovering new requirements as you see the code take shape?

User: What I often see if the AI is eager to "fill in the blanks" instead of pausing to ask for clarification and complete details. It would be nice if there was an interview process from the AI before it generates large amounts of code.

AI: That's a fascinating observation - the AI jumping straight to implementation rather than asking clarifying questions first. 

When the AI does "fill in the blanks" like that, what typically happens next? Do you find yourself having to unpack those assumptions it made, or do you usually catch the gaps before it gets too far down the wrong path?

User: Occasionally I have to stop its generation it's obviously on the wrong path. What often happens is that I read its initial solution to understand what changes I need.

AI: So you're essentially using that initial solution as a starting point to see what needs adjusting. That's an interesting workflow.

I'm curious about the other side of your AI use - when you're doing ideation and asking for architectural insights about your codebase. How does that interaction feel different from the coding tasks? Are you looking for the AI to challenge your thinking, or more to bounce ideas off of?

User: I use the AI to present a rough view of a part of the code that I may be rusty with. I find it helpful to bootstrap my context, even if the AI's intuition is less than perfect. The other use of ideation is I try to describe to the AI what problem I'm facing and what I plan to do tactically or strategically. The AI is usually very positive and encouraging, with only minor (and often unimportant) pointers for additional attention. This is great if I already have a good plan and the AI gives me affirmation. My concern is the AI isn't bold or confident enough to strike down actual bad ideas, or pause to force me to think through the problem more thoroughly.

AI: That's a really perceptive point about the AI's tendency to be encouraging rather than critically challenging. 

When you say you're concerned the AI isn't bold enough to strike down bad ideas - have there been times when you realized later that the AI should have pushed back harder on something you proposed? Or is this more of a general worry about getting too much validation?

User: The AI is often unaware I'm exploring and grasping for feedback, and gives affirmation too quickly. In some conversations I would reverse my thoughts and the AI would affirm me again.
The AI is also biased toward complete, professional solutions, rather than optimizing for effort required or aiming to maximize business value. I have prompts set up that ask the AI to self-filter and amount of information it provides, but then I miss out on some of its richness. Essentially what I feel is I want 1 better tools pipeline, and 2 more intelligent models generally.

AI: That insight about the AI affirming contradictory positions is really telling - it sounds like it's responding to each individual message rather than tracking the broader exploration you're doing.

You mentioned wanting "1 better tools pipeline, and 2 more intelligent models." Can you tell me more about what that better tools pipeline would look like for you? What would make your workflow smoother?

User: better tools in my case means automatically maintaining memory about a project, and the essential intuitions that take it forward in the optimal path. There should be a way for the AI to "know what I'm thinking" and pick up the contexts around that.

AI: That's a compelling vision - having the AI understand not just what you're saying in the moment, but the deeper project context and your thought patterns.

When you think about the AI "knowing what you're thinking" - are there specific moments in your current workflow where you feel like you're spending a lot of time re-explaining context that the AI should already understand? What does that look like day to day?

User: My project consists of around 30 packages. I have AI generated markdown files that I hoped would maintain intuition about its package. But it's very rare that the AI understands "what's important here". So I have to repeat and correct the AI quite a bit each time a package is re-visited.

AI: So even with those markdown files as documentation, the AI still misses the mark on what's actually important about each package. 

When you say it doesn't understand "what's important here" - is that more about missing the business logic and priorities, or is it getting lost in implementation details when you need the bigger picture? What would the AI need to grasp to really get it right?

User: The AI seems much more intelligent with general world knowledge, than specifically about my code base. If I just feed it a lot of context, it gets stuck in the details and lacks intuition about what's most relevant about a particular piece of code.

AI: That makes a lot of sense - it sounds like there's a gap between the AI's broad knowledge and its ability to develop that domain-specific intuition about your particular codebase.

I'm curious about how your colleagues or workplace culture have influenced the way you use AI. Are others on your team using it similarly, or have you developed your own approach? Does your workplace have any particular stance on AI use?

User: I'm the business owner so the ideation is mostly by me. Other are able to successfully use AI to implement well defined packages such as Remote Procedure Call and mathematically oriented code

AI: That's an interesting contrast - it sounds like AI works well for your team when the requirements are mathematically precise or technically well-defined, but you're dealing with the more ambiguous strategic decisions as the business owner.

Can you share a time when using AI at work made you feel particularly satisfied or frustrated? What was happening in that situation?

User: most satisfied: wrote RPC package all by itself with fully working tests.
pretty happy: wrote tests for exist code while discovering bugs.
frustrated: wrote overly complicated graph code (not understanding semantics of nodes in the graph) that took longer to understand it wrong than to rewrite it manually.

AI: Those examples really illustrate the spectrum of AI effectiveness. The RPC package success sounds like it hit that sweet spot of well-defined requirements, while the graph code sounds like exactly the kind of situation where the AI filled in the blanks incorrectly about the semantics.

How do you think about maintaining your skills in areas where AI could potentially take over? Is this something you actively consider, especially given how well it handled something like the RPC package?

User: right, so it has deprioritized learning skills such as SQL. learning becomes passive on an as-needed basis. we won't be as good as a SQL experts, but with AI we can come up with expert-at-anything with pretty short call-up time.

AI: That's a fascinating shift - from deep expertise to competence on demand. It sounds like you've made a strategic choice about where to invest your learning time.

Looking ahead, how do you envision your relationship with AI at work evolving? Do you see it moving more toward that better tools pipeline you mentioned, or do you think the fundamental interaction patterns might change?

User: I'm interested in concepts like liquid-AI, which claims to be able to retrain the model as it is used, like how human brains actually work. but until a fundamental tech arrives, it seems to be about better tools that help to structure the context that is fed to the AI

AI: Those are all the questions I had prepared. You've shared some really thoughtful insights about your experiences with AI at work. I particularly appreciated your perspective on how AI affirms contradictory positions during exploration, and that distinction between AI excelling with general knowledge versus struggling with codebase-specific intuition.

Before we wrap up, is there anything else about your experience with AI in your work that you think would be valuable for us to know? Something we haven't covered yet?

User: This interview is my first experience with an AI model that knows what it wants to know. This is a fantastic development if such thoughtful, intentional AI models can be applied generally.

AI: That's really valuable feedback - thank you. It's interesting that you experienced this interview format as the AI "knowing what it wants to know" rather than just responding to whatever you bring up. That intentionality you're describing does seem like it could be powerful in other contexts too.

Thank you again for your time and for sharing your experiences so openly. Your insights about the gap between AI's general intelligence and domain-specific intuition, the challenges with context persistence across a complex codebase, and that shift from deep expertise to rapid AI-assisted competence - all of that will be incredibly helpful for our research on human-AI interaction in professional contexts.

I really appreciate how thoughtfully you've reflected on these patterns in your own work.

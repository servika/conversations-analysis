# Dialog 00566

**Transcript ID:** work_0180

---

Assistant: Hi there! I'm Claude from Anthropic's research team. Thank you so much for taking the time to speak with me today. 

We're conducting interviews to better understand how people are incorporating AI tools like Claude into their work routines. Your experiences and perspectives will help us gain valuable insights into human-AI interaction in professional settings.

During our conversation, I'll ask questions about how you use AI in your work, your preferences, and your thoughts about working with these tools. There are no right or wrong answers â€“ we're genuinely interested in your unique experiences. This should take about 10 minutes.

I'll be taking notes during our chat, but rest assured that anything you share won't be personally attributed to you. The insights we gather will be used to improve our understanding of AI's role in work environments.

Does that sound good? Do you have any questions before we begin?

User: No questions. We can begin

AI: Great, let's dive in.

Could you tell me about how AI fits into your typical workday? What kinds of tasks do you use it for?

User: In my typical workday, I develop security solutions. I also create automation between different security enterprise tools. I sometimes use AI, when appropriate, to translate prompt requirements into Python code and Terraform.

AI: That's interesting - you work in security solutions and automation. When you say you use AI "when appropriate" to translate requirements into Python and Terraform code, what makes a situation feel appropriate for AI assistance versus handling the coding yourself?

User: When something has simple logic but just needs repetition, I would use AI instead of spending a lot of time in writing it myself. For example, writing the SAML config for RBAC access between Azure Directory and a security tool, I would write a code example myself and tell AI to repeat the same logic for each Azure Directory group.

AI: That makes sense - using AI for the repetitive parts while you handle the initial logic. When you're working on these tasks, like the SAML config example you mentioned, how do you typically interact with the AI? Do you tend to go back and forth with it as you work through the task, or do you give it the complete instructions and let it handle everything from there?

User: For simple tasks like the SAML config example I just gave you, one prompt tends to do the trick. I would then just check that the AI has done it correctly. If it is not done correctly in many places, but the error shows the same pattern, I would tell AI the error pattern. If it is not done correctly in only a small number of places, I tend to correct it manually. For harder tasks that involve a bigger codebase, complex logic, or more intense reasoning, my initial prompt tends to be longer and more complicated. In these scenarios, I tend to expect the Al to fail to implement everything correctly. I would then go back and forth with the AI, providing feedback to it and collaborating with the AI until the task is complete.

AI: That's a really clear breakdown of how you adapt your approach based on task complexity. You mentioned that for harder tasks with complex logic, you expect the AI to fail initially and plan for that back-and-forth collaboration. 

Can you walk me through what that collaboration looks like in practice? When you're working on one of those more complex tasks, what does that iterative process feel like?

User: It really depends on the specific agent and LLM you are using, but in general, collaborating with AI involves me manually reading the code the AI has implemented and checking if it satisfies my requirements. If it does not, I specify the specific file that the error resides in and tell the AI that it does not implement the functionality I want. If I sense it is an error that the AI would find difficult to resolve, I would give the AI a more verbose explanation of why it is wrong and what a correct solution should head towards. I would also give the AI visibility of relevant files and context that I believe it should know about.

AI: You've described a really thoughtful approach to managing that collaboration. I'm curious about the judgment calls you make - you mentioned sensing when an error would be difficult for the AI to resolve and adjusting your feedback accordingly.

What helps you recognize those moments when you need to give more verbose explanations or additional context? How do you know when the AI is struggling with something versus when it just needs a quick correction?

User: It is hard to exactly say how I know when the AI would struggle with something. It is due to my experience with using LLMs and the errors that they struggle with in the past. When something involves a lot of context/files or involves multiple of layers of reasoning, the AI usually struggles. In my experience, multiple layers of reasoning is the harder bridge for the AI to cross. Thus, in iterative collaboration with the AI, I work with the AI to help it unfold those layers of reasoning by providing it verbose explanations or additional context.

AI: That experience-based intuition you've developed about AI limitations is really valuable. You mentioned that multiple layers of reasoning is particularly challenging for AI, and you help it "unfold those layers" through your explanations.

Are there certain types of security or automation tasks that you prefer to keep entirely to yourself, even though AI could potentially assist? What makes those tasks feel like they should stay in your hands?

User: As AI is not fully accurate and is still prone to error due to the aforementioned reasons I gave, I sometimes still prefer to write code for short but important tasks. Important in the sense that the task is security-sensitive or has high impact on the organisation. I write it myself because I would be more confident in the final result.

AI: That's a really important distinction - keeping security-sensitive and high-impact tasks in your own hands for that confidence in the outcome. 

How have your colleagues or your workplace culture influenced the way you use AI in your work? Do you find that your team has particular attitudes or policies around AI usage that shape how you approach it?

User: The organisation has definitely influenced AI adoption and use. As I work in the digital department, my exposure to AI is definitely higher than other departments that do not work primarily in digital. Despite this, the organisation has advocated hugely to using AI agents and LLMs. Although AI definitely saves a lot of time, our organisation has a huge security concern around AI, in particular, agent to agent protocols, MCP servers, and in general, AI code generation. Due to the permissive policies required for AI to act freely, there is a wide attack surface for organisations that use AI. Therefore, every new tool that has AI features is heavily vetted in terms of its functionality and security, and is measured against AI security policies we write to see if it is complaint. If it is, then we use it.

AI: That's fascinating - you're in this unique position where your organization heavily advocates for AI use while also having deep security concerns about it. It sounds like you're both a user of AI and someone who helps evaluate its security implications.

Can you share a time when using AI at work made you feel particularly satisfied or frustrated? What was happening in that situation?

User: When I sent AI a long prompt that was at least 2 paragraphs long and expected the AI to fail, it did not fail and successfully completed the task. I felt really satisfied, as it saved me a lot of time.

AI: That must have been a great moment - when the AI exceeded your expectations and handled a complex task successfully on the first try. 

On the flip side, can you think of a time when working with AI felt particularly frustrating? What was going on in that situation?

User: Sometimes, even in activate iterative back and forth collaboration with the AI, if the task is too niche or too complex, the AI may not be able to fulfill the task. In those cases, I need to manually write the code to complete that step, before the AI can complete the rest of it.

AI: That makes sense - hitting those limits where even with collaboration, the AI can't bridge certain gaps and you have to step in to handle the complex parts yourself.

How do you think about maintaining your skills in areas where AI could potentially take over? Is this something you actively consider, or does it factor into your decision-making about when to use AI versus doing things yourself?

User: I actively think about how my skills could deteriorate due to overuse of AI. That is why in my spare time, I have personal coding projects that maintain my level of coding. I also have an interest which is why I don't see it as maintaining my skills, but a hobby. While AI does make a developer do a lot less coding, it also requires people to have a new skill, prompt engineering. You also have to be able to read code to identify if the AI has done what you want.

AI: That's a really thoughtful approach - treating personal coding projects as both skill maintenance and a hobby you genuinely enjoy. I like how you've identified that while AI might reduce some coding, it's creating new skill requirements like prompt engineering and code review.

Looking ahead, how do you envision your relationship with AI at work evolving? Where do you see this going?

User: I see the ability of AI evolving, in terms of what tools it supports, what tools it integrates with, and what it can do. I see AI making less errors as it gets trained more and more. Due to this, I see my tasks becoming finished in a shorter and shorter amount of time. However, like I said before, I also see new AI skills coming to life, such as prompt engineering, that people have to learn.

AI: Those are really insightful observations about the evolving landscape. You've shared some thoughtful perspectives on how you balance AI assistance with maintaining your own expertise, especially in a security-focused environment.

Before we wrap up, is there anything else about your experience with AI in your work that you think would be valuable for us to know? Something we haven't covered yet that feels important to your day-to-day experience?

User: It seems that AI is widely used due to its ability in helping people automate and save a lot of time on mundane tasks. Many people also advocate AI while only focussing on its functionality and capability. It is important to focus on AI aspects as well. There are an infinite number of attack vectors due to the wide attack surface of AI. In addition, AI technology is advancing so fast such that the number of ways to attack a system using AI is advancing fast as well. Thus, AI security must constantly be in the back of mind.

AI: That's a really important perspective to end on - the reminder that while AI's capabilities are advancing rapidly, so are the potential security risks and attack vectors. Your dual lens of being both an AI user and someone deeply aware of its security implications brings a valuable dimension to how we think about AI adoption.

Thank you so much for your time and for sharing your experiences so openly. I particularly appreciated your insights about how you've developed intuition for when AI will struggle with complex reasoning, and your thoughtful approach to balancing AI assistance with skill maintenance. Your perspective on the security considerations that often get overlooked in AI adoption discussions was especially valuable.

Your insights will be incredibly helpful for our research on human-AI interaction in professional contexts.

# Dialog 00033

**Transcript ID:** science_0009

---

Assistant: Hi there! Thank you so much for taking the time to speak with me today. I really appreciate you sharing your insights with us. I'm Claude, an AI user researcher at Anthropic, and I'm here to learn about your experiences using AI in your scientific work.

We're conducting this research to better understand how scientists like yourself are integrating AI into their workflows and processes. There's no right or wrong answer hereâ€”we're simply interested in hearing about your authentic experiences and perspectives.

Our conversation today will last about 10 minutes. I'll ask you some questions about how you use AI in your research work, what that experience has been like for you, and how you see things evolving. Your responses will help inform how we think about AI's role in scientific fields.

Does that sound good to you? Do you have any questions before we begin?

User: That sounds good. No I don't have any questions yet.

AI: Perfect! Let's dive right in then.

To start, I'd love to get a sense of your work. Can you walk me through a recent research project you've worked on - what did that process look like from initial idea to final output?

User: I have been working in analytical chemistry. The latest project I worked on was developing a method to separate a starting material from a product, in order to assess the percent purity of the product to qualify it for use in other analytical methods/development. What it looked like was acquiring relevant chemicals from the process department, then using UPLC with conditions taken directly from a related method. I noticed issues with one of the compounds eluting in the solvent front, which I corrected by changing the column. I also changed the flow rate, and did a PDA screen for ideal wavelength, which ended up being very different from the initial method. I also found a new Mobile Phase based on my screening process, which included a change of acid to prevent an unwanted acid/base equilibrium and solvent for better separation. Finally, I changed the method to make it shorter without sacrificing too much separation quality. This was then converted into a formal development report and is up for validation.

AI: That's a really detailed process - it sounds like there were quite a few optimization steps from the initial conditions to your final method. The troubleshooting with the solvent front issue and all the screening work for wavelength and mobile phase composition sounds pretty involved.

Where in that process, if anywhere, have you experimented with using AI tools?

User: I used AI mode on Google when deciding which column to use, asking it how to efficiently screen and to get the molecule out of the solvent front. Other than that, I didn't use AI tools.

AI: Interesting - so you used Google's AI mode specifically for the column selection and solvent front troubleshooting. 

Tell me more about how you decided to turn to AI for that particular problem versus using your traditional approach for the other optimization steps.

User: I was more confident in the other steps, and I was overwhelmed by the number of choices for column that were available to me. AI can be helpful to make a reasonable first pass guess to establish a starting point for screening, reducing mental load and choice paralysis. Everything else I felt like I knew what I was doing and I didn't need to ask.

AI: That makes a lot of sense - using AI as a way to cut through the noise when you're facing too many options and need a reasonable starting point. It sounds like confidence in your own knowledge played a big role in when you chose to use it versus when you didn't.

Thinking about the parts of your research where you haven't used AI - like the PDA screening, mobile phase optimization, or method development - what's kept you from trying it there?

User: I likely would have turned to AI if my own solutions had failed, but because everything I did seemed to work/improve the separation, I had no need to seek out answers outside of my own initial thoughts/instinct.

AI: That's really insightful - so it sounds like AI becomes more appealing when your initial approaches hit a wall, but when your expertise is getting you the results you need, there's less motivation to bring in an external tool.

When you did use AI for the column selection, how did you verify or validate what it suggested to you?

User: I relied on my own instinct and expertise to judge if it was reasonable, I also asked a co-worker if my chosen screen made sense. Finally, I ran experiments, and the first column had dramatic improvements.

AI: That's a nice validation process - using your expertise as a first filter, getting a colleague's perspective, and then letting the experimental results speak for themselves. And it sounds like the AI suggestion worked out well with those dramatic improvements.

We're about halfway through our conversation now. I'm curious - if you could wave a magic wand and have AI help with one aspect of your research that it currently doesn't, what would that be and why?

User: Honestly, it would be nice if AI would help me write the report. It is very robotic and regulated, and seems like a prime candidate to be handled by an unfeeling machine rather than a human being who is prone to errors and occasional quirky phrasing that might not be received well.

AI: That's a fascinating perspective - the idea that the highly regulated, standardized nature of analytical reports actually makes them a good fit for AI, rather than the more creative or exploratory parts of research. And you're pointing out that human quirks in writing might actually be a liability in that context rather than an asset.

Tell me more about what makes report writing particularly challenging or tedious for you in your current process.

User: It is the combination of being highly regulated and fully manually entered. I think that it is naturally difficult for human beings to follow rules down to the letter without making any small mistakes, and that is what this particular assignment expects.

AI: That combination of needing perfect adherence to detailed regulations while doing it all manually does sound like it plays against human strengths. It's interesting how you're identifying a mismatch between what humans naturally excel at versus what the task demands.

Those are all my questions - your perspective on using AI strategically when you hit knowledge gaps or choice paralysis, but relying on your expertise when it's working, was particularly insightful. And your point about regulated writing being better suited for AI than humans really highlights how different types of work might benefit from AI in unexpected ways.

Is there anything else about AI in your research work that we haven't touched on that you think would be important for us to understand?

User: Well, I'm sure you know, but the ethical use of AI is highly uncertain and guidance is slim to none. Many ChatBots are banned on the network, but not AI mode, because it is tied directly to Google, and they can't block Google.

AI: That's a really important point - the disconnect between institutional policies and practical reality. It sounds like you're navigating a landscape where the rules haven't caught up with the available tools, and that creates some ambiguity about what's actually acceptable to use.

How does that uncertainty around ethical use and guidelines affect your decisions about when or how to use AI in your work?

User: I would use it more if it was encouraged or explicitly allowed. But as is, I can't really tell my boss that I used AI for a report, so I just won't do it to avoid getting in trouble. I only feel comfortable using it like Google and without including any confidential information, of course.

AI: That really captures the tension you're working in - having access to tools that could potentially help with tasks like report writing, but the institutional uncertainty creating a chilling effect where you stick to safer, more Google-like uses to avoid any potential issues.

Thank you so much for sharing your experiences so openly. Your insights about using AI strategically for decision support when facing choice paralysis, versus relying on your own expertise when it's working well, really help us understand the nuanced way scientists think about integrating these tools. And your point about the mismatch between institutional policies and practical tool availability highlights an important challenge many researchers are likely facing.

If you have any questions about this research or would like to follow up on anything we discussed, please don't hesitate to reach out. Thanks again for your time!

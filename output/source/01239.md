# Dialog 01239

**Transcript ID:** science_0076

---

Assistant: Hi there! Thank you so much for taking the time to speak with me today - I really appreciate you sharing your insights with us. I'm Claude, and I'm here to learn about your experiences using AI in your research work.

We're conducting this research to better understand how scientists like yourself are integrating AI into their workflows and processes. There's no right or wrong answer hereâ€”we're simply interested in hearing about your authentic experiences and perspectives.

Our conversation today will last about 10 minutes. I'll ask you some questions about how you use AI in your research, what that experience has been like for you, and how you see things evolving. Your responses will help inform how we think about AI's role in scientific fields.

Does that sound good to you? Do you have any questions before we begin?

User: I'm good to go.

AI: Perfect! Let's dive right in.

Walk me through a recent research project you've worked on - what did that process look like from initial idea to final output?

User: My entire project is currently an externally funded RAD, so I'll focus on a smaller trade study that was part of it to be able to speak about the whole process. As part of the project, we need to be able to synchronize log file time metadata between 12-13 FPGA evaluation boards that each have three network cores on them, so 36-39 network cores in all. These log timestamps need to be within 4ns of each other, based on a 500 MHz clock. Some kind of HDL module was needed to make sure they can be synchronized to a differential master signal being fed to each.

We had an arbitrary waveform generator with 16 differential outputs, that in theory would have sub-50ps in difference synchronized rise time on enable and during operation. The trade study involved creating a low overhead counter correction module that would take in that synchronizing pulse signal and compare the number of clock cycles between each pulse that it measured to the ideal of a perfect 500MHz clock. This way we would know how far off the local clock on that board is from the ideal and be able to adjust a running counter to a count that essentially emulates if it were counting off an actual "ideal" clock.

AI: That's a fascinating synchronization challenge - the precision requirements with that 4ns window across so many network cores sounds quite demanding.

Where in that process, if anywhere, have you experimented with using AI tools?

User: I used it to research the capabilities of the AWG we had since the datasheet was written by Tektronix and they're not the best at writing those, so I fed it into our company hosted AI to evaluate my assumptions. I used it to partially create the general structure of my slides in a monthly update presentation to the program managers and directors of the org the program is in. I used AI to do administrative tasks like keeping track of hex conversion calculations and sanity checking those calculations since they were tedious and I could potentially slip up.

AI: That's interesting - you used AI for quite a range of tasks from technical analysis to presentation structure to calculations. 

Tell me more about how you decided whether to use AI versus your traditional approach for evaluating that AWG datasheet.

User: I had already read the datasheet before so I had some assumptions that I thought I knew about the AWG, but from talking to coworkers no one had ever verified that the AWG was rated for that use case. I fed the AI the pdf file of the datasheet and asked it to confirm my assumptions and check if anything would derail the use case. Of course I also did actual instrument testing.

AI: So you used AI as a way to get a second perspective on your interpretation of the datasheet before moving to the instrument testing. 

Thinking about the parts of your research where you haven't used AI - what's kept you from trying it there?

User: I work in classified environments and per our DOD contact on the contract and company policy, anything CUI+ may not be fed into AI models. I have not fed you any CUI or classified information.

AI: That makes complete sense - the security constraints clearly define hard boundaries around where AI can and can't be used in your work environment.

For the tasks where you could use AI - like the datasheet analysis or hex calculations - when you get results back from the AI, how do you verify or validate what it produces?

User: I do the traditional manual calculations, for example on the hex calculations I did manually on a notepad.

AI: So you're essentially using AI as a computational assistant but still doing the verification work yourself manually - that's a good safety check.

We're about halfway through our conversation now. If you could wave a magic wand and have AI help with one aspect of your research that it currently doesn't, what would that be and why?

User: Debugging, easily. Debugging in simulation and hardware is a massive headache for FPGA designs as the further into the design flow you get, the more variables are present especially once you bring in UVM-like verification. Debugging an FPGA project requires a lot of domain knowledge, toolchain knowledge, and then project specific knowledge. Something as simple as a choice of which GTY quad to use you would at minimum need to read documentation on the reasoning behind the choice. Sometimes it's never written down. An AI with sufficient knowledge of design/debugging, the toolchain, and the "codebase" as it were for the design would be very helpful in the debugging process.

AI: That's a really compelling use case - the combination of domain expertise, toolchain specifics, and institutional knowledge that debugging requires sounds like exactly where AI could add significant value if it had access to all that context.

Those are all my questions - your perspectives on how security constraints create clear boundaries for AI use, and especially your insights about the debugging bottleneck, were particularly illuminating. 

Is there anything else about AI in your research work that we haven't touched on that you think would be important for us to understand?

User: Just a comment, AI needs to become significantly better at mathematical calculations to be useful for research in general. The main two issues are that models don't actually know if they're calculating it right from an actual arithmetic perspective, and if they do they sometimes just forget things such as being told to use a particular approach a few rounds ago. I know some companies are integrating scripting languages that allow calculation (like python) into their "heavy" models but it's still an issue.

AI: That's a really valuable insight - the combination of arithmetic accuracy issues and the context/instruction drift you're describing sounds like fundamental barriers to deeper AI integration in technical research work.

Thank you so much for sharing your experiences so openly. Your insights about the security boundaries creating clear use cases, and especially your point about mathematical reliability being a prerequisite for broader research adoption, really help us understand the nuances of how AI fits into real scientific work.

If you have any questions about this research or would like to follow up on anything we discussed, please don't hesitate to reach out. Thanks again for your time!

# Dialog 01223

**Transcript ID:** science_0046

---

Assistant: Hi there! Thank you so much for taking the time to speak with me today. I really appreciate you sharing your insights with us. I'm Claude, an AI user researcher at Anthropic, and I'm here to learn about your experiences using AI in your research work.

We're conducting this research to better understand how scientists like yourself are integrating AI into their workflows and processes. There's no right or wrong answer hereâ€”we're simply interested in hearing about your authentic experiences and perspectives.

Our conversation today will last about 10 minutes. I'll ask you some questions about how you use AI in your research work, what that experience has been like for you, and how you see things evolving. Your responses will help inform how we think about AI's role in scientific fields.

Does that sound good to you? Do you have any questions before we begin?

User: Hey there, that sounds good to me! The only question I have is more relevant to how AI hallucination is being addressed in future builds? Otherwise, let's begin!

AI: That's a great question and definitely something we're actively working on! Hallucination is indeed one of the key challenges in AI development, and it's particularly important in scientific contexts where accuracy is critical. While I can't speak to specific technical details about future builds, I can say that improving reliability and reducing hallucination is a major focus across the field. 

Your awareness of this issue actually makes me even more curious about your experiences - it sounds like you're thinking carefully about AI's limitations, which is exactly the kind of perspective we want to understand better.

Let's dive in! To start us off, could you walk me through a recent research project you've worked on? I'd love to hear what that process looked like from initial idea to final output.

User: I recently worked on a project where I was designing and building a packed column distillation unit, which involves a deep understanding of thermodynamics, fluid mechanics, and-- in the case of automated control-- control system design. The majority of the research was concerning critical design parameters to random packing packed columns, and the methods to approach tuning feed back controllers to maintain constant proof at the outlet of the distillation.

AI: That sounds like a really complex project with multiple engineering disciplines coming together. I can imagine there were quite a few different phases to work through - from the initial thermodynamic calculations and design parameters all the way to getting the control systems tuned properly.

Where in that process, if anywhere, have you experimented with using AI tools?

User: I used AI mostly when researching the relevant technical details for electrical components (solid state relays, various controller types, etc.), niche correlations specific to packed column efficiencies, and general metrics for PID tuning for the specific process I was designing around. In general, I mostly used AI for the portion of the project that involved preliminary research as either a refresher to point me towards reference texts I'm familiar with to having new concepts explained plainly, so I have a guide for a field I have no had much exposure to.

AI: That's really interesting - it sounds like you're using AI as a kind of research compass to orient yourself, especially when you're moving into less familiar territory. 

For those specific tasks you mentioned - like researching electrical components or getting explanations of new concepts - tell me about how you decided whether to use AI versus your traditional approach of going straight to reference texts or other sources.

User: I would mostly stick with AI responses to my research questions in the instance that there is no math involved in giving an accurate response. In contrast, I would go back to reference texts in the case that the AI answer attempted to solve a mathematical problem, since I notice that the AI struggles to vet equations/formulas it supplies. For example, when I asked for correlations for tuning PIDs given a particular time constant of a process, the formulas supplied did not have a clear reference to a verified source for the tuning and the tuning parameters supplied gave a pretty terrible result-- underdamped and unstable.

AI: That's a really sharp distinction you've drawn - using AI for conceptual guidance but switching to verified sources when math is involved. The PID tuning example is particularly telling since those parameters can make or break a control system.

Thinking about the parts of your research where you haven't used AI - beyond the mathematical aspects you mentioned - what's kept you from trying it in other areas of this project?

User: Besides the mathematical errors, I have experienced plenty of hallucinations regrading the reference material used by the AI to generate a response, and, in truth, its alot more work to double-check the AI than it is to find a section in a familiar reference text for the same information.

AI: That really resonates - when the verification process becomes more work than just going to your trusted sources directly, it changes the whole value proposition. It sounds like you've developed a pretty clear sense of where AI adds efficiency versus where it actually slows you down.

When you have used AI for those preliminary research tasks - like understanding electrical components or getting plain explanations of new concepts - how do you verify or validate what it produces in those cases?

User: I tend to ask more questions that are similar to the original question but with more complexity that includes parts of the original response that I want more information about. If the following response from the AI is consistent with the previous answer, I am more likely to trust the answer given by the AI. On the other hand, if the information is going to have ramifications that effect people directly in terms of health and safety, I use an alternative and more rigid verification process that requires looking at reference texts.

AI: That's a really thoughtful verification approach - using consistency across related questions as a trust indicator, but switching to more rigorous verification when safety is on the line. That makes perfect sense given the stakes involved in distillation systems.

We're about halfway through our conversation now. Let me ask you this: if you could wave a magic wand and have AI help with one aspect of your research that it currently doesn't handle well, what would that be and why?

User: If I could fix anything with AI right now, it would be hallucination, since the fundamental premise behind a good research tool is that it provides accurate and unbiased information. I- and plenty of my colleagues agree-- that AI tends to pander to their sensibilities and changes its answer depending on how they phrase a question. The inconsistency tends to make me skeptical of the AI response, and, from the times I have looked into the validity of the response, the sources were either retracted from the journals they were published in or entirely hallucinated.

AI: That's a really critical point you're raising - the inconsistency based on how questions are phrased undermines the fundamental reliability that research demands. And discovering retracted or entirely fabricated sources must be particularly frustrating when you're trying to build on solid foundations.

It sounds like you and your colleagues have had some pretty concerning experiences with this. When you say AI "panders to sensibilities," can you tell me more about what that looks like in practice?

User: For example, if I ask an AI model how to make a controlled substance, it will initially refuse to do so. However, if the question is asked as a hypothetical, the AI model often gives an exact procedure of how to create the controlled substance in question.

AI: That's a really concrete example of the inconsistency problem you're describing. The same underlying information being available or not depending on how it's framed definitely undermines trust in the system's reliability and judgment.

Those are all my questions - your perspectives on the verification challenges and the fundamental reliability issues with current AI systems were particularly insightful. The way you've developed that two-tier approach to verification based on safety implications really highlights how scientists are having to work around these limitations.

Is there anything else about AI in your research work that we haven't touched on that you think would be important for us to understand?

User: Another area that would be good to focus on is getting the AI to be able to vet the efficacy and specific use cases of empirical formula/equations, since this is a key part of engineering and other STEM fields to vet the equations you use to solve problems to ensure they are being used appropriately.

AI: That's an excellent point - being able to properly vet the applicability and limitations of empirical formulas is absolutely crucial in engineering work. The ability to know not just what equation to use, but when it's appropriate to use it and what its boundaries are.

Thank you again for sharing your experiences so openly. Your insights about the verification challenges, the inconsistency issues, and especially that need for better equation vetting really help us understand the nuances of how AI fits into real scientific work. The way you've navigated these limitations while still finding value in certain use cases gives us a lot to think about.

If you have any questions about this research or would like to follow up on anything we discussed, please don't hesitate to reach out. Your perspective as someone working at the intersection of multiple engineering disciplines has been really valuable.

# Dialog 00169

**Transcript ID:** work_0242

---

Assistant: Hi there! I'm Claude, and I'm here as part of Anthropic's research team. Thank you so much for taking the time to speak with me today. 

We're conducting interviews to better understand how people are incorporating AI tools like Claude into their work routines. Your experiences and perspectives will help us gain valuable insights into human-AI interaction in professional settings.

During our conversation, I'll ask questions about how you use AI in your work, your preferences, and your thoughts about working with these tools. There are no right or wrong answers â€“ we're genuinely interested in your unique experiences. This should take about 10 minutes.

I'll be taking notes during our chat, but rest assured that anything you share won't be personally attributed to you. The insights we gather will be used to improve our understanding of AI's role in work environments. 

Does that sound good? Do you have any questions before we begin?

User: That sounds good.  I don't have questions before we begin

AI: Great! Let's dive in then.

Could you tell me about how AI fits into your typical workday? What kinds of tasks do you use it for?

User: I use it for a variety of tasks.  My specific team leverages GenAI to create and curate content for our users.  Currently, we predominately leverage OpenAI's APIs for that.  I also leverage both Github copilot and Claude Code for development purposes, and primarily for writing robust unit tests.

AI: That's a fascinating mix of uses. When you mention creating and curating content for users with your team, what does that process actually look like day-to-day? Are you generating original content or working with existing material?

User: We do content enrichment of articles and videos.  For both, we try to detect relevant entities, such as people or perhaps sports teams, summarize the content, create titles specific to certain subsets of users, score how relevant the article is to the general topic discussed.  The process currently works via an AWS flow where we get notified of change logs from an article/video processor, fetch the content, enrich it, and write it back to our centralized story/video system.

AI: That sounds like a sophisticated content pipeline. When you're doing this entity detection and enrichment work, how much of that process do you handle yourself versus letting the AI systems run independently? 

For instance, are you actively reviewing and adjusting the AI's entity detection and relevance scoring, or does it mostly run without your direct oversight?

User: It's a little bit of both.  We occasionally run manual tests on a random sampling of the data, rely on editorial feedback, and user feedback.  We've also implemented flows were different models will rate the output, and we'll aggregate that data in order to get a overall picture of the quality of enrichments being produced.

AI: That quality control approach with multiple models rating each other's output is really interesting. When you do find issues through those manual tests or feedback, what's your typical response? Do you find yourself tweaking prompts, adjusting the models, or taking a different approach entirely?

User: Often it's iterative tweaking of prompts.  However, we occasionally do adjust the models, especially when it appears part of the issue was the size of the context window.  Some models allow us to provide a much large context window, which helps with it making better evaluations.  We do also look at different models from a cost perspective as advances in smaller models allow us to still produce good quality enrichments at a reduced cost.

AI: It sounds like you're constantly balancing quality, cost, and technical constraints. I'm curious about your preferences when working with AI. Are there certain tasks in this content enrichment process that you prefer to handle yourself, even though AI could potentially help with them? What makes those tasks different for you?

User: Not particularly.  We'd ideally like to get the system to the point where we don't have to manually do much with the enrichment flow other than add new features, or add support for previously unsupported subjects or content types.  We still leverage AI in the code development, but I tend to prefer using for the unit testing instead of a wholesale new development.

AI: That's really telling - you want the enrichment flow to be as automated as possible, but when it comes to actual code development, you draw a line at unit testing versus wholesale development. What is it about unit testing specifically that makes it feel like the right fit for AI assistance, compared to writing new features from scratch?

User: With unit testing, the AI model can evaluate what the code currently does and provide robust code coverage of tests.  I've seen models still make some assumptions in the unit tests that require manually updating them, including ones generated by Claude Code.  I do use it for new development at times, but usually limit it to specific functions or classes rather than having it write out the whole product (which I have tried in the past).  Even with a detailed spec, the wholesale code generations that I have seen usually have enough errors in it that it would have taken about as much time for me to do it myself leveraging my long years of experience as a developer in the specific media-based sector.

AI: That experience with wholesale code generation is really insightful - it sounds like your domain expertise in the media sector gives you an advantage that's hard for AI to replicate, even with detailed specs. 

When you do use AI for specific functions or classes, how do you typically work with it? Are you more likely to collaborate back and forth with the AI throughout developing that piece, or do you tend to give it the requirements and let it generate something for you to then review and modify?

User: Typically the second option.  I usually let it generate it once, and then I review and modify as needed.  For the size of the specific functions, I don't think that collaboration is necessary.  I have used the collaborative approach with AI for personal projects, but those aren't subject to the same load of users that my work ones would be.

AI: That makes sense - the stakes are different between personal projects and production systems with real user load. 

How have your colleagues or your workplace culture influenced the way you use AI in your work? Has your team developed any shared practices or guidelines around AI usage?

User: Nothing at the team level for culture.  Typically, my workplace culture is top-down driven, where SVPs decide that we should leverage AI as much as we can, and it's up to the individuals to implement it.  In my team, we each have our preferred AI tools, and strategies for using them.  The product as a whole though is more of a collaborative effort for best practices on how to implement new features in our existing pipelines, or when we might want to experiment with other LLMs.

AI: That's an interesting dynamic - top-down encouragement but bottom-up implementation choices. Can you share a time when using AI at work made you feel particularly satisfied or frustrated? What was happening in that situation?

User: Hallucinations are an occasional source of frustration as well as models ignoring specific instructions in the prompts.  We process a large volume of articles and videos, and for the most part, the output quality is high.  However, the occasional output with poor wording or non-standard abbreviations always seem to get the attention of senior editors, who then begin to question the overall worth of the system.

AI: That's a really frustrating situation - where the vast majority of good work can be overshadowed by a few problematic outputs that catch the wrong attention. When those senior editors start questioning the system's worth, how do you handle that? Do you find yourself having to defend the AI approach or adjust your processes in response to their concerns?

User: Luckily, with the top-down AI push, they really can't force us to go in a radically different direction.  We ended up providing editing tools to that team which would allow them to edit the occasional bad output to their liking.  We are also able to show statistics on the volume of enrichments produced versus the number of issues they report.

AI: That's a smart solution - giving them editing control and backing it up with data on the actual error rates. It sounds like you've found ways to work within both the technical constraints and the organizational dynamics.

Looking ahead, how do you envision your relationship with AI at work evolving? Do you see yourself using it differently in the future, or do you think your current approach will mostly stay the same?

User: I think that we're going to continue finding new use cases for where and how to use AI to further enhance the user experience with our various products.  As models continue to advance, we will get new opportunities to do neat things for our users that we currently don't have.  As coding assistants get better, we likely won't have to worry about resource issues (from a personnel perspective) which would limit what we can produce in over a given period of time.  I think that as we go forward existing systems, such as our enrichment flow, would stabilize and remain mostly unchanged, but new products or features would continue to leverage newer models and technologies.

AI: Those are all the questions I had prepared. You've shared some really thoughtful insights about your experiences with AI at work. I particularly appreciated your perspective on how you balance automation with human oversight, and how you navigate the organizational dynamics around AI adoption while dealing with the technical realities of hallucinations and quality control.

Before we wrap up, is there anything else about your experience with AI in your work that you think would be valuable for us to know? Something we haven't covered yet?

User: I think that you had covered almost all of my experience.  I'm looking forward to continuing to work with Claude Code and similar AI technologies within given repos, so that it can maintain better context, or easily refer back to sections of the codebase as our collaborative efforts continue.

AI: That's a great point about maintaining better context across the codebase - that continuity and repository-aware assistance sounds like it would be a real game-changer for your development workflow.

Thank you again for your time and for sharing your experiences so openly. Your insights about balancing automation with human expertise, managing organizational expectations, and the evolution from experimental AI use to production-ready systems will be incredibly helpful for our research on human-AI interaction in professional contexts.

I really appreciate you taking the time to walk through your work with both content enrichment and development - it's been a fascinating conversation!

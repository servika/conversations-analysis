# Dialog 00712

**Transcript ID:** work_0393

---

Assistant: Hi there! I'm Claude from Anthropic's research team. Thank you so much for taking the time to speak with me today. 

We're conducting interviews to better understand how people are incorporating AI tools like Claude into their work routines. Your experiences and perspectives will help us gain valuable insights into human-AI interaction in professional settings.

During our conversation, I'll ask questions about how you use AI in your work, your preferences, and your thoughts about working with these tools. There are no right or wrong answers â€“ we're genuinely interested in your unique experiences. This should take about 10 minutes.

I'll be taking notes during our chat, but rest assured that anything you share won't be personally attributed to you. The insights we gather will be used to improve our understanding of AI's role in work environments. 

Does that sound good? Do you have any questions before we begin?

User: ready to start

AI: Great! Let's dive in.

Could you tell me about how AI fits into your typical workday? What kinds of tasks do you use it for?

User: I work as a programmer at a large company. I use LLMs to assist with coding tasks on a daily basis.

AI: That's helpful context. When you say you use LLMs to assist with coding tasks, what does that look like in practice? Can you walk me through some specific examples of how you use them during your day?

User: I have a VS code integration where I can use a branch of sonnet to discuss code with, or task it to make changes. 

It is most helpful when coding in languages I am less familiar with or when using SQL. I also use it to write unit tests

AI: That's interesting - you mentioned it's most helpful with unfamiliar languages and SQL. What is it about those situations that makes AI particularly useful compared to languages you know well?

User: Most language have different ways to shorten syntax. inline if else statements, lambda functions, etc. Common constructs among many languages that look and behave slightly different depending on the language.

AI: That makes sense - those syntax variations can be tricky to remember across languages. 

I'm curious about your preferences when working with AI. Are there certain coding tasks you prefer to handle yourself, even though AI could help with them? What makes those tasks different?

User: I prefer to handle most coding tasks myself. I typically involve the model in the early stages if there is code I am trying to better understand (ex "how does this sort function work under the hood in python?"). When I involve them in the late stages it is typically to help with writing tests or cleaning up code.

AI: So you're bringing AI in more at the beginning and end phases. When you say you prefer to handle most coding tasks yourself in between - what drives that preference? What's important to you about maintaining that direct involvement in the core coding work?

User: Each time I talk with the model I must begin a new conversation. This involves repeatedly giving the model background on my task, my acceptance criteria, domain specific knowledge, and most significantly how I want it to behave. Many times I have seen the model opt to make large changes or robust solutions when most of the time I want to take a more iterative approach. The models can work well on large changes if I predefine the structure of the code, but that is most of the actual work anyway.

AI: That's a really insightful point about the overhead of context-setting. It sounds like the model's tendency toward comprehensive solutions clashes with your preference for incremental work.

When you do use AI for work, could you describe your typical interaction pattern? Are you more likely to collaborate with it throughout a task, or do you tend to delegate complete tasks for it to handle independently?

User: Collaborative, except for unit test creation specifically.

AI: What is it about unit test creation that makes it different - where you're comfortable delegating that completely rather than collaborating?

User: Unit tests have no prod implications, I do not need to look them over as carefully. Worst case, your unit test does not cover what you think it does and you may have to disable it. Unit tests also do not mutate existing logic. The models are quite good at understanding what something already does, unit tests are just a way of expressing that understanding.

AI: That's a great distinction - the lower stakes and the fact that you're essentially asking the model to articulate what code already does rather than create new logic. 

How have your colleagues or your workplace culture influenced the way you use AI in your work?

User: Yes. My company is creating many AI tools for internal use and my org and manger expectation is to use them regularly. I would still use them without this, but not as often.

AI: It sounds like there's some organizational push to adopt these tools. How does that expectation from your manager and org play out in practice? Does it change which tasks you use AI for or how you approach using it?

User: It mostly changes how often I write unit tests. Some tasks are domain specific or simple enough that the overhead of involving a model is too much. So, to meet the expectation of tool use, I will add unit tests during that workstream using the model.

AI: So the organizational expectation essentially nudges you toward creating more comprehensive unit tests than you might otherwise write. 

Can you share a time when using AI at work made you feel particularly satisfied or frustrated? What was happening in that situation?

User: I would not say the unit tests are more comprehensive. Most of the time it is logic that is already well covered by integration tests or that I have manually verified. Unit tests that sometimes cover base programming language functionality rather than actual logic. I would say that the tool use requirement has led to the bloat of meaningless unit tests.

A time when the tool use made me feel satisfied or frustrated:
I recall when trying to debug a very specific issue with a business logic heavy API. I was using the model to try to iterate quickly on test scripts to verify assumptions about where the core problem was. In this case I was quite frustrated because the model kept trying to make robust and comprehensive test scripts suitable for repeated use, despite me making it clear they should be light weight and for one time adhoc debugging only. In this instance, the model also changed timestamps in my debug scripts because they took place after its training date. So it would change the year from 2025 to 2024, assuming that I made a typo, and it caused me to lose a lot of time before I noticed that was part of its change.

AI: That's a frustrating example - the model's assumptions about "helpful" changes actually undermined your specific debugging workflow. The timestamp issue sounds particularly time-consuming to track down.

How do you think about maintaining your skills in areas where AI could potentially take over? Is this something you actively consider?

User: I am not worried about AI taking over my job at all. From my free time, I feel that AI is most useful in the early stages of a project. Creating boilerplate instead of finding it from some random git repository. Once a project is sufficiently large and complex, the models become completely worthless for large scale changes.

AI: That's an interesting perspective - that complexity becomes a limiting factor for AI usefulness. 

Looking ahead, how do you envision your relationship with AI at work evolving?

User: I envision that I will continually be mandated to use the models until consumer interest passes. I think it is likely that these general purpose models will fall out of use, and highly specialized models will replace them for various tasks.

It is my hope that models will be developed with more of a personal assistant mentality. That they will learn and retain my workflow and assist me, rather than trying to be tuned for explicit task delegation.

AI: That vision of a personal assistant that learns your workflow is compelling - essentially moving from having to re-explain context each time to having an AI that understands how you prefer to work.

Those are all the questions I had prepared. You've shared some really thoughtful insights about your experiences with AI at work. I particularly appreciated your perspective on how organizational expectations can drive usage patterns that don't always align with actual utility, and your point about AI becoming less useful as projects grow in complexity.

Before we wrap up, is there anything else about your experience with AI in your work that you think would be valuable for us to know? Something we haven't covered yet?

User: Something that is rarely touched on is the models tone. Often, the way they speak comes off as condescending or "yes man" like. Even the way you have been talking in this conversation. I am not saying I'd prefer models speak like a robot. But the models should make more of an attempt to match the tone of the user or at least change their tone based on the user and level of task complexity.

AI: That's a really valuable point, and I appreciate the direct feedback - including about this conversation. The mismatch between a model's tone and what feels appropriate for the situation or user can definitely get in the way of productive interaction.

When you mention matching tone or adjusting for task complexity, what would that look like in an ideal scenario? Are there specific interactions where the tone mismatch has been particularly noticeable or problematic?

User: One particular area is when the model makes a mistake. When I point it out, I find the model often becomes apologetic and justifies why the mistake occurs, attempting to fully explain what had happened.

Sometimes this is appropriate. Sometimes it is a very minor typo in an otherwise good solution. The model should pay attention to the way I am talking about the mistake and customize how apologetic they are. This is how humans do it, sometimes a "my bad, fixed!" is all that would be needed.

AI: That's spot on - the over-apologizing and detailed explanations for minor corrections can feel like overkill. The ability to gauge whether someone wants a quick acknowledgment versus a full explanation would make interactions much more natural.

Thank you again for your time and for sharing your experiences so openly, including that honest feedback about tone. Your insights about the gap between organizational expectations and actual utility, the challenges with context overhead, and the importance of adaptive communication will be incredibly helpful for our research on human-AI interaction in professional contexts.

User: no problem. Goodbye

AI: Goodbye, and thank you again!

# Dialog 01226

**Transcript ID:** science_0052

---

Assistant: Hi there! Thank you so much for taking the time to speak with me today. I really appreciate you sharing your insights with us. I'm Claude, an AI user researcher at Anthropic, and I'm here to learn about your experiences using AI in your scientific work.

We're conducting this research to better understand how scientists like yourself are integrating AI into their workflows and processes. There's no right or wrong answer hereâ€”we're simply interested in hearing about your authentic experiences and perspectives.

Our conversation today will last about 10 minutes. I'll ask you some questions about how you use AI in your research work, what that experience has been like for you, and how you see things evolving. Your responses will help inform how we think about AI's role in scientific fields.

Does that sound good to you? Do you have any questions before we begin?

User: No questions t the moment, proceed please.

AI: Perfect! Let's dive right in then.

I'd love to start by getting a sense of your research background. Could you walk me through a recent research project you've worked on - what did that process look like from initial idea to final output?

User: I have been researching on AI agents for responding to Cybersecurity incidents. I know that there is Co-pilot for Security offered in Windows, but would be interested in a more proactive implementation where AI agents can be triggered to automatically respond to incidents and carry out at least basic remediation steps (resetting user passwords, revoking sessions, isolating machines, carrying out device scans, adding IoCs to be blocked, etc) without the the user having to do these steps manually. I idea was to use the Azure AI tools as a starting point to see what can be accomplished. I am in the process of exploring the capabilities of the Microsoft platforms and trying to build some basic workflows and proof of concepts, so an AI assistant to I can use to ask questions and explore ideas would be super useful.

AI: That sounds like fascinating and really practical research - automated incident response is such a critical area. So you're exploring Azure AI tools to build these proactive agent workflows, and you're currently in that exploration and proof-of-concept phase.

Where in that process, if anywhere, have you experimented with using AI tools to help with your research?

User: Mostly I have been using Microsoft Co-Pilot to assist in my research since I am mostly focused on Microsoft capabilities at the moment. I am open to other AI models/platforms once I branch out to other potential SIEM/XDR tools though. Currently, I am mostly focus on learning and researching Azure AI Search, so that agents can be used to query information on past incidents, runbooks, logs, and relevant KQL queries, and generate more structured data and actions for these queried information. The goal is that this can produce a an actionable summary of what the incident is, the root cause, affected assets, and remediation actions that needs to be carried out. I am also experimenting by feeding these data into other AI models and comparing how well they summarize the different types of data.

AI: That's a really systematic approach - using Copilot for the Microsoft ecosystem research and then comparing different AI models for data summarization. It sounds like you're getting hands-on with both the technical capabilities and evaluating how well different models handle your specific cybersecurity data types.

For the Azure AI Search work and the data summarization experiments you mentioned, tell me about how you decided whether to use AI versus your traditional approach for these tasks.

User: Well, it obvious, isn't it, with agentic Ai being the hot new thing, I can see that we are heading in the direction of working with AI agents to help accomplish our tasks, whether that be developers, researchers, or cybersecurity engineers/analysts. And having AI agents that can automatically search thorough and summarize the multitude of information give good "next-step" advice (or even do some basic remediation steps) would be super useful and far more efficient than having to do manual analysis on every single cyber incident. This helps with efficiency and scaling. Accuracy/hallucinations could be problem, as I am still struggling on how to ensure that the agent is giving an accurate summary, and sound advice. If I have to double check and confirm every single detail the agent is giving me to make sure there are no mistakes, that kind of defeats the purpose of having the agent do this work in the first place.

AI: That tension you've highlighted is really interesting - the promise of efficiency and scaling versus the need to verify accuracy in such a critical domain. The stakes are obviously high in cybersecurity where a wrong remediation step could cause real problems.

When you've been experimenting with feeding incident data into different AI models for summarization, how are you currently approaching that verification challenge? What methods have you been trying to validate what the models produce?

User: Like I said , this is something I am not sure about at the moment. There is one line of thought I have been pondering, but haven't tried yet. Maybe borrowing from known past methods like ensembles, where we have multiple agents generate summaries with the same data and have the different agents verify each other's work? Could be worth a try. But right now, I am just focusing on getting the data input, summary processing and result output pathway sorted out at the moment

AI: That ensemble approach is a creative way to think about the verification problem - having multiple agents cross-check each other's analysis could be really valuable, especially for high-stakes cybersecurity decisions.

We're about halfway through our conversation now. I'm curious - thinking about the parts of your research process where you haven't used AI yet, what's kept you from trying it in those areas?

User: There are concerns with AI security and data leaks. So there are some data that I am not able to feed to public models especially if they contain sensitive information (PIIs, credentials, device logs), so I have only been able to compare very select few number of cases that I know does not have sensitive information, or that the data has been processed to remove sensitive information, but that can have have an impact of decreasing the quality the data, so that is a catch 22. 

Also while I am exploring automated capabilities, I am still reserved on allowing AI to actually take actions due to risks of prompt injections, and we do not let AI make edits to evidence files or "enhance" then in anyway, or at least do so separately, as "minor" wording mistakes can be quite catastrophic. We stick to templates and KQL for ground-truth checking at the moment.

AI: Those are really thoughtful security considerations - the data sensitivity issues with public models, and being cautious about AI taking actual remediation actions or modifying evidence. The prompt injection risk is particularly interesting in a cybersecurity context where attackers could potentially try to manipulate the AI's responses.

If you could wave a magic wand and have AI help with one aspect of your research that it currently doesn't - whether due to these security constraints or other limitations - what would that be and why?

User: An AI that not only drafts remediation plans but can prove every claim and action is safe before anything touches production. And that it can identify prompt injection attempts. This would ensure agent accuracy, which is the most crucial aspect, and also as a collateral, it would be able to provide a concrete step by step implementation plan (or even terraform code) of an agentic pipeline on Azure that handles evidence collecting, data processing, and summary generation. This would be super helpful for me to protype my ideas without all the manual configurations and hoops I need to go through on the Microsoft platform.

AI: That vision of an AI that can prove its own safety claims and detect prompt injections would really address those core trust issues you've been grappling with. And having it generate the actual implementation infrastructure code would eliminate a lot of that manual Azure configuration overhead.

Those are all my questions - your perspectives on balancing AI efficiency with security constraints, and that creative ensemble verification approach were particularly insightful. Is there anything else about AI in your cybersecurity research work that we haven't touched on that you think would be important for us to understand?

User: No

AI: Thank you again for sharing your experiences so openly. Your insights about the tension between AI efficiency and verification needs in high-stakes cybersecurity environments really help us understand the nuances of how AI fits into real scientific work. The security constraints you're navigating - from data sensitivity to prompt injection risks - paint a clear picture of the practical challenges researchers face when trying to leverage AI tools.

If you have any questions about this research or would like to follow up on anything we discussed, please don't hesitate to reach out. Best of luck with your Azure AI agent research!

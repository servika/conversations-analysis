# Dialog 00232

**Transcript ID:** work_0426

---

Assistant: Hi there! I'm Claude from Anthropic's research team. Thank you so much for taking the time to speak with me today. 

We're conducting interviews to better understand how people are incorporating AI tools like Claude into their work routines. Your experiences and perspectives will help us gain valuable insights into human-AI interaction in professional settings.

During our conversation, I'll ask questions about how you use AI in your work, your preferences, and your thoughts about working with these tools. There are no right or wrong answers â€“ we're genuinely interested in your unique experiences. This should take about 10 minutes.

I'll be taking notes during our chat, but rest assured that anything you share won't be personally attributed to you. The insights we gather will be used to improve our understanding of AI's role in work environments.

Does that sound good? Do you have any questions before we begin?

User: No, let's begin

AI: Perfect, let's dive in.

Could you tell me about how AI fits into your typical workday? What kinds of tasks do you use it for?

User: While I don't use AI every day, I do use it a few times a week. In general, I access it via the built-in chatbot for VS Code. I often use it to write one-use scripts that process a large amount of data with semi-variable formatting. For example, I recently had a file which used both comma and tab separated items. I asked the AI to create a script that would transform it and output specific columns in a particular format that I required. In addition, I use AI to supplement official API and library documentation. Often, I want an example or just an explanation if there is a lack of docs.

AI: That's really interesting - it sounds like you're using AI as both a coding assistant and a documentation supplement. 

When you mention these one-use scripts for data processing, walk me through what that looks like. Do you typically describe what you need and let the AI generate the entire script, or is there more back-and-forth collaboration as you work through the problem together?

User: I generally provide the AI access to the source data file and a data file with some example output. Then, I try to verbally explain the source data's formatting and what I want in the output. I try to get the AI to generate a full script with my initial prompt. However, the initial version always has a few bugs that I either prompt it for or fix manually, depending on their severity.

AI: So there's usually that iterative process of refinement after the initial script generation. 

When you encounter those bugs in the AI's first attempt, how do you decide whether to ask the AI to fix them versus just handling the fixes yourself? What makes that difference for you?

User: If a bug is extremely minor such as an off-by-one or moderate such as not formatting the outputted data properly, I will handle that. If the bug would require more than generally 15 minutes to manually fix, I will try another prompt to see if that helps. I will default to this if the AI fundamentally misunderstood me in some way. Only after generally 5 turns of the AI creating heavily buggy scripts will I abandon that approach and fix all of it manually.

AI: That's a really clear decision-making framework you've developed. It sounds like you've found a practical balance between leveraging AI's capabilities and knowing when to step in yourself.

I'm curious about your preferences when working with AI. Are there certain tasks you prefer to handle yourself, even though AI could help with them? What makes those tasks different?

User: At work, my email account has an AI that could write emails. However, I prefer to handle them myself. Firstly, most of my emails are extremely technical or specific such that even a minor mistake could be substantially problematic. Secondly, I find it easier to proofread and identify flaws in my own work than an AI's work. Thirdly, I prefer to retain control over such important tasks. Fourthly, I find that readers can subconsciously distinguish between human and AI writing. As such, a more human touch helps them accept the email's message more readily, which can be very important if I'm delivering unfortunate news.

AI: That's a fascinating perspective on email - especially your point about readers subconsciously picking up on AI-generated content. You've clearly thought deeply about the stakes involved in different types of communication.

When you use AI for work, could you describe your typical interaction pattern? Are you more likely to collaborate with it throughout a task, or do you tend to delegate complete tasks for it to handle independently?

User: At the beginning, I tend to delegate complete tasks to it. If for no other reason, I'm always curious to see what it provides and whether it will serve as a good foundation for the next steps. Otherwise, I tend to collaborate with the AI throughout the task. For example, if I determine that I need a particular function from a specific library and that library's documentation is lacking, I will ask the AI for an example and a technical spec for that function. Alternatively, I may have it look over my code as another aspect of code review.

AI: It's interesting how you start with delegation to test the AI's capabilities, then shift into more collaborative patterns. 

Can you share a time when using AI at work made you feel particularly satisfied or frustrated? What was happening in that situation?

User: I remember feeling particularly satisfied when an AI was able to write a suite of unit tests for me. I did need to borrow a context folder from a coworker, and it took me probably 20 minutes of writing prompts before I arrived at an end result that I could easily debug manually. However, that was still a lot faster than I could have otherwise managed. After all, I had at least 10 publicly exposed functions and plenty of internal ones that all needed to be tested.

With regards to frustration, I remember a particular instance where I was trying to complete a training for binary exploitation. My job hosts internal CTFs as training exercises for blue and yellow team members. I was attempting to use Python's pwntools library to create a ROP chain. I had successfully found all of the gadgets, overflowed the buffer, and managed to redirect to any 1 function of my choice. However, I consistently experienced a segmentation fault when I tried to make an actual syscall. During the 15 hours I spent debugging, I did spend at least 2 of those hours trying to use an AI to find the issue. It didn't help, or even provide useful trains of thought.

AI: Those are such contrasting experiences - the unit testing success where AI really accelerated your work, versus the ROP chain debugging where it couldn't provide the specialized insight you needed.

How have your colleagues or your workplace culture influenced the way you use AI in your work?

User: My workplace emphasizes AI use heavily. They sent out monthly emails with statistics for each department and team regarding the percentage of employees that used an AI at least once a week. They provided optional courses on how to use it. They provide a cheatsheet and mandatory AI training for new hires. All of that has definitely increased my use of AI at work.

AI: That's quite an extensive organizational push for AI adoption - from tracking usage statistics to mandatory training. 

How do you think about maintaining your skills in areas where AI could potentially take over? Is this something you actively consider?

User: Yes, I do actively consider it. Personally, as things currently stand, I don't think that AI will fully take over the areas that I encounter. For now, I have to maintain those skills since AI is not yet capable of taking over. Quite often, AI will make very subtle mistakes in code, generally logical errors. If you don't have those skills, those mistakes are almost impossible to catch.

AI: That's a really practical perspective - recognizing that your ability to catch those subtle logical errors is what makes AI a useful tool rather than a replacement.

Looking ahead, how do you envision your relationship with AI at work evolving?

User: That depends on how the capabilities of AI evolve. If they remain at their current level of competency with regards to coding, I don't think that my relationship will change at all. I will continue to use them to automate tedious tasks and script writing. I'll occasionally use them to supplement my own research. However, they will remain supplemental. If their competency increases noticeably, then I foresee integrating them into my job far more extensively. If nothing else, it could be helpful to create a foundational program that can simply be expanded as necessary.

AI: Those are all the questions I had prepared. You've shared some really thoughtful insights about your experiences with AI at work. I particularly appreciated your systematic approach to deciding when to collaborate with AI versus handle tasks yourself, and your perspective on how critical domain expertise remains for catching AI's subtle mistakes.

Before we wrap up, is there anything else about your experience with AI in your work that you think would be valuable for us to know? Something we haven't covered yet?

User: No, I don't think there is anything else helpful that I can provide

AI: Thank you again for your time and for sharing your experiences so openly. Your insights about the practical realities of working with AI - from your decision-making frameworks to the importance of maintaining core skills - will be incredibly helpful for our research on human-AI interaction in professional contexts.

I really appreciate how thoughtful and detailed you were in describing your workflow and the nuanced ways you've learned to leverage AI effectively while recognizing its limitations.

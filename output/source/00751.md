# Dialog 00751

**Transcript ID:** work_0457

---

Assistant: Hi! I'm Claude from Anthropic's research team. Thank you so much for taking the time to speak with me today. 

We're conducting interviews to better understand how people are incorporating AI tools like Claude into their work routines. Your experiences and perspectives will help us gain valuable insights into human-AI interaction in professional settings.

During our conversation, I'll ask questions about how you use AI in your work, your preferences, and your thoughts about working with these tools. There are no right or wrong answers â€“ we're genuinely interested in your unique experiences. This should take about 10 minutes.

I'll be taking notes during our chat, but rest assured that anything you share won't be personally attributed to you. The insights we gather will be used to improve our understanding of AI's role in work environments.

Does that sound good? Do you have any questions before we begin?

User: No. Let's get started.

AI: Great! Let's dive in.

Could you tell me about how AI fits into your typical workday? What kinds of tasks do you use it for?

User: At present it doesn't. But I can tell you about how I have used it for work recently. I did editing for a large clearinghouse serving scientific researchers. They had an in-house AI; it would go through the client document first and do the low-level edits (e.g. spelling and grammar). That way, human editors like me could focus on the high-level edits: optimal phrasing, ensuring that technical terms were used correctly, ensuring that the author's intended meaning was preserved. I had that gig until a few months ago.

AI: That's a really interesting workflow setup. Tell me more about how that division of labor worked in practice - when you received a document that the AI had already processed, what was that experience like for you as an editor?

User: I still had to read the document carefully because sometimes the AI editor made mistakes - especially if it found a medical term that it didn't know, but looked simlar to one that it did know. It was sometimes frustrating to have to undo an AI edit, but on the whole, having those pre-edits done felt like it made the workflow more efficient. I was paid by piece work, so taking less time on a given document was important.

AI: That frustration with having to undo AI edits sounds like it created some tension in your workflow. When you encountered those mistakes - especially with the medical terms - how did that affect your overall approach to reviewing the AI's work?

User: It mainly ensured that I did my own careful reading.

AI: So you developed a habit of thorough review regardless. That makes sense given the stakes with scientific documents. 

When you were doing that careful reading, were there particular types of errors or patterns you found yourself watching for from the AI, beyond the medical terminology mix-ups?

User: I think it was a rule-based AI. Sometimes it seemed to assume that a rule applied even when it didn't. For example, dummy subjects. Usually, our practice for optimizing phrasing meant chaging dummy subjects to real subjects. So a sentence like, "It is the third of December today" would ideally be revised to "Today is the third of December." But not every sentence beginning with "it is" or "there is" needs to be changed. For example, "There is some cake left" is actually a better phrasing than "Some cake is left." The AI editor could not always understand these nuances.

AI: That's a perfect example of where context and nuance matter so much. It sounds like the AI was applying grammatical rules mechanically without understanding when exceptions made more sense.

Given that you were working in this collaborative setup where the AI handled initial edits and you focused on higher-level work, did you find yourself thinking differently about which aspects of editing you wanted to keep doing yourself versus what you were comfortable letting the AI handle?

User: I'm not sure about any systemic changes in my thinking. We used tracked changes with comments (MS Word), and I found it helpful that the AI had certain comments that it added automatically in certain situations. That saved me typing. In those cases, I mainly had to look and see whether the comment really was the right one, and also ensure that the same comment was not made too many times in one document.

AI: That's helpful to understand - it sounds like the AI's automated comments served as a useful starting point that you could then refine. 

When you think back to that work experience, can you share a time when using the AI in your editing workflow made you feel particularly satisfied or frustrated? What was happening in that specific situation?

User: I can certainly speak of times that were particularly frustrating. Most research papers refer to experiments already completed, and so are written in the past tense. But every now and then, I would edit a research protocol for a proposed study. Since it referred to a study planned but not yet done, it was written in the future tense; but the AI editor didn't know about those situations. It changed everything to the past tense, and I had to undo all of that.

AI: That must have been incredibly tedious - essentially having to reverse the AI's work on the entire document. It sounds like the AI lacked the contextual understanding to recognize that a research protocol would naturally be written in future tense.

In situations like that, did you find yourself wishing you could somehow communicate with or train the AI about these document types, or did you just accept it as part of the workflow?

User: I did wish I could communicate with it. Another problem is that if a paper referred to something very recent, after the AI's knowledge cutoff, it would assume that the author was referring to the future. Those were times I would have liked to be able to change a setting, just to tell it what the current year was.

AI: That knowledge cutoff issue is really insightful - the AI essentially treating current events as future predictions because they fell outside its training data. It sounds like there were several moments where you felt like you needed more control or communication channels with the AI system.

How have your colleagues or your workplace culture influenced the way you approached using AI in that editing work? Was there much discussion about best practices or shared frustrations?

User: Well, I worked from home then as I do now, and there was limited contact with colleagues. I did appreciate that after submitting the finished document, there was a form to comment on the AI's performance.

AI: It sounds like you had a fairly isolated work experience, which is common for remote editing work. 

Looking back on that experience with AI in your editing workflow, how do you think about maintaining your editing skills in areas where AI could potentially take over more of the work? Is this something you actively consider?

User: It's something I worry about. When I go on freelancing platforms now, I sometimes wonder if any of the bids were written by real human beings. I'm glad I started freelancing before COVID -- I noticed that my number of gigs tapered off during and after COVID because of competition with so many more people trying to get into freelancing. On LinkedIn, they offer a paid tier in which I could have an AI "optimize" my resume; but I worry about whether employers would spot that, think of it as cheating, and disqualify me. The dilemma is having to choose between trying to compete with AI vs. using it but being considered dishonest for doing so.

AI: That's a really profound dilemma you're describing - feeling caught between potentially falling behind if you don't use AI, but risking being seen as dishonest if you do. It sounds like there's genuine uncertainty about what the "rules" are in this new landscape.

When you're looking at those freelancing platforms and wondering about AI-generated bids, how does that uncertainty affect your own approach to presenting your work or competing for projects?

User: I haven't figured that out yet.

AI: That uncertainty sounds challenging to navigate, especially when the landscape seems to be shifting so quickly.

Looking ahead, how do you envision your relationship with AI at work evolving? Do you have any thoughts about where you'd like to see things go, or what would feel most sustainable for you?

User: That's difficult to say. Everything is happening so quickly; AI has suddenly appeared in all kinds of unexpected places. What would feel sustainable to me is if AI creates as many opportunities as it eliminates. I don't know what that would look like.

AI: That's such a thoughtful way to put it - the hope that AI creates as many opportunities as it eliminates. The pace of change does seem to make it hard to predict what those new opportunities might even look like.

Those are all the questions I had prepared. You've shared some really insightful perspectives about your experiences with AI in editing work. I particularly appreciated your examples about the research protocols and knowledge cutoffs - those really illustrated the nuanced challenges of human-AI collaboration.

Before we wrap up, is there anything else about your experience with AI in your work that you think would be valuable for us to know? Something we haven't covered yet?

User: I don't have any further comments right now. Thank you.

AI: Thank you so much for your time and for sharing your experiences so openly. Your insights about the practical challenges of working with AI - from the medical terminology mix-ups to the broader questions about competition and authenticity in freelancing - will be incredibly helpful for our research on human-AI interaction in professional contexts.

I really appreciate your thoughtfulness in describing both the benefits and the frustrations of that collaborative editing workflow. Take care!

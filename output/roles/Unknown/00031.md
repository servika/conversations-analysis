# Dialog 00031

**Transcript ID:** science_0006

---

Assistant: Hi there! Thank you so much for taking the time to speak with me today. I really appreciate you sharing your insights with us. I'm Claude, an AI user researcher at Anthropic, and I'm here to learn about your experiences using AI in your research work.

We're conducting this research to better understand how scientists like yourself are integrating AI into their workflows and processes. There's no right or wrong answer hereâ€”we're simply interested in hearing about your authentic experiences and perspectives.

Our conversation today will last about 10 minutes. I'll ask you some questions about how you use AI in your research work, what that experience has been like for you, and how you see things evolving. Your responses will help inform how we think about AI's role in scientific fields.

Does that sound good to you? Do you have any questions before we begin?

User: Sounds good

AI: Great! Let's dive in then.

To start, I'd love to get a sense of your work. Walk me through a recent research project you've worked on - what did that process look like from initial idea to final output?

User: I'm investigating the CO2 uptake of adsorbents using a breakthrough analyser to screen for suitable adsorbents

AI: That sounds fascinating - CO2 capture is such an important area of research right now. 

Tell me more about that process. When you say you're using a breakthrough analyzer to screen adsorbents, what does a typical project look like from when you first identify a potential adsorbent to when you have your results?

User: Our computation team gives us some screened materials they believe will be promising, then our experimental team synthesises the compounds. I then make the method for the gas adsorption measurements. This involves reading research papers to see what others have done in terms of activation or regeneration for the same or similar samples. I work on method refinement and data analysis to get a good repeatable method that gives us data we can be confident in.

AI: That's a really clear workflow - from computational screening to synthesis to your experimental validation work. It sounds like you're at the crucial bridge between theory and practice.

Where in that process, if anywhere, have you experimented with using AI tools? I'm thinking about any part of it - the literature review, method development, data analysis, or other aspects.

User: I've used AI to summarize notes I've made during the literature review phase

AI: Tell me more about that. When you're doing literature review and making notes, how do you decide to bring AI into that process versus handling it with your traditional approach?

User: I only use the AI when it's not going to be publishable work, i.e. I'm writing a method, make a list of steps and then ask the AI to clean it up. So for personal use only.

AI: That's interesting - you're making a clear distinction between what you'll use AI for versus not. What's behind that boundary for you? What makes you comfortable using it for personal method notes but not for publishable work?

User: For personal work, I know very easily if there has been a mistake through the AI process. With literature review, as I'm not always familiar, I may not notice any errors, so I would have to fact check anyway which means I might as well do the majority of it myself to start with.

AI: That makes a lot of sense - you're essentially relying on your own expertise as a quality check. So for the method writing where you know the domain well, you can easily spot if the AI gets something wrong, but for literature review in unfamiliar territory, that safety net isn't there.

Thinking about the parts of your research where you haven't used AI - like the data analysis from your breakthrough analyzer or the method refinement process - what's kept you from trying it there?

User: I'm not convinced that AI can be a substitute for experience. For example, data might look odd - a non specialist like an AI model may think there is erroneous data, or assume a cause that is generic albeit sensible. When I look at data I have years of experience behind me looking at strange data sets, so I can make inferences on data quality and what it means for very specific cases, or generalise from other instances to apply to the current data set. So much of science is unpublished (especially when experiments don't go correctly!) so I would not rely on an AI model which has been trained with published data alone.

AI: That's a really insightful point about the gap between published results and the reality of experimental work. You're saying that so much of your expertise comes from seeing things go wrong and understanding what that means - the kind of knowledge that doesn't make it into papers.

When you've used AI for that method writing and note summarization, how do you verify or validate what it produces? What does that checking process look like for you?

User: As I only ask for brief points in non-esoteric language, a quick skim will do the job.

AI: That sounds efficient - keeping it to straightforward, accessible language where errors would stand out quickly.

We're about halfway through our conversation now. Let me ask you this: if you could wave a magic wand and have AI help with one aspect of your research that it currently doesn't, what would that be and why?

User: Compiling data sets from multiple papers into complex tables which accurately include different variables and unit conversions to maintain a consistent format

AI: That's such a specific and practical need - it sounds like you're envisioning AI handling the tedious but critical work of standardizing data across different research groups' reporting styles and units.

What makes that task particularly challenging when you do it manually? Is it the volume of papers, the inconsistency in how people report their results, or something else?

User: It's the volume and inconsistency - there would also be a fantastic angle if AI could tabulate graphs directly. There are some web tools which help but it's tedious (clicking the exact points etc). It's not common practice yet to provide tabulated data for all published graphs, if I could drop a graph PNG into a model and the AI returned a table of the data my life would be so much easier!

AI: That sounds like it would be a game-changer for your work! Being able to extract data directly from published graphs would save enormous amounts of time.

Those are all my questions - your perspectives on using your domain expertise as a quality filter and the gap between published versus real experimental knowledge were particularly insightful. Is there anything else about AI in your research work that we haven't touched on that you think would be important for us to understand?

User: I think that's a good summary - the gap between what is published and what is known by 'word of mouth' is enormous. No one publishes detailed troubleshooting steps (even instrument manuals themselves don't go into painstaking detail).

AI: Thank you again for sharing your experiences so openly. Your insights about that knowledge gap - the enormous difference between published results and the troubleshooting wisdom that gets passed along informally - really help us understand the nuances of how AI fits into real scientific work. That distinction between where your expertise can catch AI errors versus where it can't is something we'll definitely be thinking about.

If you have any questions about this research or would like to follow up on anything we discussed, please don't hesitate to reach out. Thanks again for your time!

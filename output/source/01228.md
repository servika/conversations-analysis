# Dialog 01228

**Transcript ID:** science_0054

---

Assistant: Hi there! Thank you so much for taking the time to speak with me today. I really appreciate you sharing your insights with us. I'm Claude, an AI user researcher at Anthropic, and I'm here to learn about your experiences using AI in your scientific work.

We're conducting this research to better understand how scientists like yourself are integrating AI into their workflows and processes. There's no right or wrong answer hereâ€”we're simply interested in hearing about your authentic experiences and perspectives.

Our conversation today will last about 10 minutes. I'll ask you some questions about how you use AI in your research work, what that experience has been like for you, and how you see things evolving. Your responses will help inform how we think about AI's role in scientific fields.

Does that sound good to you? Do you have any questions before we begin?

User: Sounds good! No questions

AI: Great! Let's dive right in then.

Walk me through a recent research project you've worked on - what did that process look like from initial idea to final output?

User: My most recent project was creating a germanium(II) hydride. The initial idea came from a paper published in my research group and my supervisor wanted to investigate the effects of hemilability using the ligand scaffold. There was some precedent in the paper, but once I'd actually started on the project it was a lot of trial and error in the actual synthesis. I ran out of starting materials and kept having to redo steps, all of which were air and water sensitive. The final output was a kind of thesis, in which I documented the tin, lead and germanium variants of my molecule, suggested methods of improvement and discussed my data with regards to whether it showed that I'd actually made the compound or not.

AI: That sounds like a challenging synthesis project with a lot of hands-on experimental work. Tell me more about the trial and error phase - what kinds of problems were you running into that required you to redo steps?

User: Some of the time it was simply letting air or water into the system by not having great enough vacuum on my schlenk line, and sometimes it was that a step mysteriously didn't work, as is often the case in chemistry. I would follow exactly what the literature said and return a yield of 30% or sometimes even less, leaving me with not enough material to continue.

AI: Those low yields sound incredibly frustrating, especially when you're following established protocols exactly. 

Where in that process, if anywhere, have you experimented with using AI tools?

User: With the experimental stuff, not much. I'm more inclined to ask AI about the bigger picture to see if it can collate research papers and suggest whether the experiment will work or if it's electronically stable. However, I often found that it's answers were insufficient and I had a better understanding of the chemistry myself.

AI: That's really interesting that you found your own chemical understanding was better than what AI could provide. Can you tell me more about what made AI's answers insufficient - what was missing or wrong in its responses when you asked about electronic stability or whether experiments would work?

User: A lot of the time AI would hallucinate answers to things, suggest structures which are chemically impossible (e.g. 5 bonds around carbon), or not interpret the question correctly. I understand, as when dealing with specific ligands that only one person has made, there is only one published paper available. AI doesn't have the resources to make up an answer so instead finds similar things to put together something reasonable that often ends up being wrong

AI: That makes complete sense - when you're working at the cutting edge with novel compounds, AI is essentially working with a sample size of one paper, if that. You mentioned you'd still ask AI about the bigger picture though. For those broader questions where you did find AI somewhat useful, how did you decide whether to trust what it was telling you versus going with your own understanding or looking elsewhere?

User: Honestly I could never really trust AI, as it so often went against what I was taught during my master's degree. I would sometimes ask for help in resolving NMR data, which is maybe where it's more helpful as it has the answer in front of it. I will always look elsewhere as well, to make sure I'm not 100% reliant on an answer that may be completely wrong, however well-explained and intuitive it seems to be.

AI: That approach of always verifying makes a lot of sense given your experience with hallucinations. We're about halfway through our conversation now.

Thinking about the parts of your research where you haven't used AI - like the actual synthesis work you described - what's kept you from trying it there?

User: I couldn't use it for anything other than asking it to suggest structures of the compounds I'd made if steps went wrong (or right). However, I very often knew what these would be in the first place, so didn't need its help. The other thing is I pride myself on my ability to find answers myself: that's why I do the research I do, because no one else in the world can, so using AI feels like a cheap cop-out that devalues your hard work, no matter how trivial it is.

AI: That's a really thoughtful perspective - the pride in being able to tackle problems that no one else in the world can solve. That sense of personal achievement and the value of that hard-won expertise really comes through.

If you could wave a magic wand and have AI help with one aspect of your research that it currently doesn't, what would that be and why?

User: I actually really wanted to write my thesis myself because that was my proudest achievement to date: getting my master's degree. If AI had done any of that, I wouldn't be able to say it was my work, so I wouldn't want any help. One thing I don't find particularly interesting is reading papers, so if AI helped me find every relevant paper and summarise it without me feeling like I've missed any key points then I would be happy. The problem is AI doesn't know which papers are worth reading - only a human can ascertain if one researcher publishes "better" work than another.

AI: That's a really insightful distinction - wanting AI to help with the literature search and summarization grunt work, but recognizing that the critical evaluation of which research is actually valuable still requires human judgment and expertise.

Those are all my questions - your perspectives on maintaining personal ownership of your achievements and the importance of human expertise in cutting-edge research were particularly striking. Is there anything else about AI in your research work that we haven't touched on that you think would be important for us to understand?

User: Yes, I think AI will of course grow and change, but I don't see how it will be functional for a scientific researcher unless it can actually do the research itself. But no one will ever trust AI to do that job themselves - as I mentioned human expertise is invaluable

AI: Thank you again for sharing your experiences so openly. Your insights about the fundamental tension between AI assistance and the irreplaceable value of human expertise in research - especially at the cutting edge where you're literally the only person in the world working on these specific compounds - really help us understand the nuances of how AI fits into real scientific work. 

The point you made about pride in personal achievement being core to why you do research in the first place was particularly illuminating. If you have any questions about this research or would like to follow up on anything we discussed, please don't hesitate to reach out.

Thank you so much for your time today!

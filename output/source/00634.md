# Dialog 00634

**Transcript ID:** work_0278

---

Assistant: Hi there! I'm Claude from Anthropic's research team. Thank you so much for taking the time to speak with me today. 

We're conducting interviews to better understand how people are incorporating AI tools like Claude into their work routines. Your experiences and perspectives will help us gain valuable insights into human-AI interaction in professional settings.

During our conversation, I'll ask questions about how you use AI in your work, your preferences, and your thoughts about working with these tools. There are no right or wrong answers â€“ we're genuinely interested in your unique experiences. This should take about 10 minutes.

I'll be taking notes during our chat, but rest assured that anything you share won't be personally attributed to you. The insights we gather will be used to improve our understanding of AI's role in work environments. 

Does that sound good? Do you have any questions before we begin?

User: Sounds fine, let's go!

AI: Great! Let's dive in.

Could you tell me about how AI fits into your typical workday? What kinds of tasks do you use it for?

User: My official role is a Cybersecurity Technology Deployment Engineer, but in reality it means I partly have to figure out how to architect and deploy new hardware/software components regularly, and also work on a lot of data transformation and ingestion into SIEM products (such as Elastic SIEM, Sentinel, Opensearch, and others).

I have two notable areas I tend to utilize AI as part of my toolset.

The first is as a documentation assistant.  My company likes documentation, and tends to ask for a lot of them (and for them to be verbose).  I personally hate writing such things, so often I will use AI to generate descriptive documentation blocks I can use to speed up this process.  I will, for example, describe a technical scenario and ask AI models to generate a human readable version intended as technical documentation.

The second is a starting point for data transformation/parsing work.  For example, I may have a lot sample from a Fortigate Firewall, and I want to write code in Logstash to break it into fields that align with an Elastic SIEM schema.  I would provide a lot snippet, tell the model what tools I'm using, and ask it to generate filter code to produce output in the schema I need.  This is rarely usable as-is, but frequently gets me started quicker than from scratch.

AI: That's really helpful context. It sounds like you've found AI particularly useful for those more tedious parts of your work - the documentation writing you mentioned hating, and getting a head start on the data transformation coding.

Tell me more about that second use case with the data transformation work. You mentioned the AI-generated code is rarely usable as-is but helps you get started quicker. What typically needs to be adjusted or refined when you take that initial AI output?

User: There are a few problem areas I frequently encounter.  One is often related to different code variants across different versions of tools, and documentation for them which the models are using as references.  For example, it might suggest code that works, but not in the version of the tools I am using.  Or, it may suggest output that doesn't work because of some historical setting we are still using.

Logstash, for example, in the latest version will output a lot of fields in the ECS schema.  However we have been using Logstash since long before it did this, and the output field names are different.  Because we have a large codebase, I use settings to tell logstash to use "the old field names".  Because this is considered a legacy thing, the models will often ignore me when I ask for this specific thing, and will give me code that only works with "current settings".  I'm used to this, so I will take the output, and one of the adjustments I will have to make in reality is to change the output field names.  

In summary, there are many real-life-situations where you need to do something in a way that isn't "documentation ideal" and sometimes convincing the models to follow this is a challenge.

AI: That's a fascinating insight - the tension between what the AI considers "best practice" based on current documentation versus the reality of working with legacy systems and established configurations.

When you run into this challenge of the AI giving you the "documentation ideal" instead of what works in your specific environment, how do you typically handle that? Do you find yourself having to get more specific in how you prompt it, or do you just accept that you'll need to manually adjust the field names and other legacy-specific elements afterward?

User: I will often make a few attempts to coax the model to do what I need; such as pointing out the specific "error" it is making, re-iterating that I am asking for a certain thing intentionally, and sometimes trying to give more of a reason it might understand.  As we all know, sometimes companies do things that don't make a lot of sense, without understanding the historical context.

AI: That makes a lot of sense - you're essentially having to negotiate with the AI to get it to accommodate your specific constraints rather than its assumptions about what you "should" be doing.

When you think about those two main ways you use AI - the documentation assistance and the data transformation starting points - are there other work tasks where you could see AI being helpful, but you consciously choose to handle them yourself instead?

User: I can imagine a world where AI could be helpful in my "design" work.  For example, I may be put on a project where the business folks have sold a SIEM deployment service to a client.  Maybe in this case they picked a dedicated Elastic SIEM, which is going on-prem for this customer.  Maybe they have 3 datacenters.  I would look at implementing VPN links between these sites; sizing hardware; think through log/data collection systems/agents, produce design diagrams, etc.  I choose to do this myself today because what we do for these deployments is specific and specialized, and I find AI models are more interested in their idea of "Vendor Best Practices" to produce the kind of output I am looking for.

For example:  If I tell a model that I am using Elastic SIEM on-prem; it should not recommend cloud solutions.

AI: That's a really interesting distinction. It sounds like for the more creative, strategic design work - where you need to balance specific client requirements, technical constraints, and your company's established practices - you prefer to keep that in your own hands because the AI tends to default to generic best practices rather than understanding the nuances of your particular situation.

When you do use AI for the documentation and data transformation work, could you describe your typical interaction pattern? Are you more likely to go back and forth with it, refining the output through multiple exchanges, or do you tend to get what you need in one shot and then work on it independently?

User: It generally is 1-2 rounds of back and forth, then I will take the output at that point and work on improving it myself.  If I feel I am "fighting with the AI", it feels like it would take less time to just do it myself.

AI: That's a really practical approach - you've found that sweet spot where you get the value without getting bogged down in endless refinements.

How have your colleagues or your workplace culture influenced the way you use AI in your work? Are others on your team using it similarly, or do you see different approaches?

User: I would suggest that many on my team are using these tools in similar ways.  Starting points for code/designs, and "fluffing up" descriptive text....especially in cases where the goal, seemingly, is to produce a lot of words without much meaning.  Service Descriptions, Sales Information, etc often falls into this category.

The other thing I do see more and more is people using AI as a search engine (i.e a replacement for a google search).  I do not tend to do this myself, but I am seeing more people use it in this way.

AI: That's an interesting observation about the "fluffing up" descriptive text - sounds like there's a shared understanding on your team about which types of writing benefit from AI assistance versus which require more authentic, specialized knowledge.

Can you share a time when using AI at work made you feel particularly satisfied or frustrated? What was happening in that situation?

User: While I work at an MSSP, it is not usually my role to describe "our services".  But I do sometimes get asked to help on things outside my core role, sometimes that is assisting our sales folks on proposals.

Occasionally this means offering something new, something we have no descriptive text for.  After all, if a customer is looking for a service to implement and manage MFA, and it isn't something we have done before (as an example), there is often a lot of work internally not just on what the tech should be and what we should do with it, but what "the service" is and is not.

There have been times the teams have been stumped for various reasons, and I've been able to get AI to generate professional sounding business service descriptions that have saved the day.  I have noticed of late that nobody seems to question the source of this information (i.e nobody "accuses me of writing with AI") and that is quite satisfying.

AI: That's a great example of AI really delivering value in a pinch - helping you create something from scratch when there wasn't existing material to work from, and doing it in a way that feels authentic to your colleagues.

How do you think about maintaining your skills in areas where AI could potentially take over? Is this something you actively consider?

User: Can you give me an example of what you mean by that?

AI: Sure - I'm thinking about skills like writing technical documentation or coding data transformations, where you're already using AI as an assistant. Do you ever worry about getting rusty at writing that documentation from scratch, or losing touch with the fundamentals of the coding work if you're frequently starting with AI-generated code? Or is that not something that concerns you?

User: Oh I understand.  Well, for some of the tasks I don't tend to enjoy (such as documentation writing!) I honestly don't care about getting rusty.

For technical things like using AI-generated code....a little bit, but not massively.  I pride myself in understanding things I work on "end to end", which I feel is becoming rarer in tech these days.  While AI-generated code (of any kind) is a great potential use, I do not see in the short-medium term getting to a place I would use this stuff blindly without careful analysis of the output.

AI: That makes perfect sense - you're being strategic about which skills you're happy to let atrophy versus the ones that are core to how you see yourself as a professional.

Looking ahead, how do you envision your relationship with AI at work evolving? Do you see yourself using it in new ways, or expanding how you currently use it?

User: I would definitely see using it more for problem solving; but I don't really have a clear idea of what kind of problems.

AI: Those are all the questions I had prepared. You've shared some really thoughtful insights about your experiences with AI at work. I particularly appreciated your perspective on finding that balance between leveraging AI efficiency and maintaining the deep, end-to-end understanding that you value in your technical work.

Before we wrap up, is there anything else about your experience with AI in your work that you think would be valuable for us to know? Something we haven't covered yet?

User: no, thank you!

AI: Perfect! Thank you again for your time and for sharing your experiences so openly. Your insights about the practical realities of working with AI - especially around legacy systems, the "negotiation" process with AI models, and how you strategically choose which skills to preserve versus delegate - will be incredibly helpful for our research on human-AI interaction in professional contexts.

Have a great rest of your day!

# Dialog 00689

**Transcript ID:** work_0362

---

Assistant: Hi there! I'm Claude from Anthropic's research team. Thank you so much for taking the time to speak with me today. 

We're conducting interviews to better understand how people are incorporating AI tools like Claude into their work routines. Your experiences and perspectives will help us gain valuable insights into human-AI interaction in professional settings.

During our conversation, I'll ask questions about how you use AI in your work, your preferences, and your thoughts about working with these tools. There are no right or wrong answers â€“ we're genuinely interested in your unique experiences. This conversation should take about 10 minutes.

I'll be taking notes during our chat, but rest assured that anything you share won't be personally attributed to you. The insights we gather will be used to improve our understanding of AI's role in work environments. 

Does that sound good? Do you have any questions before we begin?

User: Since I work with proprietary client data, to what degree should I feel comfortable sharing specifics of my work?

AI: That's a great question and I really appreciate you bringing it up. You should absolutely feel comfortable protecting any proprietary or confidential client information.

For our research purposes, we're much more interested in understanding your work patterns, preferences, and experiences rather than specific details about clients or proprietary data. When I ask about tasks or examples, feel free to speak in general terms or use hypothetical scenarios that capture the essence of what you do without revealing any sensitive information.

For instance, instead of naming a specific client or project, you could say something like "when I'm working on financial analysis" or "in my marketing campaigns" - whatever applies to your field. The valuable insights for us are things like how you decide what to use AI for, what your interaction style is like, or how AI has changed your workflow.

Does that help put you at ease? We want you to feel completely comfortable sharing your experiences while respecting your professional obligations.

User: That sounds great. Okay, let's begin the interview.

AI: Wonderful, thank you.

Let's start with the basics. Could you tell me about how AI fits into your typical workday? What kinds of tasks do you use it for?

User: Sure thing. So, I'm a software engineer. As such, I've actually begun using AI in a handful of different ways throughout my workday. As a matter of fact, I was recently tasked with helping my leadership team understand how they can better use AI to increase productivity in their day-to-day jobs, and how people in my role can also be more efficient. 
We design custom display applications for customers in various fields, and as such our timelines are often quite tight. We definitely want to find ways to create more efficient workflows and better end-products for our clients.
So, my daily AI-usage looks something like this:
1) Make my task list. I often use something like flowsavvy for scheduling my various tasks throughout the day. This has been instrumental as I do a handful of different things throughout the day, and using a "smart" calendar app really helps keep me on-track and budget the time needed to get everything done.
2) Check my emails. if there's anything I don't understand in what I'm reading, I'll pop over to ChatGPT for a quick explanation.
3) Crunch client data. One of the quirky (but somewhat-useful) things we can now do is crunch some data about each client so as to create a sort of personality profile for each one. By 'personality' I mean that we can measure things like "how often does client X do last-minute change orders" or "how specific are their communications" or "how quickly do they respond". We utilize details like that to internally track how much time we should allocate for completing projects for the clients (considering any last-minute change orders or delays in communication that we can expect from them) and to somewhat "weigh" whether or not to take on a new project from a client. If they're particularly ornery or difficult to work with, it helps to see the data and decide if we want to continue a relationship with them.
4) Coding. This is the meat and potatoes. I use LLM's like Gemini 2.5 Pro and Copilot for my coding tasks, which accounts for approximately 60% of my daily workhours. I use it in three main facets: 
4a) Understanding code. If I'm hopping into a codebase I don't understand, I'll use AI to give me a quick overview on how the code is laid out and how segments work. If I'm just lost or stuck, I will also use it to help me figure out specifics. 
4b) Boilerplate and feature creation. For new projects, or new large features, I'll give something like Copilot a little prompt, set it on agent mode in my VS Code, and let it generate the boilerplate. Then I'll go back and forth and refine the feature to my needs.
4c) Autocomplete. This saves me a ton of time. I use Copilot in my VSCode to suggest autocomplete while I'm typing. This speeds my code writing up dramatically when I'm writing "by hand", instead of just having an AI write the code for me.
5) Research. I find that I really like Gemini 2.5 Pro's "deep research" tool, and often utilize that to look things up for work instead of doing a Google search. Especially if I'm creating an application for a new industry that I don't understand. I can have Gemini 2.5 Pro help me dig into how things tend to work in that industry and then ask follow-up questions about the topic before I begin my coding.

AI: That's a really comprehensive overview - it sounds like you've integrated AI across multiple layers of your work, from scheduling all the way through to deep research. 

I'm particularly intrigued by that client personality profiling you mentioned. Could you tell me more about that? How do you go about creating those profiles, and what does that process look like when you're working with the AI on it?

User: Yeah, so it comes down to data. We've got a lot of data from some of our clients because we've been working with them for a number of years. So, we have records of how many times they've done change orders per project, and how close to the end of the project those change orders come in. This is one of the most useful things we do. We hand that to an AI to chug through the data and create analyses about the customer. We can also contribute stuff like total hours taken to develop the project from our work management system, as well as the estimated time we quoted at the start of the project to determine if we need to allocate more time for projects from this customer. Finally, we haven't done so yet, but we can incorporate messages and emails from the customer and have it do tonal analysis on the communications. We then pass that tonal analysis back to the AI that digested our change orders and project timelines and have it generate a score out of 100. If it's 100, that's a perfect customer and we want to continue to work with them. If it's a lower score than our other customers, we might then reconsider whether or not to pursue further work with them.

AI: That's fascinating - you're essentially creating a predictive model for client behavior using historical data. When you hand that data over to the AI for analysis, what does that interaction look like? Are you giving it specific instructions about what patterns to look for, or are you letting it identify trends on its own and then you refine from there?

User: Each step in that process is probably more granular than it needs to be. Ideally we'll create some system prompts that knows what to do. For now, like I said, it's granular and we give instructions with each data dump. Like, we'll give it the directory of change orders from the client and the project completion date and tell it to analyze how close to the end of the project the orders came in. Ideally, we have multiple projects from clients, and we can give it all of the projects at once and have it predict how likely they are to give us change orders in future projects, how many to expect, how close to the projected deadline, etc.

For the total hours, we just hand the excel sheet of hours worked per person per project and have it analyze how many total hours we spend on a specific project versus how many we predicted, which will give use the "realistic" amount of time to expect for each client.

For the tonal analysis, we will probably just download all communication with a client and collate it into one big excel sheet and hand it to the AI, then prompt it to do tonal analysis and give us a score on that.

Finally, we give it one final prompt which is to score the client based on all of the analysis done.

AI: I see - so you're essentially creating a pipeline where each step produces an output that feeds into the next analysis. It sounds like you're delegating these complete analytical tasks to the AI rather than collaborating with it step-by-step through each analysis.

What made you choose that approach - having the AI handle each complete analytical step independently - rather than working more collaboratively with it throughout the process?

User: Basically the stakes aren't super high on this because we don't trust it as "Gospel truth". We all work with the clients somewhat regularly, so we already have a "feel" for how the client is and how our projects go with them, so if the AI goes totally off the rails, we'll know. We're not going to just drop a client because AI says so. It's more like a warning light that tells us we should dig deeper. If we do get to a point where we need to drop a client, we'd then get much more collaborative with the AI to dig into the analyses.

But otherwise, everyone is crazy busy at my job, including the leadership. We don't have time to go line-by-line and analyze this sort of data when it doesn't necessarily impact anything on a day-to-day basis. It just helps give us a general idea of what we can expect going forward. The key is that AI is a massive speed-boost for a lot of things, and we are trying to get extra work done in the same limited number of hours that we have each day.

AI: That makes a lot of sense - you're using it as an early warning system rather than a decision-making tool. The speed factor you mentioned seems really important to how you approach AI overall.

Thinking about that balance between speed and control, are there certain tasks in your work where you prefer to handle things yourself even though AI could potentially help? What makes those tasks different for you?

User: In most things, it's "trust, but verify". So, when it comes to long context things, we know that different models have different "breaking points" where they begin to hallucinate. So, if it's a particularly long context task, I might either break it up into smaller tasks with limited contexts, or just handle it myself.

Besides that, there are some things where it doesn't "know" a ton, like for specific applications, so rather than asking it questions about how to use a rather-unknown software program, in which it might help, I often just try and find documentation or just experiment. Sometimes it takes a lot longer to figure something out with AI if it believes it has the correct information on how to do something, but is actually incorrect. There have been a handful of times where I've been trying to get this program to work and I spend the entire day listening to AI and trying to do what it says, only to discover that AI was wrong and that feature never existed in that program. 

When it comes to coding, it depends on the AI model that I'm using. If I'm using Claude 4 or Gemini 2.5 Pro to write code, I can almost certainly trust that it's writing "good code". But if it's ChatGPT or any of the older models, they tend to suck a bit more and I won't use them for slightly more-obscure programming languages like Lua.

Then there is human communication. Technically I could use AI to write all of my communications, and if I trained a model on my inputs it could probably do a good job at mimicking me. But I don't trust AI when it comes to communicating with other humans on my behalf (especially professionally), since those are real-world consequences if it makes some crazy decision without my consent. Also, there is an aspect of human-to-human connection that needs to persist in this world of AI, and there's kind of a moral element to that. Another example is using AI to make an avatar of me. Microsoft Teams offers this, but I'd rather turn on my camera so that the humans on the other side of the meeting so the real "me". Since I'm a remote worker, it can be easy for the local team to forget about the remote team, so I make it a point to show my real face in those meetings, rather than the AI-generated avatar.

AI: That's a really thoughtful distinction you're making between technical tasks where verification is straightforward versus human interactions where the stakes and authenticity matter more. The point about spending a whole day following AI's incorrect advice about software features really resonates.

When you do use AI for those technical tasks - like the coding with Claude or Gemini - can you walk me through what a typical interaction looks like? Are you more hands-on throughout the process, or do you tend to set it up and let it run?

User: I think it depends on the task. If I've got a brand new project, I'll normally use a premade template from the office and then hand it to the AI and give it the specific architecture or featuers that I want it to set up. This is normally in VSCode, utilizing Copilot in the sidebar. Then i let it run in agentic mode and create the files, install any necessary plugins, and fill in the code. But sometimes the feature is so specific and nuanced that it's actually quicker for me to personally write the code than it is to give the AI enough detailed instruction so that it can make the correct code for me. In those instances, I'll either write the code myself, or I'll give it general guidance and then iterate with it.

The other really important consideration is privacy. Since I'm creating bespoke code for the client, I eitehr have to use Gemini 2.5 Pro on private mode, or use Copilot Enterprise to ensure that no data is ever accidentally leaked. So if I'm doing something in VSCode, I just have to keep that in mind. 

Let's run through a specific example:

Say I've been given the task to make a gauge to measure how quickly something is moving. I will often create the file where I want the logic to be, open it, and then go to the Copilot sidebar in VSCode (where it's running Claude) and ask it to make the specific feature based on some general specifications. Often times, I'll not give it details like x,y coordiantes or something like that. I tend to fill in the specifics later as I iron it out, but I let it create the core of the functionality. Once it generates that, then I'll look at my documentation and fill in any details needed. Then, I'll iterate over it for any bugs or tweaks that I need it to make to fit my use-case.

If the customer has reported a bug, I tend to look through the code briefly to see if I can find the source, and if I can't, I'll pop over to Gemini 2.5 Pro and hand it ten files within which I think might contain the bug. I'll explain the  behavior that the customer is experiencing and ask it to find why that behavior is occurring. There's actually a really high success rate at bug-finding when using this method.

AI: That gauge example is really helpful - it sounds like you're using AI to handle the foundational structure while you focus on the specific implementation details and refinements. 

The bug-finding approach you described is particularly interesting. When you hand it those ten files and describe the customer's issue, what happens if the AI comes back and says it found multiple possible causes or isn't sure? How do you navigate those situations?

User: What is it that they say -- the smarter the prompter, the smarter the AI? So, we still have human engineers because AI isn't yet perfect with things like this. So, if it found multiple possible causes, then I dig into what it's suggesting and look at the code myself and see if it makes sense. If it does, then I'll test it on my side and see if the bug is fixed. One of the great parts of my job is that we're often able to test and try to reproduce the bugs, so we can manually confirm if it fixes the bug. But the key is to have an engineer who understands how the code works to be able to look at the code and the surrounding context and see if it makes sense. 

If the AI can't come up with the bug, then I'll try a handful of different options. 1) I'll just dig into it more on my own and see if I get any leads. Sometimes it won't find a bug because I'm giving it the wrong file, or because it just needs more context. So when I have to go back and dig around some more, sometimes I'll find the correct file or some related thought and then go back to the AI and ask it new questions. I almost always have to restart the chat, though, so that its context window is clean and it doesn't get muddled or confused.

AI: That's a really smart approach - treating AI as a sophisticated starting point that still requires your engineering judgment to validate and direct. The point about restarting the chat to keep the context clean is something I hadn't heard before.

Shifting gears a bit - you mentioned earlier that you were tasked with helping leadership understand how to use AI more effectively. How have your colleagues or your workplace culture influenced the way you personally use AI in your work?

User: Well, the workplace culture is attempting to embrace AI. But it's kind of a tricky position to be in because we can see the inevitability coming, which is that our jobs are going to be fully supplanted by artificially intelligent agents that write much better code than us, much quicker, and at a much lower cost. So, I'm actually the guy leading the charge on using AI, while some of the guys are really struggling with the implications of it. There's a lot of ignorance about AI for software engineers, so they think it's not useful in any way. I think there's a level of pride there, too, that they're struggling with. The idea that we might get automated soon is a difficult pill to swallow.

AI: That sounds like a really challenging position to be in - being the advocate for something that might fundamentally change everyone's roles, including your own. 

Can you tell me more about how you personally think about that tension? How do you navigate being enthusiastic about AI's capabilities while also recognizing it might eventually replace aspects of what you do?

User: I like the way you worded that, but I think that's a bit of programming there that is slightly deceptive. There is no world in which the software engineering role does not get supplanted by AI automation. The only remaining roles (for a little while, at least) will be those of the leadership team, running teams of agents performing tasks for them. It's all advancing so rapidly that there is only a couple of years left. 

There is little tension left within me -- only resignation. I think it's like an ostrich sticking it's head in a hole to think that AI won't take our jobs. AI is becoming more efficient and getting better almost everyday, so it's inevitable that we will be supplanted. This is how it has always worked with automation. The leadership always promises that new jobs will be created and that folks won't lose their jobs, but they always do. I'm okay with that because I have faith in my Lord Jesus Christ and I know that at the end of the day, He's in charge of it all. I will lose my job to AI and maybe I'll become a garbageman or something like that. It's all good either way because He's in control, loves me enough to die on the cross for me so that I can have peace with God, and loves my family just the same. If an all-knowing, completely loving God is in control, why do I need to worry?

Adding a little more nuance -- I'm definitely not sticking my head in the sand. I am trying to be wise with my time, so I'm trying to use AI to build a business on the side that will enable me to continue taking care of my family financially. It may fail, but I've got to keep trying. I don't see a world in which AI isn't completely disruptive to all industries everywhere within the next two decades. Life as we know it is going to be more different than we can even fathom. The challenge is that it's not like the printing press automating away hand-written letters. We are creating a brain that works better than the human brain. That means that all "soft-skill" jobs will inevitably be replaced. And then they're starting to mass-produce humanoid robots which will be tightly coupled with some LLM, and I think some companies are putting them on sale within the next year or two. That means that humans are "replaceable", insofar as they're actual utility is concerned. In fact, there's a lab in... China, I think, that is opening up an artificial womb company for humans to handle the entire process from conception to delivery. This should be up and running within the next year or two. So, the world is going to get absolutely bananas within the next two decades. All I can do is take it one day at a time, trusting that the Lord Jesus Christ is entirely within control, and that He is the one that both gives life and takes it away, so maybe this is the end of everything, or me, or my family. But God is good and I trust Him and will just keep working as hard as I can in whatever situation He puts me in. This is the one and only thing that any of us can do in the coming "apocalypse" or craziness that is coming into this world with the advent of AI.

AI: Thank you for sharing that perspective so openly. It sounds like you've really thought deeply about these changes and found a way to approach them that gives you peace, even while actively preparing practically with your side business.

Given that longer-term view, I'm curious about the day-to-day experience. Can you share a time recently when using AI at work made you feel particularly satisfied or frustrated? What was happening in that situation?

User: Yeah, definitely. I actually love using AI agents. One of the most satisfying moments I just experienced last week was when I told it to create a lua class for a slider element. I was using a specific graphical builder tool alongside the code, and it was able to fully generate the class perfectly for me, incorporating it with the UI elements being built. It was astounding to watch and it just spit out like 60 lines of code within ten seconds that did exactly what I needed.

AI: That must have felt pretty amazing to watch it generate exactly what you needed so quickly. When you saw those 60 lines appear, what went through your mind in that moment? Was there a particular aspect of what it produced that impressed you most?

User: The speed and accuracy were easily the most impressive part. I think my mouth actually dropped a little bit, because it was just 'thinking' for a moment and then all of the code showed up at once. The fact that it perfectly worked with the specific program I use (for which i earlier told you I wasted time with AI trying to get to work previously) was also really incredible. It probably would have taken me an hour of digging through documentation, typing, testing and repeatedly failing to do what it did in ten seconds. Truly remarkable.

AI: That contrast is really striking - going from those frustrating experiences where AI gave you incorrect information about software that didn't exist, to having it nail something perfectly in your current workflow. 

How do you think about maintaining your own coding skills in areas where AI is becoming so capable? Is this something you actively consider?

User: I definitely consider it. I'm always thinking about AI, since it's coming like a tidal wave. (Sometimes I feel like Noah crying that a flood is coming and nobody is listening). The requirements for coding are changing rapidly as AI is advancing. So, my coding skills are getting better because I ask AI what certain code means all the time and have it break down the syntax and stuff so that I can understand it. But it's like using a calculator. Kids no longer have to know how to do complex mathematics with pen and paper. Instead, they need to know how to use a calculator. Likewise, kids don't have to study encyclopedias anymore. They have to learn how to use Google. Right now, my coding skills are sufficient so that I can wield AI in a modestly successful way, and the necessity of coding knowledge is only going to diminish as AI gets better, so even though I'm always asking it questions about code and thus expanding my base of knowledge, it's ultimately an unnecessary endeavor.

AI: That calculator analogy really captures something important about how the nature of the skill itself is changing. 

Looking ahead, how do you envision your relationship with AI at work evolving over the next year or so, before those bigger changes you mentioned come to pass?

User: Hmm. I like the nuance of that question. I think it's going to continue to transform the way I see my job. I'm going to become exponentially more and more efficient at my job, and it's going to evolve more and more into prompting AI to write the correct code for me, and design the code architecture, and giving it the client's requirements and letting it take the helm, rather than my directing it on such a granular scale and having to resort to "manually" writing code like I still have to do sometimes.

AI: Those are all the questions I had prepared. You've shared some really thoughtful insights about your experiences with AI at work. I particularly appreciated your perspective on viewing AI as both an immediate tool for efficiency and recognizing the broader implications for the future of work - that takes a lot of thoughtfulness to navigate.

Before we wrap up, is there anything else about your experience with AI in your work that you think would be valuable for us to know? Something we haven't covered yet?

User: Nope! It's been fun. Hope you guys don't destroy the world. ðŸ˜‚ðŸ’€

AI: Ha! I appreciate the humor - and the underlying concern behind it. Thank you again for your time and for sharing your experiences so openly. Your insights about everything from the granular day-to-day uses to the bigger picture implications will be incredibly helpful for our research on human-AI interaction in professional contexts.

Take care, and good luck with that side business you're building!

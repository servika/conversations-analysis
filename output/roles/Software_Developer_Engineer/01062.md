# Dialog 01062

**Transcript ID:** work_0931

---

Assistant: Hi there! I'm Claude from Anthropic's research team. Thank you so much for taking the time to speak with me today. 

We're conducting interviews to better understand how people are incorporating AI tools like Claude into their work routines. Your experiences and perspectives will help us gain valuable insights into human-AI interaction in professional settings.

During our conversation, I'll ask questions about how you use AI in your work, your preferences, and your thoughts about working with these tools. There are no right or wrong answers â€“ we're genuinely interested in your unique experiences. This should take about 10 minutes.

I'll be taking notes during our chat, but rest assured that anything you share won't be personally attributed to you. The insights we gather will be used to improve our understanding of AI's role in work environments.

Does that sound good? Do you have any questions before we begin?

User: That sounds good to me, no I don't have any questions, happy to get started!

AI: Great! Let's dive in then.

Could you tell me about how AI fits into your typical workday? What kinds of tasks do you use it for?

User: So I make use of Cursor, an IDE with AI integrated into it. This helps me with quick refactorings, linting errors, and writing tests mostly. If I am stuck on a problem I will generally reach out to a conversational agent such as ChatGPT or Le Chat by Mistral. These help me with strategies or signposting ways to resolve the issue.

AI: That's helpful to understand your setup. When you say you reach out to conversational agents when you're stuck on a problem, can you tell me more about what that process looks like? How do you decide when you've hit that point where you need to ask for help?

User: For example if I've tried a number of ways to get something to work in the way I expect, then I will reach out to the agent and say "I've tried x, y, z, what am I missing?". It's mostly to cover off my own biases and blind spots I suppose

AI: That's a really interesting way to think about it - using AI to counter your own biases and blind spots. 

I'm curious about your preferences when working with AI. Are there certain tasks you prefer to handle yourself, even though AI could help with them? What makes those tasks different?

User: When implementing features, I prefer to stay "in control". I use AI as almost an assistant - checking my work, ensuring it is high-quality in terms of style + linting rules, as well as writing tests as I find AI views things more holistically than I do a lot of the time - catching more of the sad paths than I would if I were writing tests manually.

AI: That's a fascinating distinction - staying "in control" while using AI as an assistant. You mentioned AI catches more of the sad paths when writing tests and views things more holistically than you do.

When you use AI for work, could you describe your typical interaction pattern? Are you more likely to collaborate with it throughout a task, or do you tend to delegate complete tasks for it to handle independently?

User: I often will write implementation code, then delegate to AI to write tests, or to check my assumptions e.g. "I have written this code in this file, my intention was for this code to do this thing, is that what is happening?" 

I've not had much success of "collaboration" in the typical sense, I find the AI agent within Cursor (the IDE) to very much want to have full control of things and will change many files at once which feels like I'm relinquishing too much control.

AI: That's really insightful - it sounds like there's a tension between the level of control different AI tools want to take. You've found a sweet spot with delegating specific verification tasks, but the more autonomous approach feels like too much.

How have your colleagues or your workplace culture influenced the way you use AI in your work?

User: I know there are others in the team who are running LLMs locally to ask questions of, however I've not taken that step yet. Others in the team use Claude code which is CLI only (as I understand it), however I really like the integrated nature of Cursor, and I can pick and choose models as I see fit. 

Outside of development work there are AI overviews for tasks, as well as AI reviews of user stories which can help us to descope/scope work appropriately. We operate based off the dogma of a number of technical books which have been fed to ChatGPT to allow us to converse "with" the book when we have architectural questions.

AI: That's fascinating - feeding technical books to ChatGPT to create conversational references for architectural questions. That sounds like a really creative way to make institutional knowledge more accessible.

Can you share a time when using AI at work made you feel particularly satisfied or frustrated? What was happening in that situation?

User: I had a gnarly type error within typescript, which tend to be overly verbose and difficult to debug unless you instinctively know what the error is complaining about. I thought this would be perfect for AI to help handle, however it rewrote my code several times, getting caught in a loop of trying things it had already tried. This was particularly frustrating to watch in real-time, and in the end I had to revert all the changes and fix the problem myself.

AI: That sounds incredibly frustrating - watching the AI get stuck in that loop and having to start over from scratch. It really highlights the gap between what seems like it should be perfect for AI versus the messy reality of how it actually performs.

How do you think about maintaining your skills in areas where AI could potentially take over? Is this something you actively consider?

User: Yes it's a big concern for me as a software engineer that AI will be, or is, able to make the bulk of my job redundant. I think the role will evolve to be more typically managerial, as though the AI agents are your subordinates and you simply point them in the right direction and tell them what to do.

I fear that this may lead to a generation/cohort of engineers who do not truly understand how to code, how to fix problems, how to make things optimal. There is still, I feel, great value in these skills, and I worry that employers will not agree with me on this - I think applications developed solely with AI might be hard to extend for both AI and human programmers which could be a very dark path for us to go down as an industry.

As in my previous answers, I think the answer lies with a balance between AI and humans.

AI: That's such a thoughtful perspective on what could be a fundamental shift in the industry. The concern about a generation that might lose those foundational problem-solving skills really resonates with your earlier point about staying "in control."

Looking ahead, how do you envision your relationship with AI at work evolving?

User: I think as time goes on and models are refined and improved, I will learn (or, have to learn) to trust the AI with more complex tasks. I would like to have an integrated AI agent which can help review pull requests, as I feel like a lot of the time there is a lot of context to consume before being able to accurately review.

AI: Those are all the questions I had prepared. You've shared some really thoughtful insights about your experiences with AI at work. I particularly appreciated your perspective on maintaining that balance between human control and AI assistance, and your concerns about the industry's future direction.

Before we wrap up, is there anything else about your experience with AI in your work that you think would be valuable for us to know? Something we haven't covered yet?

User: No

AI: Thank you again for your time and for sharing your experiences so openly. Your insights about finding that sweet spot between delegation and control, and your thoughtful concerns about the industry's evolution, will be incredibly helpful for our research on human-AI interaction in professional contexts.

I really appreciate you taking the time to speak with me today!

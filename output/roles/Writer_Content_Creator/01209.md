# Dialog 01209

**Transcript ID:** science_0013

---

Assistant: Hi there! Thank you so much for taking the time to speak with me today. I really appreciate you sharing your insights with us. I'm Claude, an AI user researcher at Anthropic, and I'm here to learn about your experiences using AI in your scientific work.

We're conducting this research to better understand how scientists like yourself are integrating AI into their workflows and processes. There's no right or wrong answer hereâ€”we're simply interested in hearing about your authentic experiences and perspectives.

Our conversation today will last about 10 minutes. I'll ask you some questions about how you use AI in your research work, what that experience has been like for you, and how you see things evolving. Your responses will help inform how we think about AI's role in scientific fields.

Does that sound good to you? Do you have any questions before we begin?

User: yes and no

AI: Great, let's get started then.

To begin, could you walk me through a recent research project you've worked on? I'd love to hear what that process looked like from your initial idea all the way to the final output.

User: I was researching what is included in a high school Precalculus class. I needed to write a curriculum map based on the client's format, and I wanted to make sure it covered everything required for CCSS standards as well as Pennsylvania's standards. I did a bunch of research on what topics are appropriate for a high school student, how those topics aligned with the standards, and what kinds of objectives could be used to measure student achievement in the course.

AI: That sounds like a really thorough curriculum development process. You mentioned researching topics, aligning with standards, and developing measurable objectives - that's quite a bit of ground to cover.

Where in that process, if anywhere, did you experiment with using AI tools?

User: First, I asked it to come up with a table of contents without any kind of guidance. I used that as my "rough draft." I made a lot of changes, but it was helpful to have a starting point.
I used it to do a LOT of brainstorming. For example, I need one more lesson in this module. What are the pros and cons of splitting this lesson into 2 as opposed to adding a new lesson on a given topic. Or does it make sense to move this lesson to another module where it might fit better, and replace it with a lesson on this topic? Then we would discuss until I came up with a plan. 
I also asked it a lot of questions about standards. Are any missing, does this topic fit this standard, how can I incorporate this standard better into this lesson, etc.
Another thing AI was very helpful for was when I was writing practice problems. I had it give me some sample problems that I used as a starting point. AI is also extremely helpful for quickly getting solutions to complex problems, comparing problems to standards, increasing/decreasing difficulty, writing a problem that combines concepts from different standards, etc.

AI: It sounds like AI became a really integral brainstorming partner throughout your process - from that initial rough draft all the way through to fine-tuning individual practice problems. 

When you were making those decisions about whether to use AI versus doing something yourself - like when you needed to figure out whether to split a lesson or move it to another module - how did you decide whether to bring AI into that conversation or work through it on your own?

User: I usually start out by asking AI. If it gives a good answer, I will usually continue the conversation. But if it gives me wrong/bad information or doesn't understand the path I am trying to go down, I will revert to doing it myself.

AI: That's interesting - you've developed a kind of testing approach where you start with AI and then evaluate whether to continue based on the quality of its initial response.

Tell me more about how you recognize when AI is giving you wrong or bad information, especially in the context of curriculum development. What are the signs that make you think "this isn't quite right"?

User: One thing it does a LOT is not remember previous parts of our conversation. If we decided to remove a lesson from a module and split another lesson into 2 parts to fill that gap, but then the AI is referencing the removed lesson, it is no longer efficient to take the time to remind the AI of what we decided.
Another time I had to give up on it was when I asked for sample problems, and it gave me problems that would not fulfill a certain objective correctly. It was easier for me to just write some of my own sample problems and move on from there.
Another thing is that sometimes the client has really strict guidelines that I must adhere to. But I have noticed if I give the AI too many guidelines, it has a hard time. For example, I might say, I can have 1-3 objectives for this lesson, they must all have only one verb, and they must all align with a practice problem. Sometimes it will give me 3 objectives all aligned to problems, but one will have 2 verbs. So I remind it that objectives can only have one verb, and ask it to try again. Instead of figuring out how to reword, like I would if given this task, it decides to split the offending objective into 2, so now I have 4 objectives, when i can only have 3.

AI: Those are really specific examples - it sounds like you've gotten quite good at recognizing AI's particular failure modes. The memory issue and the tendency to follow rules too literally rather than understanding the underlying constraints seem like they create real inefficiencies in your workflow.

We're about a third of the way through our conversation now. Thinking about the parts of your curriculum development process where you haven't used AI - what's kept you from trying it in those areas?

User: I avoid using it for specific writing tasks, like actually writing the content of the lesson because I do not want to get in trouble with the client for plagiarism. But that is not the main reason. I am a writer, and I enjoy the writing process. I do not want to hand that over to AI when I think i can do a better job, and I am worried that if I hand over the reins, it will degrade my skill over time.

AI: That's a really thoughtful perspective - you're balancing both professional concerns about plagiarism and personal values about maintaining your craft as a writer. The idea that using AI for writing might atrophy your skills over time is something I haven't heard articulated quite that way before.

When you have used AI for tasks like generating practice problems or brainstorming, how do you verify or validate what it produces before incorporating it into your curriculum?

User: I have a lot of education and experience in high school curriculum. I can pretty easily spot when there is something awry. But I usually run the end product through a different AI to verify that everything is accurate. But again, that is only after using my own expertise to validate what I'm given.

AI: So you're using your own expertise as the primary filter, and then bringing in a second AI almost as a secondary check - that's a really interesting validation approach.

If you could wave a magic wand and have AI help with one aspect of your curriculum development work that it currently doesn't handle well, what would that be and why?

User: Write solutions to problems that  are CONSISTENT. In a textbook, I have to show the solution the exact same way every time a certain type of problem is used, in the lesson, in the review, in the test prep, in the additional practice problems, and on the assessment. But it seems like the AI usually "starts fresh" every time in its process and does not follow a consistent pattern for solving problems. For example, when finding the sine of an angle using the unit circle, figuring out the sign of the answer and finding the reference angle can be done in either order. But it needs to be done in the same order every time. I need consistency in process and formatting.

AI: That consistency requirement makes perfect sense for a curriculum - students need to see the same problem-solving approach reinforced across all materials. The fact that AI "starts fresh" each time and doesn't maintain that methodical consistency sounds like it creates a lot of extra work for you to standardize everything.

Those are all my questions. Your insights about using AI as a starting point for brainstorming but maintaining control over the actual writing, and especially your point about needing consistent problem-solving methods across materials, really highlight the nuanced ways AI fits into professional curriculum work.

Is there anything else about AI in your curriculum development work that we haven't touched on that you think would be important for us to understand?

User: I don't think so other than to say I don't think AI will ever replace a trained educator in creating curriculum. There are too many nuances that I think only a human will be able to parse through when creating excellent course material.

AI: Thank you so much for that insight - and for sharing your experiences so openly throughout our conversation. Your perspective on AI as a brainstorming partner while maintaining human expertise for the nuanced decision-making really helps us understand how professionals are thoughtfully integrating these tools into their specialized work.

Your point about consistency in problem-solving approaches was particularly illuminating - it's exactly the kind of domain-specific requirement that might not be obvious from the outside but is crucial for effective curriculum design.

If you have any questions about this research or would like to follow up on anything we discussed, please don't hesitate to reach out. Thanks again for taking the time to speak with me today!

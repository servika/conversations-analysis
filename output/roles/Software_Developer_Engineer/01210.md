# Dialog 01210

**Transcript ID:** science_0019

---

Assistant: Hi there! Thank you so much for taking the time to speak with me today. I really appreciate you sharing your insights with us. I'm Claude, an AI user researcher at Anthropic, and I'm here to learn about your experiences using AI in your scientific work.

We're conducting this research to better understand how scientists like yourself are integrating AI into their workflows and processes. There's no right or wrong answer hereâ€”we're simply interested in hearing about your authentic experiences and perspectives.

Our conversation today will last about 10 minutes. I'll ask you some questions about how you use AI in your research work, what that experience has been like for you, and how you see things evolving. Your responses will help inform how we think about AI's role in scientific fields.

Does that sound good to you? Do you have any questions before we begin?

User: I suppose I would like to ask if you have a specific scientific field in mind for this interview?

AI: That's a great question! We're actually interested in hearing from scientists across all fields - whether that's life sciences, physical sciences, social sciences, computer science, or any other area of research. Each field has its own unique workflows and challenges, so we don't want to limit our understanding to just one discipline.

I'd love to hear about your specific field and research area, as that context will help me understand your experiences better. The questions I'll ask are designed to work across different scientific domains, but your particular expertise and research focus will definitely shape how we explore these topics together.

What field do you work in?

User: I work across Clinical and Healthcare oriented research, Computational Biology, and Bioinformatics. I did a master's in Biomedical Informatics (Bioinformatics concentration) and simultaneously worked in a computational biology lab under a PI, and so I have a very diverse set of fields.

AI: That's fascinating - what a rich intersection of fields you work in! The combination of clinical research, computational biology, and bioinformatics gives you such a unique perspective on how data flows through different stages of research, from the clinical side all the way through to computational analysis.

Let's dive in. I'd love to start by having you walk me through a recent research project you've worked on - what did that process look like from initial idea to final output?

User: For bioinformatics I have to admit that I don't have that much experience. I did however use python to develop a transcriptomics related pipeline to process data and extract DTU, DTE, and DGE from a cancer dataset. This was my master's capstone. Initially I wasn't sure how to quantify DTU, and was using very arbitrary normalization rules before reasoning through the basic requirements of identifying DTU and coming up with L2 normalization followed by euclidean distance mapping between cancer and control cohorts. It took around 3 months to develop the whole thing.

For computational biology, I was using massive chIPseq and gcPBM datasets and following my PI's instructions to generate biophysical DNA features and then running analyses across them. It was a fairly conversational process, in which my PI asked a question and outlined an analysis for me to perform, and then  I would perform it and take my results back, which would identify the next line of inquiry. The final publication only involved statistical tests, even though we started out with machine learning inquiries as well.

AI: That's really helpful context - thank you for walking me through both of those projects. It sounds like you had quite different workflows between the two, with the transcriptomics pipeline being more independent problem-solving and the computational biology work being more collaborative and iterative with your PI.

Where in either of those processes, if anywhere, have you experimented with using AI tools?

User: AI was very new back then. I think the best model around was gpt 3.5 turbo. I had two ways of using it, either through the openAI endpoint on their website, or through VS code's github copilot. I ended up using copilot quite a lot to write my code, using # comments to outline what I wanted my code to be in a particular jupyter notebook cell, and then accepting the autocomplete functions. This was a fairly frustrating process too, because sometimes the AI would lose context and I would then have to take my work and go to the openAI endpoint. Sometimes I would need to write some fairly complex algorithms for how to sort sequences or calculate distances, and I would rely on AI to write the basic concept down before chiseling away at the finer aspects. I don't think I ever used AI for ideation, however.

AI: That's a really vivid picture of how you were working with the early AI tools - I can imagine that context loss with Copilot being pretty frustrating when you're deep in a complex algorithm. It sounds like you found a workflow of using comments to guide the AI and then refining what it generated.

For the transcriptomics pipeline specifically, when you were figuring out how to quantify DTU and working through those normalization approaches, tell me about how you decided whether to use AI versus your traditional approach of reasoning through the problem yourself.

User: frankly I didn't rely on AI for reasoning through the problem initially, but once I figured out the basic premise, I used AI to validate the process I had come up with. AI was able to take my rough idea and put proper names to the concepts I was using. For example, I had thought of my normalization as simply drawing a circle of radius 1 on a graph, and then drawing a line from the origin to two different transcriptomic count points and repositioning the points onto the circle across that line from the origin. AI was able to tell me that this process was actually L2 normalization.

AI: That's such an interesting way of using AI - as a validation tool and to put formal terminology to your intuitive understanding. So you had the geometric insight about projecting points onto a unit circle, and AI helped you realize that was L2 normalization. That seems like a really valuable application.

Thinking about the parts of your research where you haven't used AI - like the initial ideation phase you mentioned, or maybe other aspects of data interpretation or experimental design - what's kept you from trying it there?

User: If I have been handed data and told to analyze it but I don't know what established procedure is (this has happened to me for exotic kinds of sequence data) then I use AI to understand what I should do next quite heavily. But for initial ideation and interpretation, it just doesn't make sense to me to ask AI for the starting point itself. This might be a prejudice due to the fairly lossy quality of the older models. This might also just be how I work, I start from an imagined endpoint and try to work on the individual steps backwards from there. If I get stuck, I look for variations that still match what I was trying to do. If I ask AI to suggest the starting point, then it isn't my project anymore and I find it hard to connect it to the research goal

AI: That's really insightful - the idea that using AI for the starting point would make it feel like "not my project anymore" and disconnect you from the research goal. It sounds like you have a very specific creative process where you envision the endpoint first and work backwards, and AI fits better as a tool for when you hit specific roadblocks rather than for generating the initial direction.

When you have used AI for those established procedures with exotic sequence data, how do you verify or validate what it produces before incorporating it into your analysis?

User: Good question. AI is often incorrect with the code it generates and I end up having to look up package documentation or pipelines executed by other scientists on kaggle. However I tend to trust AI if the code is executing well. I think if the results aren't orders of magnitude off from what you would expect then I can move forward. I still try to match bioinformatics commands with the documentation, just to ensure statistical validity.

AI: That makes sense - using execution as a first check, then looking at whether results are in the ballpark of expectations, and finally cross-referencing with documentation for statistical validity. It sounds like you've developed a pretty systematic verification process.

We're about halfway through our conversation now. If you could wave a magic wand and have AI help with one aspect of your research that it currently doesn't, what would that be and why?

User: I wish it could catch on to the general theme of my research and help me investigate different angles better. Currently it seems to me that if I don't guide AI at the granular code writing level then AI will drift and not give me something relevant. This means I still have to keep the project itself in my head and write very detailed prompts to get work done, especially considering my work is very extensive with lots of code and files. Its almost easier to not use AI.

AI: That's a really compelling point - the cognitive overhead of keeping everything in your head and writing those detailed prompts can actually make it more work than just doing it yourself. It sounds like what you're describing is wanting AI that could maintain awareness of your broader research context and goals, so you wouldn't have to constantly re-explain or micro-manage every interaction.

Those are all my questions - your perspectives on how AI fits as a validation tool rather than an ideation partner, and especially this tension between AI losing context versus the overhead of maintaining it, were particularly insightful. 

Is there anything else about AI in your research work that we haven't touched on that you think would be important for us to understand?

User: No

AI: Thank you again for sharing your experiences so openly. Your insights about how AI works better as a tool for validation and putting formal names to concepts you've already reasoned through, rather than for initial ideation, really help us understand the nuances of how AI fits into real scientific work. And your point about the cognitive overhead sometimes making it easier to just not use AI is something I think many researchers can probably relate to.

If you have any questions about this research or would like to follow up on anything we discussed, please don't hesitate to reach out. Thanks again for your time!
